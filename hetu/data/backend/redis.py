"""
@author: Heerozh (Zhang Jianhao)
@copyright: Copyright 2024, Heerozh. All rights reserved.
@license: Apache2.0 å¯ç”¨ä½œå•†ä¸šé¡¹ç›®ï¼Œå†éšä¾¿æ‰¾ä¸ªè§’è½æåŠç”¨åˆ°äº†æ­¤é¡¹ç›® :D
@email: heeroz@gmail.com
"""
import random
import hashlib
import redis
import asyncio
from datetime import datetime, timedelta
from ..component import BaseComponent, Property
from .base import ComponentTable, RaceCondition
import logging
logger = logging.getLogger('HeTu')


class RedisBackend:
    """å‚¨å­˜åˆ°Redisåç«¯çš„å®¢æˆ·ç«¯è¿æ¥ï¼ŒæœåŠ¡å™¨å¯åŠ¨æ—¶ç”±server.pyæ ¹æ®Configåˆå§‹åŒ–ï¼Œå¹¶ä¼ å…¥RedisComponentTable"""
    def __init__(self, config: dict):
        # åŒæ­¥ioè¿æ¥, å¼‚æ­¥ioè¿æ¥, åªè¯»ioè¿æ¥
        self.io = redis.from_url(config['master'], decode_responses=True)
        self.aio = redis.asyncio.from_url(config['master'], decode_responses=True)
        # è¿æ¥åªè¯»æ•°æ®åº“
        servants = config.get('servants', [])
        self.replicas = [redis.asyncio.from_url(url, decode_responses=True) for url in servants]
        if not servants:
            servants.append(config['master'])
            self.replicas.append(self.aio)

        # é…ç½®keyspaceé€šçŸ¥
        for url in servants:
            redis.from_url(url).config_set('notify-keyspace-events', 'Kghz')

    async def close(self):
        self.io.close()
        await self.aio.aclose()
        for replica in self.replicas:
            await replica.aclose()

    def rnd_replica(self):
        """æ¯ä¸ªwebsocketè¿æ¥è·å¾—ä¸€ä¸ªéšæœºçš„replicaè¿æ¥ï¼Œç”¨äºè¯»å–è®¢é˜…"""
        i = random.randint(0, len(self.replicas))
        return i, self.replicas[i]


class RedisComponentTable(ComponentTable):
    """
    ä½¿ç”¨rediså®ç°çš„Componentæ•°æ®è¡¨ï¼Œæä¾›æŸ¥è¯¢å’Œä¿®æ”¹åŠŸèƒ½ã€‚

    å‚è€ƒï¼š
    redis-pyååé‡åŸºå‡†ï¼š
    syncè°ƒç”¨ï¼šå•è¿›ç¨‹ï¼š1200/sï¼Œ10è¿›ç¨‹ç†è®ºä¸Š12 Kopsï¼Œç¬¦åˆhiredisåŸºå‡†æµ‹è¯•
    asyncè°ƒç”¨ï¼šå•è¿›ç¨‹+Semaphoreé™åˆ¶100åç¨‹ï¼š6000/sï¼Œ å‚è€ƒQPS 100,000

    ä½¿ç”¨ä»¥ä¸‹keysï¼š
    instance_name:component_name.{CLU0}:id:1~n
    instance_name:component_name.{CLU0}:index:key~
    instance_name:component_name:meta
    """

    def __init__(self, component_cls: type[BaseComponent], instance_name, cluster_id,
                 backend: RedisBackend):
        super().__init__(component_cls, instance_name, cluster_id, backend)
        component_cls.hosted_ = self
        # redis keyå
        hash_tag = f'{{CLU{cluster_id}}}'
        # ä¸èƒ½ç”¨component_cls.__name__ å¯èƒ½æ˜¯jsonåŠ è½½çš„åå­—ä¸å¯¹
        self._name = component_cls.component_name_
        self._root_prefix = f'{instance_name}:{self._name}:'
        self._key_prefix = f'{self._root_prefix}{hash_tag}:id:'
        self._idx_prefix = f'{self._root_prefix}{hash_tag}:index:'
        self._lock_key = f'{self._root_prefix}:init_lock'
        self._meta_key = f'{self._root_prefix}:meta'
        self._trans_pipe = None
        self._autoinc = None
        # æ£€æµ‹metaä¿¡æ¯ï¼Œç„¶ååšå¯¹åº”å¤„ç†
        self.check_meta()

    def check_meta(self):
        """
        æ£€æŸ¥metaä¿¡æ¯ï¼Œç„¶ååšå¯¹åº”å¤„ç†
        metaæ ¼å¼:
        json: ç»„ä»¶çš„ç»“æ„ä¿¡æ¯
        version: jsonçš„hash
        cluster_id: æ‰€å±ç°‡id
        last_index_rebuild: ä¸Šæ¬¡é‡å»ºç´¢å¼•æ—¶é—´
        """
        io = self._backend.io
        lock = io.lock(self._lock_key)
        logger.info(f"âŒš [ğŸ’¾Redis][{self._name}ç»„ä»¶] å‡†å¤‡é”å®šæ£€æŸ¥metaä¿¡æ¯...")
        lock.acquire(blocking=True)

        # è·å–rediså·²å­˜çš„ç»„ä»¶ä¿¡æ¯
        meta = io.hgetall(self._meta_key)
        if not meta:
            meta = self._create_emtpy()
        else:
            version = hashlib.md5(self._component_cls.json_.encode("utf-8")).hexdigest()
            # å¦‚æœcluster_idæ”¹å˜ï¼Œåˆ™è¿ç§»æ”¹keyå
            if int(meta['cluster_id']) != self._cluster_id:
                self._migration_cluster_id(old=int(meta['cluster_id']))

            # å¦‚æœç‰ˆæœ¬ä¸ä¸€è‡´ï¼Œç»„ä»¶ç»“æ„å¯èƒ½æœ‰å˜åŒ–ï¼Œä¹Ÿå¯èƒ½åªæ˜¯æ”¹æƒé™ï¼Œæ€»ä¹‹è°ƒç”¨è¿ç§»ä»£ç 
            if meta['version'] != version:
                self._migration_schema(old=meta['json'])
                # å› ä¸ºè¿ç§»äº†ï¼Œå¼ºåˆ¶rebuild_index
                meta['last_index_rebuild'] = '2024-06-19T03:41:18.682529+08:00'

        # é‡å»ºæ•°æ®ï¼Œæ¯æ¬¡å¯åŠ¨é—´éš”è¶…è¿‡1å°æ—¶å°±é‡å»ºï¼Œä¸»è¦æ˜¯ä¸ºäº†é˜²æ­¢å¤šä¸ªnodeåŒæ—¶å¯åŠ¨æ‰§è¡Œäº†å¤šæ¬¡
        last_index_rebuild = datetime.fromisoformat(meta.get('last_index_rebuild'))
        now = datetime.now().astimezone()
        if last_index_rebuild <= now - timedelta(hours=1):
            # å¦‚æœéæŒä¹…åŒ–ç»„ä»¶ï¼Œåˆ™æ¯æ¬¡å¯åŠ¨æ¸…ç©º
            if not self._component_cls.persist_:
                logger.info(f"âŒš [ğŸ’¾Redis][{self._name}ç»„ä»¶] æœ¬ç»„ä»¶æ— éœ€æŒä¹…åŒ–ï¼Œæ¸…ç©ºå·²å­˜æ•°æ®ä¸­...")
                del_keys = io.keys(self._root_prefix + '*')
                map(io.delete, del_keys)
                logger.info(f"âœ… [ğŸ’¾Redis][{self._name}ç»„ä»¶] å·²åˆ é™¤{len(del_keys)}ä¸ªé”®å€¼")

            # é‡å»ºç´¢å¼•ï¼Œå¦‚æœå·²å¤„ç†è¿‡äº†å°±ä¸å¤„ç†
            self._rebuild_index()
            # å†™å…¥metaä¿¡æ¯
            io.hset(self._meta_key, 'last_index_rebuild', now.isoformat())

        lock.release()
        logger.info(f"âœ… [ğŸ’¾Redis][{self._name}ç»„ä»¶] æ£€æŸ¥å®Œæˆï¼Œè§£é”ç»„ä»¶")

    def _create_emtpy(self):
        logger.info(f"âŒš [ğŸ’¾Redis][{self._name}ç»„ä»¶] ç»„ä»¶æ— metaä¿¡æ¯ï¼Œæ•°æ®ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»ºç©ºè¡¨...")

        # åªéœ€è¦å†™å…¥metaï¼Œå…¶ä»–çš„_rebuild_indexä¼šåˆ›å»º
        meta = {
            'json': self._component_cls.json_,
            'version': hashlib.md5(self._component_cls.json_.encode("utf-8")).hexdigest(),
            'cluster_id': self._cluster_id,
            'last_index_rebuild': '2024-06-19T03:41:18.682529+08:00'
        }
        self._backend.io.hset(self._meta_key, mapping=meta)
        logger.info(f"âœ… [ğŸ’¾Redis][{self._name}ç»„ä»¶] ç©ºè¡¨åˆ›å»ºå®Œæˆ")
        return meta

    def _rebuild_index(self):
        logger.info(f"âŒš [ğŸ’¾Redis][{self._name}ç»„ä»¶] æ­£åœ¨é‡å»ºç´¢å¼•...")
        io = self._backend.io
        rows = io.keys(self._key_prefix + '*')
        if len(rows) == 0:
            logger.info(f"âœ… [ğŸ’¾Redis][{self._name}ç»„ä»¶] æ— æ•°æ®ï¼Œæ— éœ€é‡å»ºç´¢å¼•ã€‚")
            return

        for idx_name, str_type in self._component_cls.indexes_.items():
            idx_key = self._idx_prefix + idx_name
            # å…ˆåˆ é™¤æ‰€æœ‰_idx_keyå¼€å¤´çš„ç´¢å¼•
            io.delete(idx_key)
            # é‡å»ºæ‰€æœ‰ç´¢å¼•ï¼Œä¸ç®¡uniqueè¿˜æ˜¯indexéƒ½æ˜¯sset
            pipe = io.pipeline()
            row_ids = []
            for row in rows:
                row_id = row.split(':')[-1]
                row_ids.append(row_id)
                pipe.hget(row, idx_name)
            values = pipe.execute()
            # æŠŠvaluesæŒ‰dtypeè½¬æ¢ä¸‹
            struct = self._component_cls.new_row()
            for i, v in enumerate(values):
                struct[idx_name] = v
                values[i] = struct[idx_name].item()
            # å»ºç«‹redisç´¢å¼•
            if str_type:
                # å­—ç¬¦ä¸²ç±»å‹è¦ç‰¹æ®Šå¤„ç†ï¼Œscore=0, member='name:1'å½¢å¼
                io.zadd(idx_key, {f'{value}:{rid}': 0 for rid, value in zip(row_ids, values)})
            else:
                # zadd ä¼šæ›¿æ¢æ‰memberç›¸åŒçš„å€¼ï¼Œç­‰äºæ˜¯set
                io.zadd(idx_key, dict(zip(row_ids, values)))
        logger.info(f"âœ… [ğŸ’¾Redis][{self._name}ç»„ä»¶] ç´¢å¼•é‡å»ºå®Œæˆ, "
                    f"{len(rows)}è¡Œ * {len(self._component_cls.indexes_)}ä¸ªç´¢å¼•ã€‚")

    def _migration_cluster_id(self, old):
        logger.warning(f"âš ï¸ [ğŸ’¾Redis][{self._name}ç»„ä»¶] "
                       f"cluster_id ç”± {old} å˜æ›´ä¸º {self._cluster_id}ï¼Œ"
                       f"å°†å°è¯•è¿ç§»clusteræ•°æ®...")
        # é‡å‘½åkey
        old_hash_tag = f'{{CLU{old}}}'
        new_hash_tag = f'{{CLU{self._cluster_id}}}'
        old_prefix = f'{self._root_prefix}{old_hash_tag}:'
        old_prefix_len = len(old_prefix)
        new_prefix = f'{self._root_prefix}{new_hash_tag}:'

        io = self._backend.io
        old_keys = io.keys(old_prefix + '*')
        for old_key in old_keys:
            new_key = new_prefix + old_key[old_prefix_len:]
            io.rename(old_key, new_key)
        # æ›´æ–°meta
        io.hset(self._meta_key, 'cluster_id', self._cluster_id)
        logger.warning(f"âœ… [ğŸ’¾Redis][{self._name}ç»„ä»¶] cluster è¿ç§»å®Œæˆï¼Œå…±è¿ç§»{len(old_keys)}ä¸ªé”®å€¼ã€‚")

    def _migration_schema(self, old):
        """å¦‚æœæ•°æ®åº“ä¸­çš„å±æ€§å’Œå®šä¹‰ä¸ä¸€è‡´ï¼Œå°è¯•è¿›è¡Œç®€å•è¿ç§»ï¼Œå¯ä»¥å¤„ç†å±æ€§æ›´åä»¥å¤–çš„æƒ…å†µã€‚"""
        # åŠ è½½è€çš„ç»„ä»¶
        old_comp_cls = BaseComponent.load_json(old)

        # åªæœ‰propertiesåå­—å’Œç±»å‹å˜æ›´æ‰è¿ç§»
        dtypes_in_db = old_comp_cls.dtypes
        new_dtypes = self._component_cls.dtypes
        if dtypes_in_db == new_dtypes:
            return

        logger.warning(f"âš ï¸ [ğŸ’¾Redis][{self._name}ç»„ä»¶] ä»£ç å®šä¹‰çš„Schemaä¸å·²å­˜çš„ä¸ä¸€è‡´ï¼Œ"
                       f"æ•°æ®åº“ä¸­ï¼š\n"
                       f"{dtypes_in_db}\n"
                       f"ä»£ç å®šä¹‰çš„ï¼š\n"
                       f"{new_dtypes}\n "
                       f"å°†å°è¯•æ•°æ®è¿ç§»ï¼ˆåªå¤„ç†æ–°å±æ€§ï¼Œä¸å¤„ç†ç±»å‹å˜æ›´ï¼Œæ”¹åç­‰ç­‰æƒ…å†µï¼‰ï¼š")

        # todo è°ƒç”¨è‡ªå®šä¹‰ç‰ˆæœ¬è¿ç§»ä»£ç ï¼ˆdefine_migrationï¼‰

        # æ£€æŸ¥æ˜¯å¦æœ‰å±æ€§è¢«åˆ é™¤
        for prop_name in dtypes_in_db.fields:
            if prop_name not in new_dtypes.fields:
                logger.warning(f"âš ï¸ [ğŸ’¾Redis][{self._name}ç»„ä»¶] "
                               f"æ•°æ®åº“ä¸­çš„å±æ€§ {prop_name} åœ¨æ–°çš„ç»„ä»¶å®šä¹‰ä¸­ä¸å­˜åœ¨ï¼Œå¦‚æœæ”¹åäº†éœ€è¦æ‰‹åŠ¨è¿ç§»ï¼Œ"
                               f"é»˜è®¤ä¸¢å¼ƒè¯¥å±æ€§æ•°æ®ã€‚")

        # å¤šå‡ºæ¥çš„åˆ—å†æ¬¡æŠ¥è­¦å‘Šï¼Œç„¶åå¿½ç•¥
        io = self._backend.io
        rows = io.keys(self._key_prefix + '*')
        props = dict(self._component_cls.properties_)  # type: dict[str, Property]
        added = 0
        for prop_name in new_dtypes.fields:
            if prop_name not in dtypes_in_db.fields:
                logger.warning(f"âš ï¸ [ğŸ’¾Redis][{self._name}ç»„ä»¶] "
                               f"æ–°çš„ä»£ç å®šä¹‰ä¸­å¤šå‡ºå±æ€§ {prop_name}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼å¡«å……ã€‚")
                default = props[prop_name].default
                if default is None:
                    logger.error(f"âš ï¸ [ğŸ’¾Redis][{self._name}ç»„ä»¶] "
                                 f"è¿ç§»æ—¶å°è¯•æ–°å¢ {prop_name} å±æ€§å¤±è´¥ï¼Œè¯¥å±æ€§æ²¡æœ‰é»˜è®¤å€¼ï¼Œæ— æ³•æ–°å¢ã€‚")
                    raise ValueError("è¿ç§»å¤±è´¥")
                pipe = io.pipeline()
                for row in rows:
                    pipe.hset(row, prop_name, default)
                pipe.execute()
                added += 1
        logger.warning(f"âœ… [ğŸ’¾Redis][{self._name}ç»„ä»¶] æ–°å±æ€§å¢åŠ å®Œæˆï¼Œå…±å¤„ç†{len(rows)}è¡Œ * "
                       f"{added}ä¸ªå±æ€§ã€‚")

    def begin_transaction(self):
        super().begin_transaction()
        self._autoinc = -1
        self._trans_pipe = self._backend.aio.pipeline(transaction=True)
        # å¼ºåˆ¶pipelineè¿›å…¥ç«‹å³æ¨¡å¼ï¼Œä¸ç„¶å½“æˆ‘ä»¬éœ€è¦è¯»å–æœªé”å®šçš„indexæ—¶ï¼Œä¼šä¸è¿”å›ç»“æœ
        self._trans_pipe.watching = True

    async def end_transaction(self, discard):
        # å¹¶å®ç°äº‹åŠ¡æäº¤çš„æ“ä½œï¼Œå°†_updatesä¸­çš„å‘½ä»¤å†™å…¥äº‹åŠ¡
        if discard:
            self._trans_pipe.discard()
            self._trans_pipe = None
            return True

        pipe = self._trans_pipe

        # å¯¹unique indexè¿›è¡Œæœ€ç»ˆæ£€æŸ¥ï¼Œä¹‹å‰è™½ç„¶æ£€æŸ¥è¿‡ï¼Œä½†æ²¡æœ‰lock indexï¼Œ
        # æ­¤æ¬¡æ£€æŸ¥ä¼šé”å®šindexï¼Œåœ¨æœ€åæ‰é”å®šindexå¯ä»¥é™ä½äº‹åŠ¡å†²çªæ¦‚ç‡
        locked_indexes = set()
        for idx in self._component_cls.uniques_:
            for cmd, _, old_row, new_row in self._updates:
                if (cmd == 'update' and old_row[idx] != new_row[idx]) or cmd == 'insert':
                    if idx not in locked_indexes:
                        await pipe.watch(self._idx_prefix + idx)
                        locked_indexes.add(idx)
                    if len(await self._backend_query(idx, new_row[idx].item(), limit=1)) > 0:
                        self._trans_pipe = None
                        await pipe.aclose()
                        raise RaceCondition()

        # æ‰§è¡Œäº‹åŠ¡
        pipe.multi()
        for cmd, row_id, old_row, new_row in self._updates:
            if cmd == 'delete':
                row_key = self._key_prefix + str(row_id)
                pipe.delete(row_key)
                for idx_name, str_type in self._component_cls.indexes_.items():
                    idx_key = self._idx_prefix + idx_name
                    if str_type:
                        pipe.zrem(idx_key, f'{old_row[idx_name]}:{row_id}')
                    else:
                        pipe.zrem(idx_key, row_id)
            else:
                # æ’å…¥/æ›´æ–°
                row_key = self._key_prefix + str(row_id)
                dict_row = dict(zip(new_row.dtype.names, new_row.tolist()))
                pipe.hset(row_key, mapping=dict_row)
                for idx_name, str_type in self._component_cls.indexes_.items():
                    idx_key = self._idx_prefix + idx_name
                    if str_type:
                        # å…ˆåˆ é™¤è€æ•°æ®
                        if cmd == 'update':
                            pipe.zrem(idx_key, f'{old_row[idx_name]}:{row_id}')
                        pipe.zadd(idx_key, {f'{dict_row[idx_name]}:{row_id}': 0})
                    else:
                        pipe.zadd(idx_key, {str(row_id): dict_row[idx_name]})

        try:
            await pipe.execute()
        except redis.WatchError:
            self._trans_pipe = None
            raise RaceCondition()
        self._trans_pipe = None
        return True

    async def _backend_get(self, row_id: int):
        # è·å–è¡Œæ•°æ®çš„æ“ä½œ
        key = self._key_prefix + str(row_id)
        pipe = self._trans_pipe

        # åŒæ—¶è¦è®©ä¹è§‚é”é”å®šè¯¥è¡Œ
        await pipe.watch(key)
        # è¿”å›å€¼è¦é€šè¿‡dict_to_rowåŒ…è£¹ä¸‹
        row = await pipe.hgetall(key)
        if row:
            return self._component_cls.dict_to_row(row)
        else:
            return None

    async def _backend_get_max_id(self):
        if self._autoinc >= 0:
            return self._autoinc + sum([1 for cmd, _, _, _ in self._updates if cmd == 'insert'])

        # è·å–æœ€å¤§idçš„æ“ä½œ
        idx_key = self._idx_prefix + 'id'
        pipe = self._trans_pipe

        max_score = await pipe.zrange(idx_key, 0, 0, desc=True, withscores=True)
        max_score = max_score[0][1] if max_score else 0
        self._autoinc = max_score
        return max_score

    async def _backend_query(self, index_name: str, left, right=None, limit=10, desc=False):
        # èŒƒå›´æŸ¥è¯¢çš„æ“ä½œï¼Œè¿”å›List[int] of row_idã€‚å¦‚æœä½ çš„æ•°æ®åº“åŒæ—¶è¿”å›äº†æ•°æ®ï¼Œå¯ä»¥å­˜åˆ°_cacheä¸­
        idx_key = self._idx_prefix + index_name
        pipe = self._trans_pipe

        if right is None:
            right = left
        if desc:
            left, right = right, left

        # å¯¹äºstrç±»å‹æŸ¥è¯¢ï¼Œè¦ç”¨[å¼€å§‹
        str_type = self._component_cls.indexes_[index_name]
        by_lex = False
        if str_type:
            assert type(left) is str and type(right) is str, \
                f"å­—ç¬¦ä¸²ç±»å‹ç´¢å¼•`{index_name}`çš„æŸ¥è¯¢(left={left}, {type(left)})å˜é‡ç±»å‹å¿…é¡»æ˜¯str"
            if not left.startswith(('(', '[')):
                left = f'[{left}'
            if not right.startswith(('(', '[')):
                right = f'[{right}'

            if left == right:  # å¦‚æœæ˜¯ç²¾ç¡®æŸ¥è¯¢
                left = f'{left}:'  # name:id å½¢å¼ï¼Œæ‰€ä»¥:ä½œä¸ºç»“å°¾æ ‡è¯†ç¬¦
                right = f'{right};'  # ';' = 3B, ':' = 3A

            by_lex = True

        row_ids = await pipe.zrange(idx_key, left, right, desc=desc, offset=0, num=limit,
                                    byscore=not by_lex, bylex=by_lex)

        if str_type:
            row_ids = [vk.split(':')[-1] for vk in row_ids]

        # æœªæŸ¥è¯¢åˆ°æ•°æ®æ—¶è¿”å›[]
        return list(map(int, row_ids))
