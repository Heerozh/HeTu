"""
@author: Heerozh (Zhang Jianhao)
@copyright: Copyright 2024, Heerozh. All rights reserved.
@license: Apache2.0 å¯ç”¨ä½œå•†ä¸šé¡¹ç›®ï¼Œå†éšä¾¿æ‰¾ä¸ªè§’è½æåŠç”¨åˆ°äº†æ­¤é¡¹ç›® :D
@email: heeroz@gmail.com
"""
import numpy as np
import random
import hashlib
import redis
import asyncio
from datetime import datetime, timedelta
from sanic.log import logger
from ...common import Singleton
from ..component import BaseComponent, Property
from .base import ComponentTable


def get_coroutine_executor():
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        return asyncio.run
    else:
        return loop.run_until_complete


class RedisBackend:
    def __init__(self, config: dict):
        # åŒæ­¥ioè¿æ¥, å¼‚æ­¥ioè¿æ¥, åªè¯»ioè¿æ¥
        self.io = redis.from_url(config['master'])
        self.aio = redis.asyncio.from_url(config['master'])
        servants = config.get('servants', [])
        if servants:
            self.replicas = [redis.asyncio.from_url(url) for url in config['servants']]
        else:
            self.replicas = [self.aio]

        # é…ç½®keyspaceé€šçŸ¥
        run = get_coroutine_executor()
        for replica in self.replicas:
            run(replica.config_set('notify-keyspace-events', 'Kghz'))

    def rnd_replica(self):
        """æ¯ä¸ªwebsocketè¿æ¥è·å¾—ä¸€ä¸ªéšæœºçš„replicaè¿æ¥ï¼Œç”¨äºè¯»å–è®¢é˜…"""
        i = random.randint(0, len(self.replicas))
        return i, self.replicas[i]


class RedisComponentTable(ComponentTable):
    """
    ä½¿ç”¨rediså®ç°çš„Componentåç«¯ã€‚

    å‚è€ƒï¼š
    redis-pyååé‡åŸºå‡†ï¼š
    syncè°ƒç”¨ï¼šå•è¿›ç¨‹ï¼š1200/sï¼Œ10è¿›ç¨‹ç†è®ºä¸Š12 Kopsï¼Œç¬¦åˆhiredisåŸºå‡†æµ‹è¯•
    asyncè°ƒç”¨ï¼šå•è¿›ç¨‹+Semaphoreé™åˆ¶100åç¨‹ï¼š6000/sï¼Œ å‚è€ƒQPS 100,000

    ä½¿ç”¨ä»¥ä¸‹keysï¼š
    instance_name:component_name.{CLU0}:id:1~n
    instance_name:component_name.{CLU0}:index:key~
    instance_name:component_name:meta
    """

    def __init__(self, component_cls: type[BaseComponent], instance_name, cluster_id,
                 backend: RedisBackend):
        super().__init__(component_cls, instance_name, cluster_id, backend)
        component_cls.hosted_ = self
        # redis keyå
        hash_tag = f'{{CLU{cluster_id}}}'
        # ä¸èƒ½ç”¨component_cls.__name__ å¯èƒ½æ˜¯jsonåŠ è½½çš„åå­—ä¸å¯¹
        self._name = component_cls.components_name_
        self._root_prefix = f'{instance_name}:{self._name}:'
        self._key_prefix = f'{self._root_prefix}.{hash_tag}:id:'
        self._idx_prefix = f'{self._root_prefix}.{hash_tag}:index:'
        self._lock_key = f'{self._root_prefix}:init_lock'
        self._meta_key = f'{self._root_prefix}:meta'
        self._trans_pipe = None
        # æ£€æµ‹metaä¿¡æ¯ï¼Œç„¶ååšå¯¹åº”å¤„ç†
        self.check_meta()

    def check_meta(self):
        """
        æ£€æŸ¥metaä¿¡æ¯ï¼Œç„¶ååšå¯¹åº”å¤„ç†
        metaæ ¼å¼:
        json: è¡¨çš„ç»“æ„ä¿¡æ¯
        version: jsonçš„hash
        cluster_id: æ‰€å±ç°‡id
        last_index_rebuild: ä¸Šæ¬¡é‡å»ºç´¢å¼•æ—¶é—´
        """
        io = self._backend.io
        lock = io.lock(self._lock_key)
        lock.acquire(blocking=True)

        # è·å–rediså·²å­˜çš„è¡¨ä¿¡æ¯
        meta = io.hgetall(self._meta_key)
        if not meta:
            meta = self._create_emtpy()
        else:
            version = hashlib.md5(self._component_cls.json_.encode("utf-8")).hexdigest()
            # å¦‚æœcluster_idæ”¹å˜ï¼Œåˆ™è¿ç§»æ”¹keyå
            if int(meta['cluster_id']) != self._cluster_id:
                self._migration_cluster_id(old=int(meta['cluster_id']))

            # å¦‚æœç‰ˆæœ¬ä¸ä¸€è‡´ï¼Œè¡¨ç»“æ„å¯èƒ½æœ‰å˜åŒ–ï¼Œä¹Ÿå¯èƒ½åªæ˜¯æ”¹æƒé™ï¼Œæ€»ä¹‹è°ƒç”¨è¿ç§»ä»£ç 
            if meta['version'] != version:
                self._migration_schema(old=meta['json'])

        # é‡å»ºæ•°æ®ï¼Œæ¯æ¬¡å¯åŠ¨é—´éš”è¶…è¿‡1å°æ—¶å°±é‡å»º
        last_index_rebuild = datetime.fromisoformat(meta.get('last_index_rebuild'))
        now = datetime.now().astimezone()
        if last_index_rebuild <= now - timedelta(hours=1):
            # å¦‚æœéæŒä¹…åŒ–è¡¨ï¼Œåˆ™æ¯æ¬¡å¯åŠ¨æ¸…ç©º
            if not self._component_cls.persist_:
                logger.info(f"âŒš [ğŸ’¾Redis] {self._name}è¡¨ æ— éœ€æŒä¹…åŒ–ï¼Œæ¸…ç©ºä¸­...")
                del_keys = io.keys(self._root_prefix + '*')
                map(io.delete, del_keys)

            # é‡å»ºç´¢å¼•ï¼Œå¦‚æœå·²å¤„ç†è¿‡äº†å°±ä¸å¤„ç†
            self._rebuild_index()
            # å†™å…¥metaä¿¡æ¯
            io.hset(self._meta_key, 'last_index_rebuild', now.isoformat())

        lock.release()

    def _create_emtpy(self):
        logger.info(f"â„¹ï¸ [ğŸ’¾Redis] {self._name}è¡¨æ— metaä¿¡æ¯ï¼Œæ­£åœ¨é‡æ–°åˆ›å»º...")

        # åªéœ€è¦å†™å…¥metaï¼Œå…¶ä»–çš„_rebuild_indexä¼šåˆ›å»º
        meta = {
            'json': self._component_cls.json_,
            'version': hashlib.md5(self._component_cls.json_.encode("utf-8")).hexdigest(),
            'cluster_id': self._cluster_id,
            'last_index_rebuild': '2024-06-19T03:41:18.682529+08:00'
        }
        self._backend.io.hmset(self._meta_key, meta)
        return meta

    def _rebuild_index(self):
        logger.info(f"âŒš [ğŸ’¾Redis] {self._name}è¡¨ æ­£åœ¨é‡å»ºç´¢å¼•...")
        io = self._backend.io
        rows = io.keys(self._key_prefix + '*')
        for idx_name in self._component_cls.indexes_:
            idx_key = self._idx_prefix + idx_name
            # å…ˆåˆ é™¤æ‰€æœ‰_idx_keyå¼€å¤´çš„ç´¢å¼•
            io.delete(idx_key)
            # é‡å»ºæ‰€æœ‰ç´¢å¼•ï¼Œä¸ç®¡uniqueè¿˜æ˜¯indexéƒ½æ˜¯sset
            pipe = io.pipeline()
            row_ids = []
            for row in rows:
                row_id = row.split(':')[-1]
                row_ids.append(row_id)
                io.hget(row, idx_name)
            values = pipe.execute()
            # zadd({member:score})å…¶å®æ˜¯setçš„æ„æ€
            io.zadd(idx_key, dict(zip(row_ids, values)))

    def _migration_cluster_id(self, old):
        logger.warning(f"âš ï¸ [ğŸ’¾Redis] {self._name}è¡¨ cluster_id ç”± {old} å˜æ›´ä¸º {self._cluster_id}ï¼Œ"
                       f"å°†å°è¯•è¿ç§»clusteræ•°æ®...")
        # é‡å‘½åkey
        old_hash_tag = f'{{CLU{old}}}'
        new_hash_tag = f'{{CLU{self._cluster_id}}}'
        old_prefix = f'{self._root_prefix}.{old_hash_tag}:'
        old_prefix_len = len(old_prefix)
        new_prefix = f'{self._root_prefix}.{new_hash_tag}:'

        io = self._backend.io
        old_keys = io.keys(old_prefix + '*')
        for old_key in old_keys:
            new_key = new_prefix + old_key[old_prefix_len:]
            io.rename(new_key, new_key)
        # æ›´æ–°meta
        io.hset(self._meta_key, 'cluster_id', self._cluster_id)

    def _migration_schema(self, old):
        """å¦‚æœæ•°æ®åº“ä¸­çš„å±æ€§å’Œå®šä¹‰ä¸ä¸€è‡´ï¼Œå°è¯•è¿›è¡Œç®€å•è¿ç§»ï¼Œå¯ä»¥å¤„ç†å±æ€§æ›´åä»¥å¤–çš„æƒ…å†µã€‚"""
        # åŠ è½½è€çš„è¡¨
        old_comp_cls = BaseComponent.load_json(old)

        # åªæœ‰propertiesåå­—å’Œç±»å‹å˜æ›´æ‰è¿ç§»
        dtypes_in_db = old_comp_cls.dtypes
        new_dtypes = self._component_cls.dtypes
        if dtypes_in_db == new_dtypes:
            return

        logger.warning(f"âš ï¸ [ğŸ’¾Redis] {self._name}è¡¨ ä»£ç å®šä¹‰ä¸å­˜æ¡£ä¸ä¸€è‡´ï¼Œ"
                       f"å­˜æ¡£ï¼š\n"
                       f"{dtypes_in_db}\n"
                       f"ä»£ç å®šä¹‰çš„ï¼š\n"
                       f"{new_dtypes}\n "
                       f"å°†å°è¯•è¿ç§»æ•°æ®ï¼š")

        # todo è°ƒç”¨è‡ªå®šä¹‰ç‰ˆæœ¬è¿ç§»ä»£ç ï¼ˆdefine_migrationï¼‰

        # æ£€æŸ¥æ˜¯å¦æœ‰å±æ€§è¢«åˆ é™¤
        for prop_name in dtypes_in_db.fields:
            if prop_name not in new_dtypes.fields:
                logger.warning(f"âš ï¸ [ğŸ’¾Redis] {self._name}è¡¨ "
                               f"æ–°çš„ä»£ç å®šä¹‰ä¸­ç¼ºå°‘å±æ€§ {prop_name}ï¼Œå¦‚æœæ”¹åäº†éœ€è¦æ‰‹åŠ¨è¿ç§»ï¼Œ"
                               f"é»˜è®¤ä¸¢å¼ƒè¯¥å±æ€§æ•°æ®ã€‚")

        # å¤šå‡ºæ¥çš„åˆ—å†æ¬¡æŠ¥è­¦å‘Šï¼Œç„¶åå¿½ç•¥
        io = self._backend.io
        props = dict(self._component_cls.properties_)  # type: dict[str, Property]
        for prop_name in new_dtypes.fields:
            if prop_name not in dtypes_in_db.fields:
                logger.warning(f"âš ï¸ [ğŸ’¾Redis] {self._name}è¡¨ "
                               f"æ–°çš„ä»£ç å®šä¹‰ä¸­å¤šå‡ºå±æ€§ {prop_name}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼å¡«å……ã€‚")
                default = props[prop_name].default
                if default is None:
                    logger.error(f"âš ï¸ [ğŸ’¾Redis] {self._name}è¡¨ "
                                 f"è¿ç§»æ—¶å°è¯•æ–°å¢ {prop_name} å±æ€§å¤±è´¥ï¼Œè¯¥å±æ€§æ²¡æœ‰é»˜è®¤å€¼ï¼Œæ— æ³•æ–°å¢ã€‚")
                    raise ValueError("è¿ç§»å¤±è´¥")
                pipe = io.pipeline()
                rows = io.keys(self._key_prefix + '*')
                for row in rows:
                    pipe.hset(row, prop_name, default)
                pipe.execute()

    def begin_transaction(self):
        super().begin_transaction()
        self._trans_pipe = self._backend.aio.pipeline(transaction=True)

    async def end_transaction(self, discard):
        # å¹¶å®ç°äº‹åŠ¡æäº¤çš„æ“ä½œï¼Œå°†_updatesä¸­çš„å‘½ä»¤å†™å…¥äº‹åŠ¡
        if discard:
            self._trans_pipe.discard()
            return True

        pipe = self._trans_pipe
        pipe.multi()

        # updatesæ˜¯ä¸€ä¸ªList[(row_id, row)]ï¼Œrow_idä¸ºNoneè¡¨ç¤ºæ’å…¥ï¼Œå¦åˆ™ä¸ºæ›´æ–°ï¼Œrowä¸ºNoneè¡¨ç¤ºåˆ é™¤
        for row_id, row in self._updates:
            if row is None:
                # åˆ é™¤
                row_key = self._key_prefix + str(row_id)
                pipe.delete(row_key)
                for index in self._component_cls.indexes_:
                    idx_key = self._idx_prefix + index
                    pipe.zrem(idx_key, row_id)
            else:
                # æ’å…¥/æ›´æ–°
                row_key = self._key_prefix + str(row.id)
                dict_row = dict(zip(row.dtype.names, row))
                pipe.hset(row_key, mapping=dict_row)
                for index in self._component_cls.indexes_:
                    idx_key = self._idx_prefix + index
                    pipe.zadd(idx_key, {row.id: dict_row[index]})

        try:
            await pipe.execute()
        except redis.WatchError:
            return False
        raise True

    async def _backend_get(self, row_id: int):
        # è·å–è¡Œæ•°æ®çš„æ“ä½œ
        key = self._key_prefix + str(row_id)
        pipe = self._trans_pipe

        # åŒæ—¶è¦è®©ä¹è§‚é”é”å®šè¯¥è¡Œ
        await pipe.watch(key)
        # è¿”å›å€¼è¦é€šè¿‡dict_to_rowåŒ…è£¹ä¸‹
        row = await pipe.hgetall(key)
        if row:
            return self._component_cls.dict_to_row(row)
        else:
            return None

    async def _backend_get_max_id(self):
        # è·å–æœ€å¤§idçš„æ“ä½œ
        idx_key = self._idx_prefix + 'id'
        pipe = self._trans_pipe

        # åŒæ—¶è¦è®©ä¹è§‚é”é”å®šIndex
        await pipe.watch(idx_key)
        max_score = await pipe.zrange(idx_key, 0, 0, desc=True, withscores=True)
        max_score = max_score[0][1] if max_score else 0

        raise max_score

    async def _backend_query(self, index_name: str, left, right=None, limit=10, desc=False):
        # èŒƒå›´æŸ¥è¯¢çš„æ“ä½œï¼Œè¿”å›List[int] of row_idã€‚å¦‚æœä½ çš„æ•°æ®åº“åŒæ—¶è¿”å›äº†æ•°æ®ï¼Œå¯ä»¥å­˜åˆ°_cacheä¸­
        idx_key = self._idx_prefix + index_name
        pipe = self._trans_pipe

        # è¦è®©ä¹è§‚é”é”å®šIndex
        await pipe.watch(idx_key)
        if desc:
            left, right = min(left, right), max(left, right)
        row_ids = await pipe.zrange(idx_key, left, right, byscore=True, desc=desc,
                                    offset=0, num=limit)

        # åŒæ—¶é”å®šæ‰€æœ‰è¯»å–çš„è¡Œ
        # row_keys = [self._key_prefix + row_id for row_id in row_ids]
        # await asyncio.gather(*[pipe.watch(row_key) for row_key in row_keys])
        # rtn = await asyncio.gather(*[pipe.hgetall(row_key) for row_key in row_keys])

        # æœªæŸ¥è¯¢åˆ°æ•°æ®æ—¶è¿”å›[]
        return row_ids




