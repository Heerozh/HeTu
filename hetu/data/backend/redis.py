"""
@author: Heerozh (Zhang Jianhao)
@copyright: Copyright 2024, Heerozh. All rights reserved.
@license: Apache2.0 å¯ç”¨ä½œå•†ä¸šé¡¹ç›®ï¼Œå†éšä¾¿æ‰¾ä¸ªè§’è½æåŠç”¨åˆ°äº†æ­¤é¡¹ç›® :D
@email: heeroz@gmail.com
"""
import numpy as np
import random
import hashlib
import redis
from datetime import datetime, timedelta
from sanic.log import logger
from ...common import Singleton
from ..component import BaseComponent, Property
from .base import ComponentTable


class RedisBackend:
    def __init__(self, config: dict):
        # åŒæ­¥ioè¿æ¥, å¼‚æ­¥ioè¿æ¥, åªè¯»ioè¿æ¥
        self.io = redis.from_url(config['master'])
        self.aio = redis.asyncio.from_url(config['master'])
        self.replicas = [redis.asyncio.from_url(url) for url in config['servants']]

    def rnd_replica(self):
        return self.replicas[random.randint(0, len(self.replicas))]


class RedisComponentTable(ComponentTable):
    """
    ä½¿ç”¨rediså®ç°çš„Componentåç«¯ã€‚

    å‚è€ƒï¼š
    redis-pyååé‡åŸºå‡†ï¼š
    syncè°ƒç”¨ï¼šå•è¿›ç¨‹ï¼š1200/sï¼Œ10è¿›ç¨‹ç†è®ºä¸Š12 Kopsï¼Œç¬¦åˆhiredisåŸºå‡†æµ‹è¯•
    asyncè°ƒç”¨ï¼šå•è¿›ç¨‹+Semaphoreé™åˆ¶100åç¨‹ï¼š6000/sï¼Œ å‚è€ƒQPS 100,000

    ä½¿ç”¨ä»¥ä¸‹keysï¼š
    instance_name:component_name.{CLU0}:id:1~n
    instance_name:component_name.{CLU0}:index:key~
    instance_name:component_name:meta
    """

    @classmethod
    def create_schema(cls):
        raise NotImplementedError

    def __init__(self, component_cls: type[BaseComponent], instance_name, cluster_id,
                 backend: RedisBackend):
        """backend: è¦ä¼ """
        super().__init__(component_cls, instance_name, cluster_id, backend)
        component_cls.hosted_ = self
        # redis keyå
        hash_tag = f'{{CLU{cluster_id}}}'
        # ä¸èƒ½ç”¨component_cls.__name__ å¯èƒ½æ˜¯jsonåŠ è½½çš„åå­—ä¸å¯¹
        self._name = component_cls.components_name_
        self._root_prefix = f'{instance_name}:{self._name}:'
        self._key_prefix = f'{self._root_prefix}.{hash_tag}:id:'
        self._idx_prefix = f'{self._root_prefix}.{hash_tag}:index:'
        self._lock_key = f'{self._root_prefix}:init_lock'
        self._meta_key = f'{self._root_prefix}:meta'

        # éœ€è¦äº‹åŠ¡ç»“æŸæ—¶ï¼Œä¸€èµ·æ‰§è¡Œçš„å†™å…¥æ“ä½œ
        self.updates = []

        # æ£€æµ‹metaä¿¡æ¯ï¼Œç„¶ååšå¯¹åº”å¤„ç†
        self.check_meta()

    def check_meta(self):
        """
        æ£€æŸ¥metaä¿¡æ¯ï¼Œç„¶ååšå¯¹åº”å¤„ç†
        metaæ ¼å¼:
        json: è¡¨çš„ç»“æ„ä¿¡æ¯
        increment: è‡ªå¢id
        version: jsonçš„hash
        cluster_id: æ‰€å±ç°‡id
        last_index_rebuild: ä¸Šæ¬¡é‡å»ºç´¢å¼•æ—¶é—´
        """
        io = self._backend.io
        lock = io.lock(self._lock_key)
        lock.acquire(blocking=True)

        # è·å–rediså·²å­˜çš„è¡¨ä¿¡æ¯
        meta = io.hgetall(self._meta_key)
        if not meta:
            meta = self._create_emtpy()
        else:
            version = hashlib.md5(self._component_cls.json_.encode("utf-8")).hexdigest()
            # å¦‚æœcluster_idæ”¹å˜ï¼Œåˆ™è¿ç§»æ”¹keyå
            if int(meta['cluster_id']) != self._cluster_id:
                self._migration_cluster_id(old=int(meta['cluster_id']))

            # å¦‚æœç‰ˆæœ¬ä¸ä¸€è‡´ï¼Œè¡¨ç»“æ„å¯èƒ½æœ‰å˜åŒ–ï¼Œä¹Ÿå¯èƒ½åªæ˜¯æ”¹æƒé™ï¼Œæ€»ä¹‹è°ƒç”¨è¿ç§»ä»£ç 
            if meta['version'] != version:
                self._migration_schema(old=meta['json'])

        # é‡å»ºæ•°æ®ï¼Œæ¯æ¬¡å¯åŠ¨é—´éš”è¶…è¿‡1å°æ—¶å°±é‡å»º
        last_index_rebuild = datetime.fromisoformat(meta.get('last_index_rebuild'))
        now = datetime.now().astimezone()
        if last_index_rebuild <= now - timedelta(hours=1):
            # å¦‚æœéæŒä¹…åŒ–è¡¨ï¼Œåˆ™æ¯æ¬¡å¯åŠ¨æ¸…ç©º
            if not self._component_cls.persist_:
                logger.info(f"âŒš [ğŸ’¾Redis] {self._name}è¡¨ æ— éœ€æŒä¹…åŒ–ï¼Œæ¸…ç©ºä¸­...")
                del_keys = io.keys(self._root_prefix + '*')
                map(io.delete, del_keys)

            # é‡å»ºç´¢å¼•ï¼Œå¦‚æœå·²å¤„ç†è¿‡äº†å°±ä¸å¤„ç†
            self._rebuild_index()
            # å†™å…¥metaä¿¡æ¯
            io.hset(self._meta_key, 'last_index_rebuild', now.isoformat())

        lock.release()

    def _create_emtpy(self):
        logger.info(f"â„¹ï¸ [ğŸ’¾Redis] {self._name}è¡¨æ— metaä¿¡æ¯ï¼Œæ­£åœ¨é‡æ–°åˆ›å»º...")

        # åªéœ€è¦å†™å…¥metaï¼Œå…¶ä»–çš„_rebuild_indexä¼šåˆ›å»º
        meta = {
            'json': self._component_cls.json_,
            'increment': 0,
            'version': hashlib.md5(self._component_cls.json_.encode("utf-8")).hexdigest(),
            'cluster_id': self._cluster_id,
            'last_index_rebuild': '2024-06-19T03:41:18.682529+08:00'
        }
        self._backend.io.hmset(self._meta_key, meta)
        return meta

    def _rebuild_index(self):
        logger.info(f"âŒš [ğŸ’¾Redis] {self._name}è¡¨ æ­£åœ¨é‡å»ºç´¢å¼•...")
        io = self._backend.io
        rows = io.keys(self._key_prefix + '*')
        for key, prop in self._component_cls.properties_:
            if prop.unique or prop.index:
                db_key = self._idx_prefix + key
                # å…ˆåˆ é™¤æ‰€æœ‰_idx_keyå¼€å¤´çš„ç´¢å¼•
                io.delete(db_key)
                # é‡å»ºæ‰€æœ‰ç´¢å¼•ï¼Œä¸ç®¡uniqueè¿˜æ˜¯indexéƒ½æ˜¯sset
                pipe = io.pipeline()
                row_ids = []
                for row in rows:
                    row_id = row.split(':')[-1]
                    row_ids.append(row_id)
                    io.hget(row, key)
                values = pipe.execute()
                # zadd({member:score})å…¶å®æ˜¯setçš„æ„æ€
                io.zadd(db_key, dict(zip(row_ids, values)))

    def _migration_cluster_id(self, old):
        logger.warning(f"âš ï¸ [ğŸ’¾Redis] {self._name}è¡¨ cluster_id ç”± {old} å˜æ›´ä¸º {self.cluster_id}ï¼Œ"
                       f"å°†å°è¯•è¿ç§»clusteræ•°æ®...")
        # é‡å‘½åkey
        old_hash_tag = f'{{CLU{old}}}'
        new_hash_tag = f'{{CLU{self._cluster_id}}}'
        old_prefix = f'{self._root_prefix}.{old_hash_tag}:'
        old_prefix_len = len(old_prefix)
        new_prefix = f'{self._root_prefix}.{new_hash_tag}:'

        io = self._backend.io
        old_keys = io.keys(old_prefix + '*')
        for old_key in old_keys:
            new_key = new_prefix + old_key[old_prefix_len:]
            io.rename(new_key, new_key)
        # æ›´æ–°meta
        io.hset(self._meta_key, 'cluster_id', self._cluster_id)

    def _migration_schema(self, old):
        """å¦‚æœæ•°æ®åº“ä¸­çš„å±æ€§å’Œå®šä¹‰ä¸ä¸€è‡´ï¼Œå°è¯•è¿›è¡Œç®€å•è¿ç§»ï¼Œå¯ä»¥å¤„ç†å±æ€§æ›´åä»¥å¤–çš„æƒ…å†µã€‚"""
        # åŠ è½½è€çš„è¡¨
        old_comp_cls = BaseComponent.load_json(old)

        # åªæœ‰propertiesåå­—å’Œç±»å‹å˜æ›´æ‰è¿ç§»
        dtypes_in_db = old_comp_cls.dtypes
        new_dtypes = self._component_cls.dtypes
        if dtypes_in_db == new_dtypes:
            return

        logger.warning(f"âš ï¸ [ğŸ’¾Redis] {self._name}è¡¨ ä»£ç å®šä¹‰ä¸å­˜æ¡£ä¸ä¸€è‡´ï¼Œ"
                       f"å­˜æ¡£ï¼š\n"
                       f"{dtypes_in_db}\n"
                       f"ä»£ç å®šä¹‰çš„ï¼š\n"
                       f"{new_dtypes}\n "
                       f"å°†å°è¯•è¿ç§»æ•°æ®ï¼š")

        # todo è°ƒç”¨è‡ªå®šä¹‰ç‰ˆæœ¬è¿ç§»ä»£ç ï¼ˆdefine_migrationï¼‰

        # æ£€æŸ¥æ˜¯å¦æœ‰å±æ€§è¢«åˆ é™¤
        for prop_name in dtypes_in_db.fields:
            if prop_name not in new_dtypes.fields:
                logger.warning(f"âš ï¸ [ğŸ’¾Redis] {self._name}è¡¨ "
                               f"æ–°çš„ä»£ç å®šä¹‰ä¸­ç¼ºå°‘å±æ€§ {prop_name}ï¼Œå¦‚æœæ”¹åäº†éœ€è¦æ‰‹åŠ¨è¿ç§»ï¼Œ"
                               f"é»˜è®¤ä¸¢å¼ƒè¯¥å±æ€§æ•°æ®ã€‚")

        # å¤šå‡ºæ¥çš„åˆ—å†æ¬¡æŠ¥è­¦å‘Šï¼Œç„¶åå¿½ç•¥
        io = self._backend.io
        props = dict(self._component_cls.properties_)  # type: dict[str, Property]
        for prop_name in new_dtypes.fields:
            if prop_name not in dtypes_in_db.fields:
                logger.warning(f"âš ï¸ [ğŸ’¾Redis] {self._name}è¡¨ "
                               f"æ–°çš„ä»£ç å®šä¹‰ä¸­å¤šå‡ºå±æ€§ {prop_name}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼å¡«å……ã€‚")
                default = props[prop_name].default
                if default is None:
                    logger.error(f"âš ï¸ [ğŸ’¾Redis] {self._name}è¡¨ "
                                 f"è¿ç§»æ—¶å°è¯•æ–°å¢ {prop_name} å±æ€§å¤±è´¥ï¼Œè¯¥å±æ€§æ²¡æœ‰é»˜è®¤å€¼ï¼Œæ— æ³•æ–°å¢ã€‚")
                    raise ValueError("è¿ç§»å¤±è´¥")
                pipe = io.pipeline()
                rows = io.keys(self._key_prefix + '*')
                for row in rows:
                    pipe.hset(row, prop_name, default)
                pipe.execute()

    async def end_transaction(self):
        # ç»§æ‰¿ï¼Œå¹¶å®ç°äº‹åŠ¡æäº¤çš„æ“ä½œï¼Œå°†_updatesä¸­çš„å‘½ä»¤å†™å…¥äº‹åŠ¡
        # updatesæ˜¯ä¸€ä¸ªList[(row_id, row)]ï¼Œrow_idä¸ºNoneè¡¨ç¤ºæ’å…¥ï¼Œå¦åˆ™ä¸ºæ›´æ–°ï¼Œrowä¸ºNoneè¡¨ç¤ºåˆ é™¤
        # å¦‚æœindexæ˜¯ç‹¬ç«‹åˆ†ç¦»çš„ï¼Œå†™å…¥æ—¶è¦åŒæ—¶æ›´æ–°index
        raise NotImplementedError

    async def _backend_get(self, row_id: int):
        # ç»§æ‰¿ï¼Œå¹¶å®ç°è·å–è¡Œæ•°æ®çš„æ“ä½œï¼Œè¿”å›å€¼è¦é€šè¿‡dict_to_rowåŒ…è£¹ä¸‹
        # å¦‚æœä¸å­˜åœ¨è¯¥è¡Œæ•°æ®ï¼Œè¿”å›None
        # åŒæ—¶è¦è®©ä¹è§‚é”é”å®šè¯¥è¡Œã€‚sqlæ˜¯è®°å½•è¯¥è¡Œçš„versionï¼Œç”¨äºåç»­çš„updateæ¡ä»¶
        raise NotImplementedError

    async def _backend_get_max_id(self):
        # ç»§æ‰¿ï¼Œå¹¶å®ç°è·å–æœ€å¤§idçš„æ“ä½œ
        # å¦‚æœè‡ªå¢idæ˜¯æ•°æ®åº“è´Ÿè´£çš„ï¼Œè¿™ä¸ªå¯ä»¥è¿”å›-1ï¼Œåªè¦ä½ end_transactionæ—¶å¤„ç†å³å¯
        # å¦‚æœæœ€å¤§idæ˜¯å•ç‹¬å‚¨å­˜çš„ï¼Œè¦ç”¨ä¹è§‚é”é”å®šè¯¥è¡Œæ•°æ®ï¼Œæˆ–è€…é”å®šidçš„ç´¢å¼•ä¹Ÿå¯ä»¥
        raise NotImplementedError

    async def _backend_query(self, index_name: str, left, right=None, limit=10, desc=False):
        # ç»§æ‰¿ï¼Œå¹¶å®ç°èŒƒå›´æŸ¥è¯¢çš„æ“ä½œï¼Œè¿”å›List[int] of row_idã€‚å¦‚æœä½ çš„æ•°æ®åº“åŒæ—¶è¿”å›äº†æ•°æ®ï¼Œå¯ä»¥å­˜åˆ°_cacheä¸­
        # æœªæŸ¥è¯¢åˆ°æ•°æ®æ—¶è¿”å›[]
        # å¦‚æœindexæ•°æ®æ˜¯ç‹¬ç«‹åˆ†ç¦»çš„ï¼Œè¦ç”¨ä¹è§‚é”é”å®šè¯¥index
        raise NotImplementedError
