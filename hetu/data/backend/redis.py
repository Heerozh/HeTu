"""
@author: Heerozh (Zhang Jianhao)
@copyright: Copyright 2024, Heerozh. All rights reserved.
@license: Apache2.0 å¯ç”¨ä½œå•†ä¸šé¡¹ç›®ï¼Œå†éšä¾¿æ‰¾ä¸ªè§’è½æåŠç”¨åˆ°äº†æ­¤é¡¹ç›® :D
@email: heeroz@gmail.com
"""
import asyncio
import hashlib
import itertools
import logging
import random
import time
import uuid
import warnings

import numpy as np
import redis

from .base import ComponentTransaction, ComponentTable, Backend, BackendTransaction, MQClient
from .base import RaceCondition, HeadLockFailed
from ..component import BaseComponent, Property
from ...common.helper import batched
from ...common.multimap import MultiMap

logger = logging.getLogger('HeTu')
MAX_SUBSCRIBED = 1000


class RedisBackend(Backend):
    """å‚¨å­˜åˆ°Redisåç«¯çš„è¿æ¥ï¼ŒæœåŠ¡å™¨å¯åŠ¨æ—¶ç”±server.pyæ ¹æ®Configåˆå§‹åŒ–ï¼Œå¹¶ä¼ å…¥RedisComponentBackendã€‚"""

    def __init__(self, config: dict):
        super().__init__(config)
        # åŒæ­¥ioè¿æ¥, å¼‚æ­¥ioè¿æ¥, åªè¯»ioè¿æ¥
        self.master_url = config['master']
        self.servant_urls = config.get('servants', [])
        self.io = redis.from_url(self.master_url, decode_responses=True)
        self._aio = redis.asyncio.from_url(self.master_url, decode_responses=True)
        self.dbi = self.io.connection_pool.connection_kwargs['db']
        # è¿æ¥åªè¯»æ•°æ®åº“
        self.replicas = [redis.asyncio.from_url(url, decode_responses=True)
                         for url in self.servant_urls]
        if not self.servant_urls:
            self.servant_urls.append(self.master_url)
            self.replicas.append(self._aio)
        # é™åˆ¶aioè¿è¡Œçš„coroutine
        try:
            self.loop_id = hash(asyncio.get_running_loop())
        except RuntimeError:
            self.loop_id = None

    def configure(self):
        # æ£€æµ‹redisç‰ˆæœ¬
        version = self.io.info('server')['redis_version']
        assert tuple(map(int, version.split("."))) >= (7, 0), "Redisç‰ˆæœ¬è¿‡ä½ï¼Œè‡³å°‘éœ€è¦7.0ç‰ˆæœ¬"
        for url in self.servant_urls:
            version = redis.from_url(url).info('server')['redis_version']
            assert tuple(map(int, version.split("."))) >= (7, 0), "Redisç‰ˆæœ¬è¿‡ä½ï¼Œè‡³å°‘éœ€è¦7.0ç‰ˆæœ¬"
        # æ£€æŸ¥servantsè®¾ç½®
        target_keyspace = 'Kghz'
        for servant_url in self.servant_urls:
            r = redis.from_url(servant_url)
            try:
                # è®¾ç½®keyspaceé€šçŸ¥
                db_keyspace = r.config_get('notify-keyspace-events')['notify-keyspace-events']
                db_keyspace = db_keyspace.replace('A', 'g$lshztxed')
                db_keyspace_new = db_keyspace
                for flag in list(target_keyspace):
                    if flag not in db_keyspace:
                        db_keyspace_new += flag
                if db_keyspace_new != db_keyspace:
                    r.config_set('notify-keyspace-events', db_keyspace_new)
            except (redis.exceptions.NoPermissionError, redis.exceptions.ResponseError):
                logger.warning(
                    f"âš ï¸ [ğŸ’¾Redis] æ— æƒé™è°ƒç”¨æ•°æ®åº“{servant_url}çš„config_setå‘½ä»¤ï¼Œæ•°æ®è®¢é˜…å°†"
                    f"ä¸èµ·æ•ˆã€‚å¯æ‰‹åŠ¨è®¾ç½®é…ç½®æ–‡ä»¶ï¼šnotify-keyspace-events={target_keyspace}")
            # æ£€æŸ¥æ˜¯å¦æ˜¯replicaæ¨¡å¼
            if servant_url != self.master_url:
                db_replica = r.config_get('replica-read-only')
                if db_replica.get('replica-read-only') != 'yes':
                    logger.warning("âš ï¸ [ğŸ’¾Redis] servantå¿…é¡»æ˜¯Read Only Replicaæ¨¡å¼ã€‚"
                                   f"{servant_url} æœªè®¾ç½®replica-read-only=yes")
                # ä¸æ£€æŸ¥replicaof masteråœ°å€ï¼Œå› ä¸ºreplicaofçš„å¯èƒ½æ˜¯å…¶ä»–replicaåœ°å€
            # è€ƒè™‘å¯ä»¥æ£€æŸ¥pubsub client buffè®¾ç½®ï¼Œçœ‹çœ‹èƒ½å¦rediså´©äº†æé†’ä¸‹
            # pubsubå€¼å»ºè®®ä¸º$å‰©ä½™å†…å­˜/é¢„ä¼°åœ¨çº¿æ•°$

    @property
    def aio(self):
        if self.loop_id is None:
            self.loop_id = hash(asyncio.get_running_loop())
        # redis-pyçš„async connectionç”¨çš„pythonçš„steam.connectï¼Œç»‘å®šåˆ°å½“å‰åç¨‹
        # è€Œaioæ˜¯ä¸€ä¸ªconnection poolï¼Œæ–­å¼€çš„è¿æ¥ä¼šæ”¾å›poolä¸­ï¼Œæ‰€ä»¥aioä¸èƒ½è·¨åç¨‹ä¼ é€’
        assert hash(asyncio.get_running_loop()) == self.loop_id, \
            "Backendåªèƒ½åœ¨åŒä¸€ä¸ªcoroutineä¸­ä½¿ç”¨ã€‚æ£€æµ‹åˆ°è°ƒç”¨æ­¤å‡½æ•°çš„åç¨‹å‘ç”Ÿäº†å˜åŒ–"

        return self._aio

    def reset_connection_pool(self):
        self.loop_id = None
        self._aio.connection_pool.reset()
        for replica in self.replicas:
            replica.connection_pool.reset()

    async def close(self):
        if self.io.get('head_lock') == str(id(self)):
            self.io.delete('head_lock')

        self.io.close()
        await self._aio.aclose()
        for replica in self.replicas:
            await replica.aclose()

    def requires_head_lock(self) -> bool:
        self.io.set('head_lock', id(self), nx=True)
        # ä¸åœ¨setä¸­getï¼Œå…¼å®¹ä¸€äº›rediså˜ç§
        locked = self.io.get('head_lock')
        if locked is None:
            locked = str(id(self))
        return locked == str(id(self))

    def random_replica(self) -> redis.Redis:
        """éšæœºè¿”å›ä¸€ä¸ªåªè¯»è¿æ¥"""
        if self.loop_id is None:
            self.loop_id = hash(asyncio.get_running_loop())
        assert hash(asyncio.get_running_loop()) == self.loop_id, \
            "Backendåªèƒ½åœ¨åŒä¸€ä¸ªcoroutineä¸­ä½¿ç”¨ã€‚æ£€æµ‹åˆ°è°ƒç”¨æ­¤å‡½æ•°çš„åç¨‹å‘ç”Ÿäº†å˜åŒ–"

        return random.choice(self.replicas)

    def get_mq_client(self) -> 'RedisMQClient':
        """æ¯ä¸ªwebsocketè¿æ¥è·å¾—ä¸€ä¸ªéšæœºçš„replicaè¿æ¥ï¼Œç”¨äºè¯»å–è®¢é˜…"""
        return RedisMQClient(self.random_replica())

    def transaction(self, cluster_id: int) -> 'RedisTransaction':
        """è¿›å…¥dbçš„äº‹åŠ¡æ¨¡å¼ï¼Œè¿”å›äº‹åŠ¡è¿æ¥"""
        return RedisTransaction(self, cluster_id)


class RedisTransaction(BackendTransaction):
    """æ•°æ®åº“äº‹åŠ¡ç±»ï¼Œè´Ÿè´£å¼€å§‹äº‹åŠ¡ï¼Œå¹¶æäº¤äº‹åŠ¡"""
    # key: 1:ç»“æœä¿å­˜åˆ°å“ªä¸ªkey, 2-n:è¦æ£€æŸ¥çš„keysï¼Œ argsï¼š è¦æ£€æŸ¥çš„keysçš„valueï¼ŒæŒ‰é¡ºåº
    LUA_CHECK_UNIQUE_SCRIPT = """
    local result_key = KEYS[1]
    local redis = redis

    for i = 1, #ARGV, 4 do
        local rows = redis.call('zrange', ARGV[i], ARGV[i+1], ARGV[i+2], ARGV[i+3], 'LIMIT', 0, 1)
        if #rows > 0 then
            redis.call('set', result_key, 0, 'PX', 100)
            return 'FAIL'
        end
    end
    redis.call('set', result_key, 1, 'PX', 100)
    return 'OK'
    """
    # key: 1:æ˜¯å¦æ‰§è¡Œçš„æ ‡è®°key, 2-n:ä¸ä½¿ç”¨ï¼Œä»…ä¾›å®¢æˆ·ç«¯åˆ¤æ–­hash slotç”¨, args: stackedçš„å‘½ä»¤
    LUA_IF_RUN_STACKED_SCRIPT = """
    local result_key = KEYS[1]
    local redis = redis
    local tonumber = tonumber
    local unpack = unpack
    local gsub = string.gsub
    local insert = table.insert

    local unique_check_ok = redis.call('get',  result_key)
    if tonumber(unique_check_ok) <= 0 then
        return 'FAIL'
    end

    local cur = 1
    local last_row_id = nil
    local rtn = {}
    while cur <= #ARGV do
        local len = tonumber(ARGV[cur])
        local cmds = {unpack(ARGV, cur+1, cur+len)}
        cur = cur + len + 1
        if cmds[1] == 'AUTO_INCR' then
            local idx_key = cmds[2]
            local ids = redis.call('zrange', idx_key, 0, 0, 'REV', 'WITHSCORES')
            if #ids == 0 then 
                last_row_id = 1
            else
                last_row_id = tonumber(ids[2]) + 1
            end
            insert(rtn, last_row_id)
        elseif cmds[1] == 'END_INCR' then
            last_row_id = nil
        else
            if last_row_id ~= nil then
                local _
                for i = 2, #cmds, 1 do
                    cmds[i], _ = gsub(cmds[i], '{rowid}', last_row_id)
                end
            end
            -- redis.log(2, table.concat(cmds, ','))
            redis.call(unpack(cmds))
        end
    end
    return rtn
    """
    lua_check_unique = None
    lua_run_stacked = None

    def __init__(self, backend: RedisBackend, cluster_id: int):
        super().__init__(backend, cluster_id)

        cls = self.__class__
        if cls.lua_check_unique is None:
            cls.lua_check_unique = backend.aio.register_script(cls.LUA_CHECK_UNIQUE_SCRIPT)
            backend.io.script_load(cls.LUA_CHECK_UNIQUE_SCRIPT)
        if cls.lua_run_stacked is None:
            cls.lua_run_stacked = backend.aio.register_script(cls.LUA_IF_RUN_STACKED_SCRIPT)
            backend.io.script_load(cls.LUA_IF_RUN_STACKED_SCRIPT)
            from redis.asyncio.client import Pipeline
            # ä¸è¦è®©pipelineæ¯æ¬¡æ‰§è¡Œluaè„šæœ¬è¿è¡Œscript existå‘½ä»¤ï¼Œè¿™ä¸ªå‘½ä»¤ä¼šå ç”¨Redis 20%CPU
            async def _pipeline_lua_speedup(_):
                return
            Pipeline.load_scripts = _pipeline_lua_speedup

        self._uuid = uuid.uuid4().hex
        self._checks = []  # äº‹åŠ¡ä¸­çš„uniqueæ£€æŸ¥
        self._stack = []  # äº‹åŠ¡ä¸­çš„æ›´æ–°æ“ä½œ
        self._request_auto_incr = False

        self._trx_pipe = backend.aio.pipeline()
        # å¼ºåˆ¶pipelineè¿›å…¥ç«‹å³æ¨¡å¼ï¼Œä¸ç„¶å½“æˆ‘ä»¬éœ€è¦è¯»å–æœªé”å®šçš„indexæ—¶ï¼Œä¼šä¸è¿”å›ç»“æœ
        self._trx_pipe.watching = True

    @property
    def pipe(self):
        return self._trx_pipe

    def stack_unique_check(self, index_key: str, start, stop, byscore) -> None:
        """åŠ å…¥éœ€è¦åœ¨end_transactionæ—¶è¿›è¡Œuniqueæ£€æŸ¥çš„indexå’Œvalue"""
        self._checks.extend([index_key, start, stop, 'BYSCORE' if byscore else 'BYLEX'])

    def stack_cmd(self, *args):
        if args[0] == 'AUTO_INCR':
            self._request_auto_incr = True
        self._stack.extend([len(args), ] + list(args))

    async def end_transaction(self, discard) -> list[int] | None:
        if self._trx_pipe is None:
            return
        # å¹¶å®ç°äº‹åŠ¡æäº¤çš„æ“ä½œï¼Œå°†_stackä¸­çš„å‘½ä»¤å†™å…¥äº‹åŠ¡
        if discard or len(self._stack) == 0:
            await self._trx_pipe.reset()
            self._trx_pipe = None
            return

        pipe = self._trx_pipe

        # 2ç§æ¨¡å¼ï¼Œå¦‚æœæœ‰uniqueè¦æ£€æŸ¥ï¼Œæˆ–è€…æœ‰auto incrè¦æ‰§è¡Œï¼Œå°±ç”¨luaè„šæœ¬æ‰§è¡Œæ‰€æœ‰å‘½ä»¤
        if len(self._checks) > 0 or self._request_auto_incr:
            # åœ¨æäº¤å‰æœ€åæ£€æŸ¥ä¸€éunique
            # ä¹‹å‰çš„insertå’Œupdateæ—¶ä¹Ÿæœ‰uniqueæ£€æŸ¥ï¼Œä½†ä¸ºäº†é™ä½äº‹åŠ¡å†²çªå¹¶ä¸é”å®šindexï¼Œå› æ­¤å¯èƒ½æœ‰å˜åŒ–
            # è¿™é‡Œåœ¨luaä¸­æ£€æŸ¥uniqueï¼Œä¸ç”¨é”å®šindex
            unique_check_key = f'unique_check:{{CLU{self.cluster_id}}}:' + self._uuid
            lua_unique_keys = [unique_check_key, ]
            lua_unique_argv = self._checks

            # ç”Ÿæˆäº‹åŠ¡stackï¼Œè®©luaæ¥åˆ¤æ–­uniqueæ£€æŸ¥é€šè¿‡çš„æƒ…å†µä¸‹ï¼Œæ‰æ‰§è¡Œã€‚å‡å°‘å†²çªæ¦‚ç‡ã€‚
            lua_run_keys = [unique_check_key, ]
            lua_run_argv = self._stack

            pipe.multi()
            await self.lua_check_unique(args=lua_unique_argv, keys=lua_unique_keys, client=pipe)
            await self.lua_run_stacked(args=lua_run_argv, keys=lua_run_keys, client=pipe)

            try:
                result = await pipe.execute()
                if result[-1] == 'FAIL':
                    raise RaceCondition(f"unique indexåœ¨äº‹åŠ¡ä¸­å˜åŠ¨ï¼Œè¢«å…¶ä»–äº‹åŠ¡æ·»åŠ äº†ç›¸åŒå€¼")
                result = result[-1]
            except redis.WatchError:
                raise RaceCondition(f"watched keyè¢«å…¶ä»–äº‹åŠ¡ä¿®æ”¹")
            else:
                return result
            finally:
                # æ— è®ºæ˜¯elseé‡Œçš„returnè¿˜æ˜¯excepté‡Œçš„raiseï¼Œfinallyéƒ½ä¼šåœ¨ä»–ä»¬ä¹‹å‰æ‰§è¡Œ
                await pipe.reset()
                self._trx_pipe = None

        else:
            # æ‰§è¡Œäº‹åŠ¡
            pipe.multi()

            cur = 0
            while cur < len(self._stack):
                arg_len = int(self._stack[cur])
                cmds = self._stack[cur + 1:cur + arg_len + 1]
                cur = cur + arg_len + 1
                await pipe.execute_command(*cmds)

            try:
                await pipe.execute()
            except redis.WatchError:
                raise RaceCondition(f"watched keyè¢«å…¶ä»–äº‹åŠ¡ä¿®æ”¹")
            else:
                return []
            finally:
                await pipe.reset()
                self._trx_pipe = None


class RedisComponentTable(ComponentTable):
    """
    åœ¨redisç§åˆå§‹åŒ–/ç®¡ç†Componentæ•°æ®è¡¨ï¼Œæä¾›äº‹åŠ¡æŒ‡ä»¤ã€‚

    å‚è€ƒï¼š
    redis-pyååé‡åŸºå‡†ï¼š
    syncè°ƒç”¨ï¼šå•è¿›ç¨‹ï¼š1200/sï¼Œ10è¿›ç¨‹ç†è®ºä¸Š12 Kopsï¼Œç¬¦åˆhiredisåŸºå‡†æµ‹è¯•
    asyncè°ƒç”¨ï¼šå•è¿›ç¨‹+Semaphoreé™åˆ¶100åç¨‹ï¼š6000/sï¼Œ å‚è€ƒQPS 100,000

    ä½¿ç”¨ä»¥ä¸‹keysï¼š
    instance_name:component_name.{CLU0}:id:1~n
    instance_name:component_name.{CLU0}:index:key~
    instance_name:component_name:meta
    """

    def __init__(
            self,
            component_cls: type[BaseComponent],
            instance_name: str,
            cluster_id: int,
            backend: RedisBackend
    ):
        super().__init__(component_cls, instance_name, cluster_id, backend)
        self._backend = backend  # ä¸ºäº†è®©ä»£ç æç¤ºçŸ¥é“ç±»å‹æ˜¯RedisBackend
        component_cls.hosted_ = self
        # redis keyå
        hash_tag = f'{{CLU{cluster_id}}}:'
        self._name = component_cls.component_name_
        self._root_prefix = f'{instance_name}:{self._name}:'
        self._key_prefix = f'{self._root_prefix}{hash_tag}id:'
        self._idx_prefix = f'{self._root_prefix}{hash_tag}index:'
        self._init_lock_key = f'{self._root_prefix}init_lock'
        self._meta_key = f'{self._root_prefix}meta'
        self._trx_pipe = None

    def create_or_migrate(self, cluster_only=False):
        """
        æ£€æŸ¥è¡¨ç»“æ„æ˜¯å¦æ­£ç¡®ï¼Œä¸æ­£ç¡®åˆ™å°è¯•è¿›è¡Œè¿ç§»ã€‚æ­¤æ–¹æ³•åŒæ—¶ä¼šå¼ºåˆ¶é‡å»ºè¡¨çš„ç´¢å¼•ã€‚
        metaæ ¼å¼:
        json: ç»„ä»¶çš„ç»“æ„ä¿¡æ¯
        version: jsonçš„hash
        cluster_id: æ‰€å±ç°‡id
        """
        if not self._backend.requires_head_lock():
            raise HeadLockFailed("redisä¸­head_locké”®")

        io = self._backend.io
        logger.info(f"âŒš [ğŸ’¾Redis][{self._name}ç»„ä»¶] å‡†å¤‡é”å®šæ£€æŸ¥metaä¿¡æ¯...")
        if cluster_only:
            logger.info(f"âŒš [ğŸ’¾Redis][{self._name}ç»„ä»¶] å¯¹è¯¥è¡¨åªæ£€æŸ¥cluster idè¿ç§»ä¿¡æ¯...")
        with io.lock(self._init_lock_key, timeout=60 * 5):
            # è·å–rediså·²å­˜çš„ç»„ä»¶ä¿¡æ¯
            meta = io.hgetall(self._meta_key)
            if not meta:
                self._create_emtpy()
            else:
                version = hashlib.md5(self._component_cls.json_.encode("utf-8")).hexdigest()
                # å¦‚æœcluster_idæ”¹å˜ï¼Œåˆ™è¿ç§»æ”¹keyå
                if int(meta['cluster_id']) != self._cluster_id:
                    self._migration_cluster_id(old=int(meta['cluster_id']))

                # å¦‚æœç‰ˆæœ¬ä¸ä¸€è‡´ï¼Œç»„ä»¶ç»“æ„å¯èƒ½æœ‰å˜åŒ–ï¼Œä¹Ÿå¯èƒ½åªæ˜¯æ”¹æƒé™ï¼Œæ€»ä¹‹è°ƒç”¨è¿ç§»ä»£ç 
                if meta['version'] != version and not cluster_only:
                    self._migration_schema(old=meta['json'])

            # é‡å»ºç´¢å¼•æ•°æ®
            if not cluster_only:
                self._rebuild_index()
            logger.info(f"âœ… [ğŸ’¾Redis][{self._name}ç»„ä»¶] æ£€æŸ¥å®Œæˆï¼Œè§£é”ç»„ä»¶")

    def flush(self, force=False):
        if not self._backend.requires_head_lock():
            raise HeadLockFailed("redisä¸­head_locké”®")

        if force:
            warnings.warn("flushæ­£åœ¨å¼ºåˆ¶åˆ é™¤æ‰€æœ‰æ•°æ®ï¼Œæ­¤æ–¹å¼åªå»ºè®®ç»´æŠ¤ä»£ç è°ƒç”¨ã€‚")

        # å¦‚æœéæŒä¹…åŒ–ç»„ä»¶ï¼Œåˆ™å…è®¸è°ƒç”¨flushä¸»åŠ¨æ¸…ç©ºæ•°æ®
        if not self._component_cls.persist_ or force:

            io = self._backend.io
            logger.info(f"âŒš [ğŸ’¾Redis][{self._name}ç»„ä»¶] å¯¹éæŒä¹…åŒ–ç»„ä»¶flushæ¸…ç©ºæ•°æ®ä¸­...")

            with io.lock(self._init_lock_key, timeout=60 * 5):
                del_keys = io.keys(self._root_prefix + '*')
                print('å‡†å¤‡åˆ é™¤', len(del_keys), 'ä¸ªé”®')
                del_keys.remove(self._init_lock_key)
                for batch in batched(del_keys, 1000):
                    with io.pipeline() as pipe:
                        list(map(pipe.delete, batch))
                        pipe.execute()

            self.create_or_migrate()

            logger.info(f"âœ… [ğŸ’¾Redis][{self._name}ç»„ä»¶] å·²åˆ é™¤{len(del_keys)}ä¸ªé”®å€¼")
        else:
            raise ValueError(f"{self._name}æ˜¯æŒä¹…åŒ–ç»„ä»¶ï¼Œä¸å…è®¸flushæ“ä½œ")

    def _create_emtpy(self):
        logger.info(f"âŒš [ğŸ’¾Redis][{self._name}ç»„ä»¶] ç»„ä»¶æ— metaä¿¡æ¯ï¼Œæ•°æ®ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»ºç©ºè¡¨...")

        # åªéœ€è¦å†™å…¥metaï¼Œå…¶ä»–çš„_rebuild_indexä¼šåˆ›å»º
        meta = {
            'json': self._component_cls.json_,
            'version': hashlib.md5(self._component_cls.json_.encode("utf-8")).hexdigest(),
            'cluster_id': self._cluster_id,
        }
        self._backend.io.hset(self._meta_key, mapping=meta)
        logger.info(f"âœ… [ğŸ’¾Redis][{self._name}ç»„ä»¶] ç©ºè¡¨åˆ›å»ºå®Œæˆ")
        return meta

    def _rebuild_index(self):
        logger.info(f"âŒš [ğŸ’¾Redis][{self._name}ç»„ä»¶] æ­£åœ¨é‡å»ºç´¢å¼•...")
        io = self._backend.io
        rows = io.keys(self._key_prefix + '*')
        if len(rows) == 0:
            logger.info(f"âœ… [ğŸ’¾Redis][{self._name}ç»„ä»¶] æ— æ•°æ®ï¼Œæ— éœ€é‡å»ºç´¢å¼•ã€‚")
            return

        for idx_name, str_type in self._component_cls.indexes_.items():
            idx_key = self._idx_prefix + idx_name
            # å…ˆåˆ é™¤æ‰€æœ‰_idx_keyå¼€å¤´çš„ç´¢å¼•
            io.delete(idx_key)
            # é‡å»ºæ‰€æœ‰ç´¢å¼•ï¼Œä¸ç®¡uniqueè¿˜æ˜¯indexéƒ½æ˜¯sset
            pipe = io.pipeline()
            row_ids = []
            for row in rows:
                row_id = row.split(':')[-1]
                row_ids.append(row_id)
                pipe.hget(row, idx_name)
            values = pipe.execute()
            # æŠŠvaluesæŒ‰dtypeè½¬æ¢ä¸‹
            struct = self._component_cls.new_row()
            for i, v in enumerate(values):
                struct[idx_name] = v
                values[i] = struct[idx_name].item()
            # å»ºç«‹redisç´¢å¼•
            if str_type:
                # å­—ç¬¦ä¸²ç±»å‹è¦ç‰¹æ®Šå¤„ç†ï¼Œscore=0, member='name:1'å½¢å¼
                io.zadd(idx_key, {f'{value}:{rid}': 0 for rid, value in zip(row_ids, values)})
            else:
                # zadd ä¼šæ›¿æ¢æ‰memberç›¸åŒçš„å€¼ï¼Œç­‰äºæ˜¯set
                io.zadd(idx_key, dict(zip(row_ids, values)))
        logger.info(f"âœ… [ğŸ’¾Redis][{self._name}ç»„ä»¶] ç´¢å¼•é‡å»ºå®Œæˆ, "
                    f"{len(rows)}è¡Œ * {len(self._component_cls.indexes_)}ä¸ªç´¢å¼•ã€‚")

    def _migration_cluster_id(self, old):
        logger.warning(f"âš ï¸ [ğŸ’¾Redis][{self._name}ç»„ä»¶] "
                       f"cluster_id ç”± {old} å˜æ›´ä¸º {self._cluster_id}ï¼Œ"
                       f"å°†å°è¯•è¿ç§»clusteræ•°æ®...")
        # é‡å‘½åkey
        old_hash_tag = f'{{CLU{old}}}:'
        new_hash_tag = f'{{CLU{self._cluster_id}}}:'
        old_prefix = f'{self._root_prefix}{old_hash_tag}'
        old_prefix_len = len(old_prefix)
        new_prefix = f'{self._root_prefix}{new_hash_tag}'

        io = self._backend.io
        old_keys = io.keys(old_prefix + '*')
        for old_key in old_keys:
            new_key = new_prefix + old_key[old_prefix_len:]
            io.rename(old_key, new_key)
        # æ›´æ–°meta
        io.hset(self._meta_key, 'cluster_id', self._cluster_id)
        logger.warning(
            f"âœ… [ğŸ’¾Redis][{self._name}ç»„ä»¶] cluster è¿ç§»å®Œæˆï¼Œå…±è¿ç§»{len(old_keys)}ä¸ªé”®å€¼ã€‚")

    def _migration_schema(self, old):
        """å¦‚æœæ•°æ®åº“ä¸­çš„å±æ€§å’Œå®šä¹‰ä¸ä¸€è‡´ï¼Œå°è¯•è¿›è¡Œç®€å•è¿ç§»ï¼Œå¯ä»¥å¤„ç†å±æ€§æ›´åä»¥å¤–çš„æƒ…å†µã€‚"""
        # åŠ è½½è€çš„ç»„ä»¶
        old_comp_cls = BaseComponent.load_json(old)

        # åªæœ‰propertiesåå­—å’Œç±»å‹å˜æ›´æ‰è¿ç§»
        dtypes_in_db = old_comp_cls.dtypes
        new_dtypes = self._component_cls.dtypes
        if dtypes_in_db == new_dtypes:
            return

        logger.warning(f"âš ï¸ [ğŸ’¾Redis][{self._name}ç»„ä»¶] ä»£ç å®šä¹‰çš„Schemaä¸å·²å­˜çš„ä¸ä¸€è‡´ï¼Œ"
                       f"æ•°æ®åº“ä¸­ï¼š\n"
                       f"{dtypes_in_db}\n"
                       f"ä»£ç å®šä¹‰çš„ï¼š\n"
                       f"{new_dtypes}\n "
                       f"å°†å°è¯•æ•°æ®è¿ç§»ï¼ˆåªå¤„ç†æ–°å±æ€§ï¼Œä¸å¤„ç†ç±»å‹å˜æ›´ï¼Œæ”¹åç­‰ç­‰æƒ…å†µï¼‰ï¼š")

        # todo è°ƒç”¨è‡ªå®šä¹‰ç‰ˆæœ¬è¿ç§»ä»£ç ï¼ˆdefine_migrationï¼‰

        # æ£€æŸ¥æ˜¯å¦æœ‰å±æ€§è¢«åˆ é™¤
        for prop_name in dtypes_in_db.fields:
            if prop_name not in new_dtypes.fields:
                logger.warning(f"âš ï¸ [ğŸ’¾Redis][{self._name}ç»„ä»¶] "
                               f"æ•°æ®åº“ä¸­çš„å±æ€§ {prop_name} åœ¨æ–°çš„ç»„ä»¶å®šä¹‰ä¸­ä¸å­˜åœ¨ï¼Œå¦‚æœæ”¹åäº†éœ€è¦æ‰‹åŠ¨è¿ç§»ï¼Œ"
                               f"é»˜è®¤ä¸¢å¼ƒè¯¥å±æ€§æ•°æ®ã€‚")

        # å¤šå‡ºæ¥çš„åˆ—å†æ¬¡æŠ¥è­¦å‘Šï¼Œç„¶åå¿½ç•¥
        io = self._backend.io
        rows = io.keys(self._key_prefix + '*')
        props = dict(self._component_cls.properties_)  # type: dict[str, Property]
        added = 0
        for prop_name in new_dtypes.fields:
            if prop_name not in dtypes_in_db.fields:
                logger.warning(f"âš ï¸ [ğŸ’¾Redis][{self._name}ç»„ä»¶] "
                               f"æ–°çš„ä»£ç å®šä¹‰ä¸­å¤šå‡ºå±æ€§ {prop_name}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼å¡«å……ã€‚")
                default = props[prop_name].default
                if default is None:
                    logger.error(f"âš ï¸ [ğŸ’¾Redis][{self._name}ç»„ä»¶] "
                                 f"è¿ç§»æ—¶å°è¯•æ–°å¢ {prop_name} å±æ€§å¤±è´¥ï¼Œè¯¥å±æ€§æ²¡æœ‰é»˜è®¤å€¼ï¼Œæ— æ³•æ–°å¢ã€‚")
                    raise ValueError("è¿ç§»å¤±è´¥")
                pipe = io.pipeline()
                for row in rows:
                    pipe.hset(row, prop_name, default)
                pipe.execute()
                added += 1
        logger.warning(f"âœ… [ğŸ’¾Redis][{self._name}ç»„ä»¶] æ–°å±æ€§å¢åŠ å®Œæˆï¼Œå…±å¤„ç†{len(rows)}è¡Œ * "
                       f"{added}ä¸ªå±æ€§ã€‚")

    @classmethod
    def make_query_cmd(
            cls, component_cls, index_name: str, left, right, limit, desc
    ) -> dict[str, str] | list[int]:
        """ç”Ÿæˆæ•°æ®åº“æŸ¥è¯¢å‘½ä»¤ï¼Œåœ¨direct_queryå’Œ_db_queryä¸­å¤ç”¨æ­¤ç±»ã€‚"""
        if right is None:
            right = left
            if index_name == 'id':
                return [int(left)]
        if desc:
            left, right = right, left

        # å¯¹äºstrç±»å‹æŸ¥è¯¢ï¼Œè¦ç”¨[å¼€å§‹
        str_type = component_cls.indexes_[index_name]
        by_lex = False
        if str_type:
            assert type(left) is str and type(right) is str, \
                f"å­—ç¬¦ä¸²ç±»å‹ç´¢å¼•`{index_name}`çš„æŸ¥è¯¢(left={left}, {type(left)})å˜é‡ç±»å‹å¿…é¡»æ˜¯str"
            if not left.startswith(('(', '[')):
                left = f'[{left}'
            if not right.startswith(('(', '[')):
                right = f'[{right}'

            if not left.endswith((':', ';')):
                left = f'{left}:'  # name:id å½¢å¼ï¼Œæ‰€ä»¥:ä½œä¸ºç»“å°¾æ ‡è¯†ç¬¦
            if not right.endswith((':', ';')):
                right = f'{right};'  # ';' = 3B, ':' = 3A

            by_lex = True

        return {'start': left, 'end': right, 'desc': desc, 'offset': 0,
                'num': limit, 'bylex': by_lex, 'byscore': not by_lex}

    async def direct_query(
            self,
            index_name: str,
            left,
            right=None,
            limit=10,
            desc=False,
            row_format='struct',
    ) -> np.recarray | list[dict | int]:
        replica = self._backend.random_replica()
        idx_key = self._idx_prefix + index_name

        cmds = RedisComponentTable.make_query_cmd(
            self._component_cls, index_name, left, right, limit, desc)

        if type(cmds) is list:  # å¦‚æœæ˜¯listè¯´æ˜ä¸éœ€è¦æŸ¥è¯¢ç›´æ¥è¿”å›id
            row_ids = cmds
        else:
            row_ids = await replica.zrange(name=idx_key, **cmds)
            str_type = self._component_cls.indexes_[index_name]
            if str_type:
                row_ids = [vk.split(':')[-1] for vk in row_ids]

        if row_format == 'id':
            return list(map(int, row_ids))
        raw = row_format == 'raw'

        key_prefix = self._key_prefix
        rows = []
        for _id in row_ids:
            row = await replica.hgetall(key_prefix + str(_id))
            if raw:
                rows.append(row)
            else:
                rows.append(self._component_cls.dict_to_row(row))

        if raw:
            return rows
        else:
            if len(rows) == 0:
                return np.rec.array(np.empty(0, dtype=self._component_cls.dtypes))
            else:
                return np.rec.array(np.stack(rows, dtype=self._component_cls.dtypes))

    async def direct_get(self, row_id: int) -> None | np.record:
        replica = self._backend.random_replica()
        key = self._key_prefix + str(row_id)
        row = await replica.hgetall(key)
        if row:
            return self._component_cls.dict_to_row(row)
        else:
            return None

    async def direct_set(self, row_id: int, **kwargs):
        aio = self._backend.aio
        key = self._key_prefix + str(row_id)

        for prop in kwargs:
            if prop in self._component_cls.indexes_:
                raise ValueError(f"ç´¢å¼•å­—æ®µ`{prop}`ä¸å…è®¸ç”¨direct_setä¿®æ”¹")
        await aio.hset(key, mapping=kwargs)

    def attach(self, backend_trx: RedisTransaction) -> 'RedisComponentTransaction':
        # è¿™é‡Œä¸ç”¨æ£€æŸ¥cluster_idï¼Œå› ä¸ºComponentTransactionä¼šæ£€æŸ¥
        # assert backend_trx.cluster_id == self._cluster_id
        return RedisComponentTransaction(
            self, backend_trx, self._key_prefix, self._idx_prefix)

    def channel_name(self, index_name: str = None, row_id: int = None):
        dbi = self._backend.dbi
        if index_name:
            return f'__keyspace@{dbi}__:{self._idx_prefix + index_name}'
        elif row_id is not None:
            return f'__keyspace@{dbi}__:{self._key_prefix + str(row_id)}'
        raise ValueError("index_nameå’Œrow_idå¿…é¡»æœ‰ä¸€ä¸ª")


class RedisComponentTransaction(ComponentTransaction):
    def __init__(
            self,
            comp_tbl: RedisComponentTable,
            trx_conn: RedisTransaction,
            key_prefix: str,
            index_prefix: str
    ):
        super().__init__(comp_tbl, trx_conn)
        self._trx_conn = trx_conn  # ä¸ºäº†è®©ä»£ç æç¤ºçŸ¥é“ç±»å‹æ˜¯RedisTransaction

        self._key_prefix = key_prefix
        self._idx_prefix = index_prefix

    async def _db_get(self, row_id: int) -> None | np.record:
        # è·å–è¡Œæ•°æ®çš„æ“ä½œ
        key = self._key_prefix + str(row_id)
        pipe = self._trx_conn.pipe

        # åŒæ—¶è¦è®©ä¹è§‚é”é”å®šè¯¥è¡Œ
        await pipe.watch(key)
        # è¿”å›å€¼è¦é€šè¿‡dict_to_rowåŒ…è£¹ä¸‹
        if row := await pipe.hgetall(key):
            return self._component_cls.dict_to_row(row)
        else:
            return None

    async def _db_query(
            self,
            index_name: str,
            left,
            right=None,
            limit=10,
            desc=False,
            lock_index=True
    ) -> list[int]:
        idx_key = self._idx_prefix + index_name
        pipe = self._trx_conn.pipe

        cmds = RedisComponentTable.make_query_cmd(
            self._component_cls, index_name, left, right, limit, desc)

        if type(cmds) is list:  # å¦‚æœæ˜¯listè¯´æ˜ä¸éœ€è¦æŸ¥è¯¢ç›´æ¥è¿”å›id
            return cmds

        if lock_index:
            await pipe.watch(idx_key)
        row_ids = await pipe.zrange(name=idx_key, **cmds)

        str_type = self._component_cls.indexes_[index_name]
        if str_type:
            row_ids = [vk.split(':')[-1] for vk in row_ids]

        # æœªæŸ¥è¯¢åˆ°æ•°æ®æ—¶è¿”å›[]
        return list(map(int, row_ids))

    def _trx_check_unique(self, old_row, new_row: np.record) -> None:
        trx = self._trx_conn
        component_cls = self._component_cls
        idx_prefix = self._idx_prefix

        for idx in component_cls.uniques_:
            if idx == 'id':  # insertä¸éœ€è¦æ£€æŸ¥id, updateä¹Ÿä¸éœ€è¦å› ä¸ºåŸºç±»é‡Œä¼šç¡®è®¤idä¸€æ ·
                continue
            if old_row is None or old_row[idx] != new_row[idx]:
                key = idx_prefix + idx
                str_type = component_cls.indexes_[idx]
                qv = new_row[idx].item()
                if str_type:
                    trx.stack_unique_check(key, f'[{qv}:', f'[{qv};', False)
                else:
                    trx.stack_unique_check(key, qv, qv, True)

    def _trx_insert(self, row: np.record) -> None:
        trx = self._trx_conn
        component_cls = self._component_cls
        idx_prefix = self._idx_prefix

        self._trx_check_unique(None, row)
        # å¼€å§‹è‡ªå¢idæ¨¡å¼, å¹¶ç”¨placeholder {rowid}æ›¿æ¢id
        trx.stack_cmd('AUTO_INCR', idx_prefix + 'id')
        row_id = '{rowid}'
        row_key = self._key_prefix + str(row_id)
        # è®¾ç½®rowæ•°æ®
        kvs = itertools.chain.from_iterable(zip(row.dtype.names, row.tolist()))
        trx.stack_cmd('hset', row_key, *kvs)
        # æ›´æ–°ç´¢å¼•
        for idx_name, str_type in component_cls.indexes_.items():
            idx_key = idx_prefix + idx_name
            if str_type:
                trx.stack_cmd('zadd', idx_key, 0, f'{row[idx_name]}:{row_id}')
            elif idx_name == 'id':
                trx.stack_cmd('zadd', idx_key, row_id, row_id)
            else:
                trx.stack_cmd('zadd', idx_key, row[idx_name].item(), row_id)
        # ç»“æŸè‡ªå¢idæ¨¡å¼
        trx.stack_cmd('hset', row_key, 'id', row_id)
        trx.stack_cmd('END_INCR')

    def _trx_update(self, row_id: int, old_row: np.record, new_row: np.record) -> None:
        trx = self._trx_conn
        component_cls = self._component_cls
        idx_prefix = self._idx_prefix

        self._trx_check_unique(old_row, new_row)
        # æ›´æ–°rowæ•°æ®
        row_key = self._key_prefix + str(row_id)
        kvs = []
        for key_name, new_value in zip(new_row.dtype.names, new_row.tolist()):
            if old_row[key_name] != new_value:
                kvs.extend([key_name, new_value])
        # å¦‚æœæ²¡ä»»ä½•æ•°æ®å˜åŠ¨ï¼Œè·³å‡º
        if len(kvs) == 0:
            return
        # æ›´æ–°æ•°æ®
        trx.stack_cmd('hset', row_key, *kvs)
        # æ›´æ–°ç´¢å¼•
        for idx_name, str_type in component_cls.indexes_.items():
            if old_row[idx_name] == new_row[idx_name]:
                continue
            idx_key = idx_prefix + idx_name
            if str_type:
                trx.stack_cmd('zadd', idx_key, 0, f'{new_row[idx_name]}:{row_id}')
                trx.stack_cmd('zrem', idx_key, f'{old_row[idx_name]}:{row_id}')
            elif idx_name == 'id':
                trx.stack_cmd('zadd', idx_key, row_id, row_id)
            else:
                trx.stack_cmd('zadd', idx_key, new_row[idx_name].item(), row_id)

    def _trx_delete(self, row_id: int, old_row: np.record) -> None:
        trx = self._trx_conn
        component_cls = self._component_cls
        idx_prefix = self._idx_prefix

        row_key = self._key_prefix + str(row_id)
        trx.stack_cmd('del', row_key)

        for idx_name, str_type in component_cls.indexes_.items():
            idx_key = idx_prefix + idx_name
            if str_type:
                trx.stack_cmd('zrem', idx_key, f'{old_row[idx_name]}:{row_id}')
            else:
                trx.stack_cmd('zrem', idx_key, row_id)


##############################


class RedisMQClient(MQClient):
    """è¿æ¥åˆ°æ¶ˆæ¯é˜Ÿåˆ—çš„å®¢æˆ·ç«¯ï¼Œæ¯ä¸ªç”¨æˆ·è¿æ¥ä¸€ä¸ªå®ä¾‹ã€‚"""

    def __init__(self, redis_conn: redis.asyncio.Redis | redis.asyncio.RedisCluster):
        # todo è¦æµ‹è¯•redis clusteræ˜¯å¦èƒ½æ­£å¸¸pub sub
        # 2ç§æ¨¡å¼ï¼š
        # a. æ¯ä¸ªwsè¿æ¥ä¸€ä¸ªpubsubè¿æ¥ï¼Œåˆ†å‘äº¤ç»™servantsï¼Œç»“æ„æ¸…æ™°ï¼Œç›®å‰çš„æ¨¡å¼ï¼Œä½†ç½‘ç»œå ç”¨é«˜
        # b. æ¯ä¸ªworkerä¸€ä¸ªpubsubè¿æ¥ï¼Œåˆ†å‘äº¤ç»™workeræ¥åšï¼Œè¿™æ ·è¿æ¥æ•°è¾ƒå°‘ï¼Œä½†ç­‰äº2å¥—åˆ†å‘ç³»ç»Ÿç»“æ„å¤æ‚
        self._mq = redis_conn.pubsub()
        self.subscribed = set()
        self.pulled_deque = MultiMap()
        self.pulled_set = set()

    async def close(self):
        return await self._mq.aclose()

    async def subscribe(self, channel_name) -> None:
        await self._mq.subscribe(channel_name)
        self.subscribed.add(channel_name)
        if len(self.subscribed) > MAX_SUBSCRIBED:
            # æŠ‘åˆ¶æ­¤è­¦å‘Šå¯é€šè¿‡ä¿®æ”¹hetu.backend.redis.MAX_SUBSCRIBEDå‚æ•°
            logger.warning(f"âš ï¸ [ğŸ’¾Redis] è®¢é˜…æ•°æ®æ•°è¶…è¿‡1000è¡Œï¼Œå¯èƒ½å¯¼è‡´ç½‘ç»œå’ŒCPUæ¶ˆè€—è¿‡å¤§ï¼Œ"
                           f"å½“å‰è®¢é˜…æ•°ï¼š{len(self.subscribed)}ã€‚")

    async def unsubscribe(self, channel_name) -> None:
        await self._mq.unsubscribe(channel_name)
        self.subscribed.remove(channel_name)

    async def pull(self) -> None:
        mq = self._mq

        # å¦‚æœæ²¡è®¢é˜…è¿‡å†…å®¹ï¼Œé‚£ä¹ˆredis mqçš„connectionæ˜¯Noneï¼Œæ— éœ€get_message
        if mq.connection is None:
            await asyncio.sleep(0.5)  # ä¸å†™åç¨‹å°±æ­»é”äº†
            return

        # è·å¾—æ›´æ–°å¾—é¢‘é“åï¼Œå¦‚æœä¸åœ¨pulledåˆ—è¡¨ä¸­ï¼Œæ‰æ·»åŠ ï¼Œåˆ—è¡¨æŒ‰æ·»åŠ æ—¶é—´æ’åº
        msg = await mq.get_message(ignore_subscribe_messages=True, timeout=None)
        if msg is not None:
            channel_name = msg['channel']
            # åˆ¤æ–­æ˜¯å¦å·²åœ¨dequeä¸­äº†ï¼Œget_messageä¼šè‡ªåŠ¨å»é‡ï¼Œè¿™é‡Œåˆ¤æ–­æ˜¯ä¸ºäº†é˜²æ­¢popçš„æ—¶é—´æ­£å¥½å¤¹æ–­2æ¡ç›¸åŒçš„æ¶ˆæ¯
            if channel_name not in self.pulled_set:
                self.pulled_deque.add(time.time(), channel_name)
                self.pulled_set.add(channel_name)
                # pop 2åˆ†é’Ÿå‰çš„æ¶ˆæ¯ï¼Œé˜²æ­¢é˜Ÿåˆ—æº¢å‡º
                dropped = set(self.pulled_deque.pop(0, time.time() - 120))
                self.pulled_set -= dropped

    async def get_message(self) -> set[str]:
        pulled_deque = self.pulled_deque

        interval = 1 / self.UPDATE_FREQUENCY
        # å¦‚æœæ²¡æ•°æ®ï¼Œç­‰å¾…ç›´åˆ°æœ‰æ•°æ®
        while not pulled_deque:
            await asyncio.sleep(interval)

        while True:
            # åªå–è¶…è¿‡intervalçš„æ•°æ®ï¼Œè¿™æ ·å¯ä»¥å‡å°‘é¢‘ç¹æ›´æ–°ã€‚setä¸€ä¸‹å¯ä»¥åˆå¹¶ç›¸åŒæ¶ˆæ¯
            rtn = set(pulled_deque.pop(0, time.time() - interval))
            if rtn:
                self.pulled_set -= rtn
                return rtn
            await asyncio.sleep(interval)

    @property
    def subscribed_channels(self) -> set[str]:
        return set(self._mq.channels) - set(self._mq.pending_unsubscribe_channels)
