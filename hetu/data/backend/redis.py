"""
@author: Heerozh (Zhang Jianhao)
@copyright: Copyright 2024, Heerozh. All rights reserved.
@license: Apache2.0 å¯ç”¨ä½œå•†ä¸šé¡¹ç›®ï¼Œå†éšä¾¿æ‰¾ä¸ªè§’è½æåŠç”¨åˆ°äº†æ­¤é¡¹ç›® :D
@email: heeroz@gmail.com
"""
import asyncio
import hashlib
import itertools
import logging
import random
import time
import uuid
import warnings

import numpy as np
import redis

from .base import ComponentTransaction, ComponentTable, Backend, BackendTransaction, MQClient
from .base import RaceCondition, HeadLockFailed
from ..component import BaseComponent, Property

logger = logging.getLogger('HeTu')


class RedisBackend(Backend):
    """å‚¨å­˜åˆ°Redisåç«¯çš„è¿æ¥ï¼ŒæœåŠ¡å™¨å¯åŠ¨æ—¶ç”±server.pyæ ¹æ®Configåˆå§‹åŒ–ï¼Œå¹¶ä¼ å…¥RedisComponentBackendã€‚"""

    def __init__(self, config: dict):
        super().__init__(config)
        # åŒæ­¥ioè¿æ¥, å¼‚æ­¥ioè¿æ¥, åªè¯»ioè¿æ¥
        self.master_url = config['master']
        self.io = redis.from_url(self.master_url, decode_responses=True)
        self._aio = redis.asyncio.from_url(self.master_url, decode_responses=True)
        self.dbi = self.io.connection_pool.connection_kwargs['db']
        # è¿æ¥åªè¯»æ•°æ®åº“
        servants = config.get('servants', [])
        self.replicas = [redis.asyncio.from_url(url, decode_responses=True) for url in servants]
        if not servants:
            servants.append(self.master_url)
            self.replicas.append(self._aio)
        # é™åˆ¶aioè¿è¡Œçš„coroutine
        try:
            self.loop_id = hash(asyncio.get_running_loop())
        except RuntimeError:
            self.loop_id = None
        # é…ç½®keyspaceé€šçŸ¥
        target_keyspace = 'Kghz'
        try:
            for url in servants:
                r = redis.from_url(url)
                db_keyspace = r.config_get('notify-keyspace-events')['notify-keyspace-events']
                db_keyspace = db_keyspace.replace('A', 'g$lshztxed')
                db_keyspace_new = db_keyspace
                for flag in list(target_keyspace):
                    if flag not in db_keyspace:
                        db_keyspace_new += flag
                if db_keyspace_new != db_keyspace:
                    r.config_set('notify-keyspace-events', db_keyspace_new)
        except redis.exceptions.NoPermissionError:
            logger.warning("âš ï¸ [ğŸ’¾Redis] æ— æƒé™è°ƒç”¨æ•°æ®åº“config_setå‘½ä»¤ï¼Œæ•°æ®è®¢é˜…å°†ä¸èµ·æ•ˆã€‚"
                           f"å¯æ‰‹åŠ¨è®¾ç½®é…ç½®æ–‡ä»¶ï¼šnotify-keyspace-events={target_keyspace}")

    @property
    def aio(self):
        if self.loop_id is None:
            self.loop_id = hash(asyncio.get_running_loop())
        # redis-pyçš„async connectionç”¨çš„pythonçš„steam.connectï¼Œç»‘å®šåˆ°å½“å‰åç¨‹
        # è€Œaioæ˜¯ä¸€ä¸ªconnection poolï¼Œæ–­å¼€çš„è¿æ¥ä¼šæ”¾å›poolä¸­ï¼Œæ‰€ä»¥aioä¸èƒ½è·¨åç¨‹ä¼ é€’
        assert hash(asyncio.get_running_loop()) == self.loop_id, \
            "Backendåªèƒ½åœ¨åŒä¸€ä¸ªcoroutineä¸­ä½¿ç”¨ã€‚æ£€æµ‹åˆ°è°ƒç”¨æ­¤å‡½æ•°çš„åç¨‹å‘ç”Ÿäº†å˜åŒ–"

        return self._aio

    async def close(self):
        if self.io.get('head_lock') == str(id(self)):
            self.io.delete('head_lock')

        self.io.close()
        await self._aio.aclose()
        for replica in self.replicas:
            await replica.aclose()

    def requires_head_lock(self) -> bool:
        locked = self.io.set('head_lock', id(self), nx=True, get=True)
        if locked is None:
            locked = str(id(self))
        return locked == str(id(self))

    def random_replica(self) -> redis.Redis:
        """éšæœºè¿”å›ä¸€ä¸ªåªè¯»è¿æ¥"""
        if self.loop_id is None:
            self.loop_id = hash(asyncio.get_running_loop())
        assert hash(asyncio.get_running_loop()) == self.loop_id, \
            "Backendåªèƒ½åœ¨åŒä¸€ä¸ªcoroutineä¸­ä½¿ç”¨ã€‚æ£€æµ‹åˆ°è°ƒç”¨æ­¤å‡½æ•°çš„åç¨‹å‘ç”Ÿäº†å˜åŒ–"

        i = random.randint(0, len(self.replicas) - 1)
        return self.replicas[i]

    def get_mq_client(self) -> 'RedisMQClient':
        """æ¯ä¸ªwebsocketè¿æ¥è·å¾—ä¸€ä¸ªéšæœºçš„replicaè¿æ¥ï¼Œç”¨äºè¯»å–è®¢é˜…"""
        return RedisMQClient(self.random_replica())

    def transaction(self, cluster_id: int) -> 'RedisTransaction':
        """è¿›å…¥dbçš„äº‹åŠ¡æ¨¡å¼ï¼Œè¿”å›äº‹åŠ¡è¿æ¥"""
        return RedisTransaction(self, cluster_id)


class RedisTransaction(BackendTransaction):
    """æ•°æ®åº“äº‹åŠ¡ç±»ï¼Œè´Ÿè´£å¼€å§‹äº‹åŠ¡ï¼Œå¹¶æäº¤äº‹åŠ¡"""
    # key: 1:ç»“æœä¿å­˜åˆ°å“ªä¸ªkey, 2-n:è¦æ£€æŸ¥çš„keysï¼Œ argsï¼š è¦æ£€æŸ¥çš„keysçš„valueï¼ŒæŒ‰é¡ºåº
    LUA_CHECK_UNIQUE_SCRIPT = """
    local result_key = KEYS[1]
    local sub = string.sub
    local redis = redis

    for i = 2, #KEYS, 1 do
        local start = ARGV[i-1]
        local stop = start
        local by = 'BYSCORE'
        if sub(start, 1, 1) == '[' then
            start = start .. ':'
            stop = start .. ';'
            by = 'BYLEX'
        end
        local rows = redis.call('zrange', KEYS[i], start, stop, by, 'LIMIT', 0, 1)
        if #rows > 0 then
            redis.call('set', result_key, 0, 'PX', 100)
            return 'FAIL'
        end
    end
    redis.call('set', result_key, 1, 'PX', 100)
    return 'OK'
    """
    # key: 1:æ˜¯å¦æ‰§è¡Œçš„æ ‡è®°key, 2-n:ä¸ä½¿ç”¨ï¼Œä»…ä¾›å®¢æˆ·ç«¯åˆ¤æ–­hash slotç”¨, args: stackedçš„å‘½ä»¤
    LUA_IF_RUN_STACK_SCRIPT = """
    local result_key = KEYS[1]
    local redis = redis
    local tonumber = tonumber
    local unpack = unpack
    local gsub = string.gsub
    local insert = table.insert

    local unique_check_ok = redis.call('get',  result_key)
    if tonumber(unique_check_ok) <= 0 then
        return 'FAIL'
    end

    local cur = 1
    local last_row_id = nil
    local rtn = {}
    while cur <= #ARGV do
        local len = tonumber(ARGV[cur])
        local cmds = {unpack(ARGV, cur+1, cur+len)}
        cur = cur + len + 1
        if cmds[1] == 'AUTO_INCR' then
            local idx_key = cmds[2]
            local ids = redis.call('zrange', idx_key, 0, 0, 'REV', 'WITHSCORES')
            if #ids == 0 then 
                last_row_id = 1
            else
                last_row_id = tonumber(ids[2]) + 1
            end
            insert(rtn, last_row_id)
        elseif cmds[1] == 'END_INCR' then
            last_row_id = nil
        else
            if last_row_id ~= nil then
                local _
                for i = 2, #cmds, 1 do
                    cmds[i], _ = gsub(cmds[i], '{rowid}', last_row_id)
                end
            end
            -- redis.log(2, table.concat(cmds, ','))
            redis.call(unpack(cmds))
        end
    end
    return rtn
    """
    lua_check_unique = None
    lua_run_stack = None

    def __init__(self, backend: RedisBackend, cluster_id: int):
        super().__init__(backend, cluster_id)

        cls = self.__class__
        if cls.lua_check_unique is None:
            cls.lua_check_unique = backend.aio.register_script(cls.LUA_CHECK_UNIQUE_SCRIPT)
        if cls.lua_run_stack is None:
            cls.lua_run_stack = backend.aio.register_script(cls.LUA_IF_RUN_STACK_SCRIPT)

        self._uuid = uuid.uuid4().hex
        self._checks = {}  # äº‹åŠ¡ä¸­çš„uniqueæ£€æŸ¥ï¼Œkeyä¸ºuniqueç´¢å¼•åï¼Œvalueä¸ºå€¼
        self._updates = []  # äº‹åŠ¡ä¸­çš„æ›´æ–°æ“ä½œ

        self._trx_pipe = backend.aio.pipeline()
        # å¼ºåˆ¶pipelineè¿›å…¥ç«‹å³æ¨¡å¼ï¼Œä¸ç„¶å½“æˆ‘ä»¬éœ€è¦è¯»å–æœªé”å®šçš„indexæ—¶ï¼Œä¼šä¸è¿”å›ç»“æœ
        self._trx_pipe.watching = True

    @property
    def pipe(self):
        return self._trx_pipe

    def stack_unique_check(self, index_key: str, value: int | float | str) -> None:
        """åŠ å…¥éœ€è¦åœ¨end_transactionæ—¶è¿›è¡Œuniqueæ£€æŸ¥çš„indexå’Œvalue"""
        if (values := self._checks.get(index_key)) is None:
            values = set()
            self._checks[index_key] = values
        values.add(value)

    def stack_cmd(self, *args):
        self._updates.extend([len(args), ] + list(args))

    async def end_transaction(self, discard) -> list[int] | None:
        if self._trx_pipe is None:
            return
        # å¹¶å®ç°äº‹åŠ¡æäº¤çš„æ“ä½œï¼Œå°†_updatesä¸­çš„å‘½ä»¤å†™å…¥äº‹åŠ¡
        if discard or len(self._updates) == 0:
            await self._trx_pipe.reset()
            self._trx_pipe = None
            return

        pipe = self._trx_pipe

        # å‡†å¤‡å¯¹unique indexè¿›è¡Œæœ€ç»ˆæ£€æŸ¥ï¼Œä¹‹å‰è™½ç„¶æ£€æŸ¥è¿‡ï¼Œä½†æ²¡æœ‰lock indexï¼Œå€¼å¯èƒ½å˜æ›´
        # è¿™é‡Œç”¨luaåœ¨äº‹åŠ¡ä¸­æ£€æŸ¥ï¼Œå¯ä»¥å‡å°‘watchçš„keyæ•°é‡
        unique_check_key = f'unique_check:{{CLU{self.cluster_id}}}:' + self._uuid
        lua_unique_keys = [unique_check_key, ]
        lua_unique_argv = []
        for k, v in self._checks.items():
            lua_unique_keys.extend([k] * len(v))
            lua_unique_argv.extend(v)

        # ç”Ÿæˆäº‹åŠ¡stackï¼Œè®©luaæ¥åˆ¤æ–­uniqueæ£€æŸ¥é€šè¿‡çš„æƒ…å†µä¸‹ï¼Œæ‰æ‰§è¡Œã€‚å‡å°‘å†²çªæ¦‚ç‡ã€‚
        lua_run_keys = [unique_check_key, ]
        lua_run_argv = self._updates

        pipe.multi()
        await self.lua_check_unique(args=lua_unique_argv, keys=lua_unique_keys, client=pipe)
        await self.lua_run_stack(args=lua_run_argv, keys=lua_run_keys, client=pipe)

        try:
            result = await pipe.execute()
            if result[-1] == 'FAIL':
                raise RaceCondition(f"unique indexåœ¨äº‹åŠ¡ä¸­å˜åŠ¨ï¼Œè¢«å…¶ä»–äº‹åŠ¡æ·»åŠ äº†ç›¸åŒå€¼")
            result = result[-1]
        except redis.WatchError:
            raise RaceCondition(f"watched keyè¢«å…¶ä»–äº‹åŠ¡ä¿®æ”¹")
        else:
            return result
        finally:
            # æ— è®ºæ˜¯elseé‡Œçš„returnè¿˜æ˜¯excepté‡Œçš„raiseï¼Œfinallyéƒ½ä¼šåœ¨ä»–ä»¬ä¹‹å‰æ‰§è¡Œ
            await pipe.reset()
            self._trx_pipe = None


class RedisComponentTable(ComponentTable):
    """
    åœ¨redisç§åˆå§‹åŒ–/ç®¡ç†Componentæ•°æ®è¡¨ï¼Œæä¾›äº‹åŠ¡æŒ‡ä»¤ã€‚

    å‚è€ƒï¼š
    redis-pyååé‡åŸºå‡†ï¼š
    syncè°ƒç”¨ï¼šå•è¿›ç¨‹ï¼š1200/sï¼Œ10è¿›ç¨‹ç†è®ºä¸Š12 Kopsï¼Œç¬¦åˆhiredisåŸºå‡†æµ‹è¯•
    asyncè°ƒç”¨ï¼šå•è¿›ç¨‹+Semaphoreé™åˆ¶100åç¨‹ï¼š6000/sï¼Œ å‚è€ƒQPS 100,000

    ä½¿ç”¨ä»¥ä¸‹keysï¼š
    instance_name:component_name.{CLU0}:id:1~n
    instance_name:component_name.{CLU0}:index:key~
    instance_name:component_name:meta
    """

    def __init__(
            self,
            component_cls: type[BaseComponent],
            instance_name: str,
            cluster_id: int,
            backend: RedisBackend
    ):
        super().__init__(component_cls, instance_name, cluster_id, backend)
        self._backend = backend  # ä¸ºäº†è®©ä»£ç æç¤ºçŸ¥é“ç±»å‹æ˜¯RedisBackend
        component_cls.hosted_ = self
        # redis keyå
        hash_tag = f'{{CLU{cluster_id}}}:'
        self._name = component_cls.component_name_
        self._root_prefix = f'{instance_name}:{self._name}:'
        self._key_prefix = f'{self._root_prefix}{hash_tag}id:'
        self._idx_prefix = f'{self._root_prefix}{hash_tag}index:'
        self._init_lock_key = f'{self._root_prefix}init_lock'
        self._meta_key = f'{self._root_prefix}meta'
        self._trx_pipe = None
        self._autoinc = None

    def create_or_migrate(self):
        """
        æ£€æŸ¥è¡¨ç»“æ„æ˜¯å¦æ­£ç¡®ï¼Œä¸æ­£ç¡®åˆ™å°è¯•è¿›è¡Œè¿ç§»ã€‚æ­¤æ–¹æ³•åŒæ—¶ä¼šå¼ºåˆ¶é‡å»ºè¡¨çš„ç´¢å¼•ã€‚
        metaæ ¼å¼:
        json: ç»„ä»¶çš„ç»“æ„ä¿¡æ¯
        version: jsonçš„hash
        cluster_id: æ‰€å±ç°‡id
        """
        if not self._backend.requires_head_lock():
            raise HeadLockFailed("redisä¸­head_locké”®")

        io = self._backend.io
        logger.info(f"âŒš [ğŸ’¾Redis][{self._name}ç»„ä»¶] å‡†å¤‡é”å®šæ£€æŸ¥metaä¿¡æ¯...")
        with io.lock(self._init_lock_key, timeout=60 * 5):
            # è·å–rediså·²å­˜çš„ç»„ä»¶ä¿¡æ¯
            meta = io.hgetall(self._meta_key)
            if not meta:
                self._create_emtpy()
            else:
                version = hashlib.md5(self._component_cls.json_.encode("utf-8")).hexdigest()
                # å¦‚æœcluster_idæ”¹å˜ï¼Œåˆ™è¿ç§»æ”¹keyå
                if int(meta['cluster_id']) != self._cluster_id:
                    self._migration_cluster_id(old=int(meta['cluster_id']))

                # å¦‚æœç‰ˆæœ¬ä¸ä¸€è‡´ï¼Œç»„ä»¶ç»“æ„å¯èƒ½æœ‰å˜åŒ–ï¼Œä¹Ÿå¯èƒ½åªæ˜¯æ”¹æƒé™ï¼Œæ€»ä¹‹è°ƒç”¨è¿ç§»ä»£ç 
                if meta['version'] != version:
                    self._migration_schema(old=meta['json'])

            # é‡å»ºç´¢å¼•æ•°æ®
            self._rebuild_index()
            logger.info(f"âœ… [ğŸ’¾Redis][{self._name}ç»„ä»¶] æ£€æŸ¥å®Œæˆï¼Œè§£é”ç»„ä»¶")

    def flush(self, force=False):
        if not self._backend.requires_head_lock():
            raise HeadLockFailed("redisä¸­head_locké”®")

        if force:
            warnings.warn("flushæ­£åœ¨å¼ºåˆ¶åˆ é™¤æ‰€æœ‰æ•°æ®ï¼Œæ­¤æ–¹å¼åªå»ºè®®ç»´æŠ¤ä»£ç è°ƒç”¨ã€‚")

        # å¦‚æœéæŒä¹…åŒ–ç»„ä»¶ï¼Œåˆ™å…è®¸è°ƒç”¨flushä¸»åŠ¨æ¸…ç©ºæ•°æ®
        if not self._component_cls.persist_ or force:

            io = self._backend.io
            logger.info(f"âŒš [ğŸ’¾Redis][{self._name}ç»„ä»¶] å¯¹éæŒä¹…åŒ–ç»„ä»¶flushæ¸…ç©ºæ•°æ®ä¸­...")

            with io.lock(self._init_lock_key, timeout=60 * 5):
                del_keys = io.keys(self._root_prefix + '*')
                del_keys.remove(self._init_lock_key)
                list(map(io.delete, del_keys))

            self.create_or_migrate()

            logger.info(f"âœ… [ğŸ’¾Redis][{self._name}ç»„ä»¶] å·²åˆ é™¤{len(del_keys)}ä¸ªé”®å€¼")
        else:
            raise ValueError(f"{self._name}æ˜¯æŒä¹…åŒ–ç»„ä»¶ï¼Œä¸å…è®¸flushæ“ä½œ")

    def _create_emtpy(self):
        logger.info(f"âŒš [ğŸ’¾Redis][{self._name}ç»„ä»¶] ç»„ä»¶æ— metaä¿¡æ¯ï¼Œæ•°æ®ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»ºç©ºè¡¨...")

        # åªéœ€è¦å†™å…¥metaï¼Œå…¶ä»–çš„_rebuild_indexä¼šåˆ›å»º
        meta = {
            'json': self._component_cls.json_,
            'version': hashlib.md5(self._component_cls.json_.encode("utf-8")).hexdigest(),
            'cluster_id': self._cluster_id,
        }
        self._backend.io.hset(self._meta_key, mapping=meta)
        logger.info(f"âœ… [ğŸ’¾Redis][{self._name}ç»„ä»¶] ç©ºè¡¨åˆ›å»ºå®Œæˆ")
        return meta

    def _rebuild_index(self):
        logger.info(f"âŒš [ğŸ’¾Redis][{self._name}ç»„ä»¶] æ­£åœ¨é‡å»ºç´¢å¼•...")
        io = self._backend.io
        rows = io.keys(self._key_prefix + '*')
        if len(rows) == 0:
            logger.info(f"âœ… [ğŸ’¾Redis][{self._name}ç»„ä»¶] æ— æ•°æ®ï¼Œæ— éœ€é‡å»ºç´¢å¼•ã€‚")
            return

        for idx_name, str_type in self._component_cls.indexes_.items():
            idx_key = self._idx_prefix + idx_name
            # å…ˆåˆ é™¤æ‰€æœ‰_idx_keyå¼€å¤´çš„ç´¢å¼•
            io.delete(idx_key)
            # é‡å»ºæ‰€æœ‰ç´¢å¼•ï¼Œä¸ç®¡uniqueè¿˜æ˜¯indexéƒ½æ˜¯sset
            pipe = io.pipeline()
            row_ids = []
            for row in rows:
                row_id = row.split(':')[-1]
                row_ids.append(row_id)
                pipe.hget(row, idx_name)
            values = pipe.execute()
            # æŠŠvaluesæŒ‰dtypeè½¬æ¢ä¸‹
            struct = self._component_cls.new_row()
            for i, v in enumerate(values):
                struct[idx_name] = v
                values[i] = struct[idx_name].item()
            # å»ºç«‹redisç´¢å¼•
            if str_type:
                # å­—ç¬¦ä¸²ç±»å‹è¦ç‰¹æ®Šå¤„ç†ï¼Œscore=0, member='name:1'å½¢å¼
                io.zadd(idx_key, {f'{value}:{rid}': 0 for rid, value in zip(row_ids, values)})
            else:
                # zadd ä¼šæ›¿æ¢æ‰memberç›¸åŒçš„å€¼ï¼Œç­‰äºæ˜¯set
                io.zadd(idx_key, dict(zip(row_ids, values)))
        logger.info(f"âœ… [ğŸ’¾Redis][{self._name}ç»„ä»¶] ç´¢å¼•é‡å»ºå®Œæˆ, "
                    f"{len(rows)}è¡Œ * {len(self._component_cls.indexes_)}ä¸ªç´¢å¼•ã€‚")

    def _migration_cluster_id(self, old):
        logger.warning(f"âš ï¸ [ğŸ’¾Redis][{self._name}ç»„ä»¶] "
                       f"cluster_id ç”± {old} å˜æ›´ä¸º {self._cluster_id}ï¼Œ"
                       f"å°†å°è¯•è¿ç§»clusteræ•°æ®...")
        # é‡å‘½åkey
        old_hash_tag = f'{{CLU{old}}}:'
        new_hash_tag = f'{{CLU{self._cluster_id}}}:'
        old_prefix = f'{self._root_prefix}{old_hash_tag}'
        old_prefix_len = len(old_prefix)
        new_prefix = f'{self._root_prefix}{new_hash_tag}'

        io = self._backend.io
        old_keys = io.keys(old_prefix + '*')
        for old_key in old_keys:
            new_key = new_prefix + old_key[old_prefix_len:]
            io.rename(old_key, new_key)
        # æ›´æ–°meta
        io.hset(self._meta_key, 'cluster_id', self._cluster_id)
        logger.warning(
            f"âœ… [ğŸ’¾Redis][{self._name}ç»„ä»¶] cluster è¿ç§»å®Œæˆï¼Œå…±è¿ç§»{len(old_keys)}ä¸ªé”®å€¼ã€‚")

    def _migration_schema(self, old):
        """å¦‚æœæ•°æ®åº“ä¸­çš„å±æ€§å’Œå®šä¹‰ä¸ä¸€è‡´ï¼Œå°è¯•è¿›è¡Œç®€å•è¿ç§»ï¼Œå¯ä»¥å¤„ç†å±æ€§æ›´åä»¥å¤–çš„æƒ…å†µã€‚"""
        # åŠ è½½è€çš„ç»„ä»¶
        old_comp_cls = BaseComponent.load_json(old)

        # åªæœ‰propertiesåå­—å’Œç±»å‹å˜æ›´æ‰è¿ç§»
        dtypes_in_db = old_comp_cls.dtypes
        new_dtypes = self._component_cls.dtypes
        if dtypes_in_db == new_dtypes:
            return

        logger.warning(f"âš ï¸ [ğŸ’¾Redis][{self._name}ç»„ä»¶] ä»£ç å®šä¹‰çš„Schemaä¸å·²å­˜çš„ä¸ä¸€è‡´ï¼Œ"
                       f"æ•°æ®åº“ä¸­ï¼š\n"
                       f"{dtypes_in_db}\n"
                       f"ä»£ç å®šä¹‰çš„ï¼š\n"
                       f"{new_dtypes}\n "
                       f"å°†å°è¯•æ•°æ®è¿ç§»ï¼ˆåªå¤„ç†æ–°å±æ€§ï¼Œä¸å¤„ç†ç±»å‹å˜æ›´ï¼Œæ”¹åç­‰ç­‰æƒ…å†µï¼‰ï¼š")

        # todo è°ƒç”¨è‡ªå®šä¹‰ç‰ˆæœ¬è¿ç§»ä»£ç ï¼ˆdefine_migrationï¼‰

        # æ£€æŸ¥æ˜¯å¦æœ‰å±æ€§è¢«åˆ é™¤
        for prop_name in dtypes_in_db.fields:
            if prop_name not in new_dtypes.fields:
                logger.warning(f"âš ï¸ [ğŸ’¾Redis][{self._name}ç»„ä»¶] "
                               f"æ•°æ®åº“ä¸­çš„å±æ€§ {prop_name} åœ¨æ–°çš„ç»„ä»¶å®šä¹‰ä¸­ä¸å­˜åœ¨ï¼Œå¦‚æœæ”¹åäº†éœ€è¦æ‰‹åŠ¨è¿ç§»ï¼Œ"
                               f"é»˜è®¤ä¸¢å¼ƒè¯¥å±æ€§æ•°æ®ã€‚")

        # å¤šå‡ºæ¥çš„åˆ—å†æ¬¡æŠ¥è­¦å‘Šï¼Œç„¶åå¿½ç•¥
        io = self._backend.io
        rows = io.keys(self._key_prefix + '*')
        props = dict(self._component_cls.properties_)  # type: dict[str, Property]
        added = 0
        for prop_name in new_dtypes.fields:
            if prop_name not in dtypes_in_db.fields:
                logger.warning(f"âš ï¸ [ğŸ’¾Redis][{self._name}ç»„ä»¶] "
                               f"æ–°çš„ä»£ç å®šä¹‰ä¸­å¤šå‡ºå±æ€§ {prop_name}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼å¡«å……ã€‚")
                default = props[prop_name].default
                if default is None:
                    logger.error(f"âš ï¸ [ğŸ’¾Redis][{self._name}ç»„ä»¶] "
                                 f"è¿ç§»æ—¶å°è¯•æ–°å¢ {prop_name} å±æ€§å¤±è´¥ï¼Œè¯¥å±æ€§æ²¡æœ‰é»˜è®¤å€¼ï¼Œæ— æ³•æ–°å¢ã€‚")
                    raise ValueError("è¿ç§»å¤±è´¥")
                pipe = io.pipeline()
                for row in rows:
                    pipe.hset(row, prop_name, default)
                pipe.execute()
                added += 1
        logger.warning(f"âœ… [ğŸ’¾Redis][{self._name}ç»„ä»¶] æ–°å±æ€§å¢åŠ å®Œæˆï¼Œå…±å¤„ç†{len(rows)}è¡Œ * "
                       f"{added}ä¸ªå±æ€§ã€‚")

    @classmethod
    def make_query_cmd(
            cls, component_cls, index_name: str, left, right, limit, desc
    ) -> dict[str, str] | list[int]:
        """ç”Ÿæˆæ•°æ®åº“æŸ¥è¯¢å‘½ä»¤ï¼Œåœ¨direct_queryå’Œ_db_queryä¸­å¤ç”¨æ­¤ç±»ã€‚"""
        if right is None:
            right = left
            if index_name == 'id':
                return [int(left)]
        if desc:
            left, right = right, left

        # å¯¹äºstrç±»å‹æŸ¥è¯¢ï¼Œè¦ç”¨[å¼€å§‹
        str_type = component_cls.indexes_[index_name]
        by_lex = False
        if str_type:
            assert type(left) is str and type(right) is str, \
                f"å­—ç¬¦ä¸²ç±»å‹ç´¢å¼•`{index_name}`çš„æŸ¥è¯¢(left={left}, {type(left)})å˜é‡ç±»å‹å¿…é¡»æ˜¯str"
            if not left.startswith(('(', '[')):
                left = f'[{left}'
            if not right.startswith(('(', '[')):
                right = f'[{right}'

            if not left.endswith((':', ';')):
                left = f'{left}:'  # name:id å½¢å¼ï¼Œæ‰€ä»¥:ä½œä¸ºç»“å°¾æ ‡è¯†ç¬¦
            if not right.endswith((':', ';')):
                right = f'{right};'  # ';' = 3B, ':' = 3A

            by_lex = True

        return {'start': left, 'end': right, 'desc': desc, 'offset': 0,
                'num': limit, 'bylex': by_lex, 'byscore': not by_lex}

    async def direct_query(
            self,
            index_name: str,
            left,
            right=None,
            limit=10,
            desc=False,
            row_format='struct',
    ) -> np.recarray | list[dict | int]:
        replica = self._backend.random_replica()
        idx_key = self._idx_prefix + index_name

        cmds = RedisComponentTable.make_query_cmd(
            self._component_cls, index_name, left, right, limit, desc)

        if type(cmds) is list:  # å¦‚æœæ˜¯listè¯´æ˜ä¸éœ€è¦æŸ¥è¯¢ç›´æ¥è¿”å›id
            row_ids = cmds
        else:
            row_ids = await replica.zrange(name=idx_key, **cmds)
            str_type = self._component_cls.indexes_[index_name]
            if str_type:
                row_ids = [vk.split(':')[-1] for vk in row_ids]

        if row_format == 'id':
            return list(map(int, row_ids))
        raw = row_format == 'raw'

        key_prefix = self._key_prefix
        rows = []
        for _id in row_ids:
            row = await replica.hgetall(key_prefix + str(_id))
            if raw:
                rows.append(row)
            else:
                rows.append(self._component_cls.dict_to_row(row))

        if raw:
            return rows
        else:
            if len(rows) == 0:
                return np.rec.array(np.empty(0, dtype=self._component_cls.dtypes))
            else:
                return np.rec.array(np.stack(rows, dtype=self._component_cls.dtypes))

    async def direct_get(self, row_id: int) -> None | np.record:
        replica = self._backend.random_replica()
        key = self._key_prefix + str(row_id)
        row = await replica.hgetall(key)
        if row:
            return self._component_cls.dict_to_row(row)
        else:
            return None

    async def direct_set(self, row_id: int, **kwargs):
        aio = self._backend.aio
        key = self._key_prefix + str(row_id)

        for prop in kwargs:
            if prop in self._component_cls.indexes_:
                raise ValueError(f"ç´¢å¼•å­—æ®µ`{prop}`ä¸å…è®¸ç”¨direct_setä¿®æ”¹")
        await aio.hset(key, mapping=kwargs)

    def attach(self, backend_trx: RedisTransaction) -> 'RedisComponentTransaction':
        # è¿™é‡Œä¸ç”¨æ£€æŸ¥cluster_idï¼Œå› ä¸ºComponentTransactionä¼šæ£€æŸ¥
        # assert backend_trx.cluster_id == self._cluster_id
        return RedisComponentTransaction(
            self, backend_trx, self._key_prefix, self._idx_prefix)

    def channel_name(self, index_name: str = None, row_id: int = None):
        dbi = self._backend.dbi
        if index_name:
            return f'__keyspace@{dbi}__:{self._idx_prefix + index_name}'
        elif row_id is not None:
            return f'__keyspace@{dbi}__:{self._key_prefix + str(row_id)}'
        raise ValueError("index_nameå’Œrow_idå¿…é¡»æœ‰ä¸€ä¸ª")


class RedisComponentTransaction(ComponentTransaction):
    def __init__(
            self,
            comp_tbl: RedisComponentTable,
            trx_conn: RedisTransaction,
            key_prefix: str,
            index_prefix: str
    ):
        super().__init__(comp_tbl, trx_conn)
        self._trx_conn = trx_conn  # ä¸ºäº†è®©ä»£ç æç¤ºçŸ¥é“ç±»å‹æ˜¯RedisTransaction

        self._key_prefix = key_prefix
        self._idx_prefix = index_prefix

    async def _db_get(self, row_id: int) -> None | np.record:
        # è·å–è¡Œæ•°æ®çš„æ“ä½œ
        key = self._key_prefix + str(row_id)
        pipe = self._trx_conn.pipe

        # åŒæ—¶è¦è®©ä¹è§‚é”é”å®šè¯¥è¡Œ
        await pipe.watch(key)
        # è¿”å›å€¼è¦é€šè¿‡dict_to_rowåŒ…è£¹ä¸‹
        if row := await pipe.hgetall(key):
            return self._component_cls.dict_to_row(row)
        else:
            return None

    async def _db_query(
            self,
            index_name: str,
            left,
            right=None,
            limit=10,
            desc=False,
            lock_index=True
    ) -> list[int]:
        idx_key = self._idx_prefix + index_name
        pipe = self._trx_conn.pipe

        cmds = RedisComponentTable.make_query_cmd(
            self._component_cls, index_name, left, right, limit, desc)

        if type(cmds) is list:  # å¦‚æœæ˜¯listè¯´æ˜ä¸éœ€è¦æŸ¥è¯¢ç›´æ¥è¿”å›id
            return cmds

        if lock_index:
            await pipe.watch(idx_key)
        row_ids = await pipe.zrange(name=idx_key, **cmds)

        str_type = self._component_cls.indexes_[index_name]
        if str_type:
            row_ids = [vk.split(':')[-1] for vk in row_ids]

        # æœªæŸ¥è¯¢åˆ°æ•°æ®æ—¶è¿”å›[]
        return list(map(int, row_ids))

    def _trx_check_unique(self, old_row, new_row: np.record) -> None:
        trx = self._trx_conn
        component_cls = self._component_cls
        idx_prefix = self._idx_prefix

        for idx in component_cls.uniques_:
            if idx == 'id':  # insertä¸éœ€è¦æ£€æŸ¥id, updateä¹Ÿä¸éœ€è¦å› ä¸ºåŸºç±»é‡Œä¼šç¡®è®¤idä¸€æ ·
                continue
            if old_row is None or old_row[idx] != new_row[idx]:
                key = idx_prefix + idx
                str_type = component_cls.indexes_[idx]
                if str_type:
                    trx.stack_unique_check(key, '[' + new_row[idx].item())
                else:
                    trx.stack_unique_check(key, new_row[idx].item())

    def _trx_insert(self, row: np.record) -> None:
        trx = self._trx_conn
        component_cls = self._component_cls
        idx_prefix = self._idx_prefix

        self._trx_check_unique(None, row)
        # å¼€å§‹è‡ªå¢idæ¨¡å¼, å¹¶ç”¨placeholder {rowid}æ›¿æ¢id
        trx.stack_cmd('AUTO_INCR', idx_prefix + 'id')
        row_id = '{rowid}'
        row_key = self._key_prefix + str(row_id)
        # è®¾ç½®rowæ•°æ®
        kvs = itertools.chain.from_iterable(zip(row.dtype.names, row.tolist()))
        trx.stack_cmd('hset', row_key, *kvs)
        # æ›´æ–°ç´¢å¼•
        for idx_name, str_type in component_cls.indexes_.items():
            idx_key = idx_prefix + idx_name
            if str_type:
                trx.stack_cmd('zadd', idx_key, 0, f'{row[idx_name]}:{row_id}')
            elif idx_name == 'id':
                trx.stack_cmd('zadd', idx_key, row_id, row_id)
            else:
                trx.stack_cmd('zadd', idx_key, row[idx_name].item(), row_id)
        # ç»“æŸè‡ªå¢idæ¨¡å¼
        trx.stack_cmd('hset', row_key, 'id', row_id)
        trx.stack_cmd('END_INCR')

    def _trx_update(self, row_id: int, old_row: np.record, new_row: np.record) -> None:
        trx = self._trx_conn
        component_cls = self._component_cls
        idx_prefix = self._idx_prefix

        self._trx_check_unique(old_row, new_row)
        # æ›´æ–°rowæ•°æ®
        row_key = self._key_prefix + str(row_id)
        kvs = itertools.chain.from_iterable(zip(new_row.dtype.names, new_row.tolist()))
        trx.stack_cmd('hset', row_key, *kvs)
        # æ›´æ–°ç´¢å¼•
        for idx_name, str_type in component_cls.indexes_.items():
            if old_row[idx_name] == new_row[idx_name]:
                continue
            idx_key = idx_prefix + idx_name
            if str_type:
                trx.stack_cmd('zadd', idx_key, 0, f'{new_row[idx_name]}:{row_id}')
                trx.stack_cmd('zrem', idx_key, f'{old_row[idx_name]}:{row_id}')
            elif idx_name == 'id':
                trx.stack_cmd('zadd', idx_key, row_id, row_id)
            else:
                trx.stack_cmd('zadd', idx_key, new_row[idx_name].item(), row_id)

    def _trx_delete(self, row_id: int, old_row: np.record) -> None:
        trx = self._trx_conn
        component_cls = self._component_cls
        idx_prefix = self._idx_prefix

        row_key = self._key_prefix + str(row_id)
        trx.stack_cmd('del', row_key)

        for idx_name, str_type in component_cls.indexes_.items():
            idx_key = idx_prefix + idx_name
            if str_type:
                trx.stack_cmd('zrem', idx_key, f'{old_row[idx_name]}:{row_id}')
            else:
                trx.stack_cmd('zrem', idx_key, row_id)


##############################


class RedisMQClient(MQClient):
    """è¿æ¥åˆ°æ¶ˆæ¯é˜Ÿåˆ—çš„å®¢æˆ·ç«¯ï¼Œæ¯ä¸ªç”¨æˆ·è¿æ¥ä¸€ä¸ªå®ä¾‹ã€‚"""

    def __init__(self, redis_conn: redis.asyncio.Redis | redis.asyncio.RedisCluster):
        # todo è¦æµ‹è¯•redis clusteræ˜¯å¦èƒ½æ­£å¸¸pub sub
        # 2ç§æ¨¡å¼ï¼š
        # a. æ¯ä¸ªwsè¿æ¥ä¸€ä¸ªpubsubè¿æ¥ï¼Œè¿™æ ·servantså¯ä»¥å‡åŒ€çš„è´Ÿè½½ï¼Œä¹Ÿæ²¡æœ‰åˆ†å‘è´Ÿæ‹…ï¼Œç›®å‰çš„æ¨¡å¼
        # b. æ¯ä¸ªworkerä¸€ä¸ªpubsubè¿æ¥ï¼Œæ”¶åˆ°æ¶ˆæ¯çš„åˆ†å‘äº¤ç»™workeræ¥åšï¼Œè¿™æ ·è¿æ¥æ•°è¾ƒå°‘ï¼Œä½†servantsæ•°åªèƒ½<=workeræ•°
        self._mq = redis_conn.pubsub()

    async def close(self):
        return await self._mq.aclose()

    async def subscribe(self, channel_name) -> None:
        await self._mq.subscribe(channel_name)

    async def unsubscribe(self, channel_name) -> None:
        await self._mq.unsubscribe(channel_name)

    async def get_message(self) -> set[str]:
        """
        ä»æ¶ˆæ¯é˜Ÿåˆ—è·å–ä¸€æ¡æ¶ˆæ¯ã€‚è¿”å›å€¼ä¸ºæ”¶åˆ°æ¶ˆæ¯çš„channel_nameåˆ—è¡¨ã€‚
        æ¯ä¸ªchannelå¯¹åº”ä¸€æ¡æ•°æ®ï¼Œchannelæ”¶åˆ°äº†ä»»ä½•æ¶ˆæ¯éƒ½è¯´æ˜æœ‰æ•°æ®æ›´æ–°ã€‚
        æœ¬æ–¹æ³•å¹¶ä¸å®æ—¶è¿”å›ï¼Œé‡åˆ°æ¶ˆæ¯ä¼šç­‰å¾…ä¸€ä¼šå†åˆå¹¶ï¼Œé˜²æ­¢é¢‘ç¹å˜åŠ¨çš„æ•°æ®ã€‚
        """
        # å¦‚æœæ²¡è®¢é˜…è¿‡å†…å®¹ï¼Œé‚£ä¹ˆredis mqçš„connectionæ˜¯Noneï¼Œæ— éœ€get_message
        if self._mq.connection is None:
            await asyncio.sleep(0.5)  # ä¸å†™åç¨‹å°±æ­»é”äº†
            return set()

        rtn = set()
        start_clock = time.monotonic()
        while True:
            msg = await self._mq.get_message(ignore_subscribe_messages=True, timeout=0.5)
            if msg is None:
                # ç­‰å¾…è‡³å°‘0.5ç§’ï¼Œæ¥åˆå¹¶æ‰¹é‡æ¶ˆæ¯
                if (time.monotonic() - start_clock) >= 0.5:
                    break
            else:
                rtn.add(msg['channel'])
        return rtn

    @property
    def subscribed_channels(self) -> set[str]:
        return set(self._mq.channels) - set(self._mq.pending_unsubscribe_channels)
