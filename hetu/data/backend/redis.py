"""
@author: Heerozh (Zhang Jianhao)
@copyright: Copyright 2024, Heerozh. All rights reserved.
@license: Apache2.0 å¯ç”¨ä½œå•†ä¸šé¡¹ç›®ï¼Œå†éšä¾¿æ‰¾ä¸ªè§’è½æåŠç”¨åˆ°äº†æ­¤é¡¹ç›® :D
@email: heeroz@gmail.com
"""
import numpy as np
import random
import hashlib
import redis
import asyncio
from datetime import datetime, timedelta
from sanic.log import logger
from ..component import BaseComponent, Property
from .base import ComponentTable, RaceCondition


def get_coroutine_executor():
    try:
        asyncio.get_running_loop()
    except RuntimeError:
        return asyncio.run
    else:
        return asyncio.create_task


class RedisBackend:
    def __init__(self, config: dict):
        # åŒæ­¥ioè¿æ¥, å¼‚æ­¥ioè¿æ¥, åªè¯»ioè¿æ¥
        self.io = redis.from_url(config['master'], decode_responses=True)
        self.aio = redis.asyncio.from_url(config['master'], decode_responses=True)
        servants = config.get('servants', [])
        if servants:
            self.replicas = [redis.asyncio.from_url(url, decode_responses=True)
                             for url in config['servants']]
        else:
            self.replicas = [self.aio]

        # é…ç½®keyspaceé€šçŸ¥
        run_async = get_coroutine_executor()
        for replica in self.replicas:
            run_async(replica.config_set('notify-keyspace-events', 'Kghz'))

    def rnd_replica(self):
        """æ¯ä¸ªwebsocketè¿æ¥è·å¾—ä¸€ä¸ªéšæœºçš„replicaè¿æ¥ï¼Œç”¨äºè¯»å–è®¢é˜…"""
        i = random.randint(0, len(self.replicas))
        return i, self.replicas[i]


class RedisComponentTable(ComponentTable):
    """
    ä½¿ç”¨rediså®ç°çš„Componentåç«¯ã€‚

    å‚è€ƒï¼š
    redis-pyååé‡åŸºå‡†ï¼š
    syncè°ƒç”¨ï¼šå•è¿›ç¨‹ï¼š1200/sï¼Œ10è¿›ç¨‹ç†è®ºä¸Š12 Kopsï¼Œç¬¦åˆhiredisåŸºå‡†æµ‹è¯•
    asyncè°ƒç”¨ï¼šå•è¿›ç¨‹+Semaphoreé™åˆ¶100åç¨‹ï¼š6000/sï¼Œ å‚è€ƒQPS 100,000

    ä½¿ç”¨ä»¥ä¸‹keysï¼š
    instance_name:component_name.{CLU0}:id:1~n
    instance_name:component_name.{CLU0}:index:key~
    instance_name:component_name:meta
    """

    def __init__(self, component_cls: type[BaseComponent], instance_name, cluster_id,
                 backend: RedisBackend):
        super().__init__(component_cls, instance_name, cluster_id, backend)
        component_cls.hosted_ = self
        # redis keyå
        hash_tag = f'{{CLU{cluster_id}}}'
        # ä¸èƒ½ç”¨component_cls.__name__ å¯èƒ½æ˜¯jsonåŠ è½½çš„åå­—ä¸å¯¹
        self._name = component_cls.components_name_
        self._root_prefix = f'{instance_name}:{self._name}:'
        self._key_prefix = f'{self._root_prefix}{hash_tag}:id:'
        self._idx_prefix = f'{self._root_prefix}{hash_tag}:index:'
        self._lock_key = f'{self._root_prefix}:init_lock'
        self._meta_key = f'{self._root_prefix}:meta'
        self._trans_pipe = None
        self._autoinc = None
        # æ£€æµ‹metaä¿¡æ¯ï¼Œç„¶ååšå¯¹åº”å¤„ç†
        self.check_meta()

    def check_meta(self):
        """
        æ£€æŸ¥metaä¿¡æ¯ï¼Œç„¶ååšå¯¹åº”å¤„ç†
        metaæ ¼å¼:
        json: è¡¨çš„ç»“æ„ä¿¡æ¯
        version: jsonçš„hash
        cluster_id: æ‰€å±ç°‡id
        last_index_rebuild: ä¸Šæ¬¡é‡å»ºç´¢å¼•æ—¶é—´
        """
        io = self._backend.io
        lock = io.lock(self._lock_key)
        lock.acquire(blocking=True)

        # è·å–rediså·²å­˜çš„è¡¨ä¿¡æ¯
        meta = io.hgetall(self._meta_key)
        if not meta:
            meta = self._create_emtpy()
        else:
            version = hashlib.md5(self._component_cls.json_.encode("utf-8")).hexdigest()
            # å¦‚æœcluster_idæ”¹å˜ï¼Œåˆ™è¿ç§»æ”¹keyå
            if int(meta['cluster_id']) != self._cluster_id:
                self._migration_cluster_id(old=int(meta['cluster_id']))

            # å¦‚æœç‰ˆæœ¬ä¸ä¸€è‡´ï¼Œè¡¨ç»“æ„å¯èƒ½æœ‰å˜åŒ–ï¼Œä¹Ÿå¯èƒ½åªæ˜¯æ”¹æƒé™ï¼Œæ€»ä¹‹è°ƒç”¨è¿ç§»ä»£ç 
            if meta['version'] != version:
                self._migration_schema(old=meta['json'])

        # é‡å»ºæ•°æ®ï¼Œæ¯æ¬¡å¯åŠ¨é—´éš”è¶…è¿‡1å°æ—¶å°±é‡å»º
        last_index_rebuild = datetime.fromisoformat(meta.get('last_index_rebuild'))
        now = datetime.now().astimezone()
        if last_index_rebuild <= now - timedelta(hours=1):
            # å¦‚æœéæŒä¹…åŒ–è¡¨ï¼Œåˆ™æ¯æ¬¡å¯åŠ¨æ¸…ç©º
            if not self._component_cls.persist_:
                logger.info(f"âŒš [ğŸ’¾Redis] {self._name}è¡¨ æ— éœ€æŒä¹…åŒ–ï¼Œæ¸…ç©ºä¸­...")
                del_keys = io.keys(self._root_prefix + '*')
                map(io.delete, del_keys)

            # é‡å»ºç´¢å¼•ï¼Œå¦‚æœå·²å¤„ç†è¿‡äº†å°±ä¸å¤„ç†
            self._rebuild_index()
            # å†™å…¥metaä¿¡æ¯
            io.hset(self._meta_key, 'last_index_rebuild', now.isoformat())

        lock.release()

    def _create_emtpy(self):
        logger.info(f"â„¹ï¸ [ğŸ’¾Redis] {self._name}è¡¨æ— metaä¿¡æ¯ï¼Œæ­£åœ¨é‡æ–°åˆ›å»º...")

        # åªéœ€è¦å†™å…¥metaï¼Œå…¶ä»–çš„_rebuild_indexä¼šåˆ›å»º
        meta = {
            'json': self._component_cls.json_,
            'version': hashlib.md5(self._component_cls.json_.encode("utf-8")).hexdigest(),
            'cluster_id': self._cluster_id,
            'last_index_rebuild': '2024-06-19T03:41:18.682529+08:00'
        }
        self._backend.io.hset(self._meta_key, mapping=meta)
        return meta

    def _rebuild_index(self):
        logger.info(f"âŒš [ğŸ’¾Redis] {self._name}è¡¨ æ­£åœ¨é‡å»ºç´¢å¼•...")
        io = self._backend.io
        rows = io.keys(self._key_prefix + '*')
        if len(rows) == 0:
            logger.info(f"âœ… [ğŸ’¾Redis] {self._name}è¡¨ æ— æ•°æ®ï¼Œæ— éœ€é‡å»ºç´¢å¼•ã€‚")
            return

        for idx_name, str_type in self._component_cls.indexes_.items():
            idx_key = self._idx_prefix + idx_name
            # å…ˆåˆ é™¤æ‰€æœ‰_idx_keyå¼€å¤´çš„ç´¢å¼•
            io.delete(idx_key)
            # é‡å»ºæ‰€æœ‰ç´¢å¼•ï¼Œä¸ç®¡uniqueè¿˜æ˜¯indexéƒ½æ˜¯sset
            pipe = io.pipeline()
            row_ids = []
            for row in rows:
                row_id = row.split(':')[-1]
                row_ids.append(row_id)
                io.hget(row, idx_name)
            values = pipe.execute()
            if str_type:
                # å­—ç¬¦ä¸²ç±»å‹è¦ç‰¹æ®Šå¤„ç†ï¼Œscore=0, member='name:1'å½¢å¼
                io.zadd(idx_key, {f'{value}:{rid}': 0 for rid, value in zip(row_ids, values)})
            else:
                # zadd ä¼šæ›¿æ¢æ‰memberç›¸åŒçš„å€¼ï¼Œç­‰äºæ˜¯set
                io.zadd(idx_key, dict(zip(row_ids, values)))

    def _migration_cluster_id(self, old):
        logger.warning(f"âš ï¸ [ğŸ’¾Redis] {self._name}è¡¨ cluster_id ç”± {old} å˜æ›´ä¸º {self._cluster_id}ï¼Œ"
                       f"å°†å°è¯•è¿ç§»clusteræ•°æ®...")
        # é‡å‘½åkey
        old_hash_tag = f'{{CLU{old}}}'
        new_hash_tag = f'{{CLU{self._cluster_id}}}'
        old_prefix = f'{self._root_prefix}{old_hash_tag}:'
        old_prefix_len = len(old_prefix)
        new_prefix = f'{self._root_prefix}{new_hash_tag}:'

        io = self._backend.io
        old_keys = io.keys(old_prefix + '*')
        for old_key in old_keys:
            new_key = new_prefix + old_key[old_prefix_len:]
            io.rename(new_key, new_key)
        # æ›´æ–°meta
        io.hset(self._meta_key, 'cluster_id', self._cluster_id)

    def _migration_schema(self, old):
        """å¦‚æœæ•°æ®åº“ä¸­çš„å±æ€§å’Œå®šä¹‰ä¸ä¸€è‡´ï¼Œå°è¯•è¿›è¡Œç®€å•è¿ç§»ï¼Œå¯ä»¥å¤„ç†å±æ€§æ›´åä»¥å¤–çš„æƒ…å†µã€‚"""
        # åŠ è½½è€çš„è¡¨
        old_comp_cls = BaseComponent.load_json(old)

        # åªæœ‰propertiesåå­—å’Œç±»å‹å˜æ›´æ‰è¿ç§»
        dtypes_in_db = old_comp_cls.dtypes
        new_dtypes = self._component_cls.dtypes
        if dtypes_in_db == new_dtypes:
            return

        logger.warning(f"âš ï¸ [ğŸ’¾Redis] {self._name}è¡¨ ä»£ç å®šä¹‰ä¸å­˜æ¡£ä¸ä¸€è‡´ï¼Œ"
                       f"å­˜æ¡£ï¼š\n"
                       f"{dtypes_in_db}\n"
                       f"ä»£ç å®šä¹‰çš„ï¼š\n"
                       f"{new_dtypes}\n "
                       f"å°†å°è¯•è¿ç§»æ•°æ®ï¼š")

        # todo è°ƒç”¨è‡ªå®šä¹‰ç‰ˆæœ¬è¿ç§»ä»£ç ï¼ˆdefine_migrationï¼‰

        # æ£€æŸ¥æ˜¯å¦æœ‰å±æ€§è¢«åˆ é™¤
        for prop_name in dtypes_in_db.fields:
            if prop_name not in new_dtypes.fields:
                logger.warning(f"âš ï¸ [ğŸ’¾Redis] {self._name}è¡¨ "
                               f"æ–°çš„ä»£ç å®šä¹‰ä¸­ç¼ºå°‘å±æ€§ {prop_name}ï¼Œå¦‚æœæ”¹åäº†éœ€è¦æ‰‹åŠ¨è¿ç§»ï¼Œ"
                               f"é»˜è®¤ä¸¢å¼ƒè¯¥å±æ€§æ•°æ®ã€‚")

        # å¤šå‡ºæ¥çš„åˆ—å†æ¬¡æŠ¥è­¦å‘Šï¼Œç„¶åå¿½ç•¥
        io = self._backend.io
        props = dict(self._component_cls.properties_)  # type: dict[str, Property]
        for prop_name in new_dtypes.fields:
            if prop_name not in dtypes_in_db.fields:
                logger.warning(f"âš ï¸ [ğŸ’¾Redis] {self._name}è¡¨ "
                               f"æ–°çš„ä»£ç å®šä¹‰ä¸­å¤šå‡ºå±æ€§ {prop_name}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼å¡«å……ã€‚")
                default = props[prop_name].default
                if default is None:
                    logger.error(f"âš ï¸ [ğŸ’¾Redis] {self._name}è¡¨ "
                                 f"è¿ç§»æ—¶å°è¯•æ–°å¢ {prop_name} å±æ€§å¤±è´¥ï¼Œè¯¥å±æ€§æ²¡æœ‰é»˜è®¤å€¼ï¼Œæ— æ³•æ–°å¢ã€‚")
                    raise ValueError("è¿ç§»å¤±è´¥")
                pipe = io.pipeline()
                rows = io.keys(self._key_prefix + '*')
                for row in rows:
                    pipe.hset(row, prop_name, default)
                pipe.execute()

    def begin_transaction(self):
        super().begin_transaction()
        self._autoinc = -1
        self._trans_pipe = self._backend.aio.pipeline(transaction=True)
        # å¼ºåˆ¶pipelineè¿›å…¥ç«‹å³æ¨¡å¼ï¼Œä¸ç„¶å½“æˆ‘ä»¬éœ€è¦è¯»å–æœªé”å®šçš„indexæ—¶ï¼Œä¼šä¸è¿”å›ç»“æœ
        self._trans_pipe.watching = True

    async def end_transaction(self, discard):
        # å¹¶å®ç°äº‹åŠ¡æäº¤çš„æ“ä½œï¼Œå°†_updatesä¸­çš„å‘½ä»¤å†™å…¥äº‹åŠ¡
        if discard:
            self._trans_pipe.discard()
            self._trans_pipe = None
            return True

        pipe = self._trans_pipe

        # å¯¹unique indexè¿›è¡Œæœ€ç»ˆæ£€æŸ¥ï¼Œä¹‹å‰è™½ç„¶æ£€æŸ¥è¿‡ï¼Œä½†æ²¡æœ‰lock indexï¼Œ
        # æ­¤æ¬¡æ£€æŸ¥ä¼šé”å®šindexï¼Œåœ¨æœ€åæ‰é”å®šindexå¯ä»¥é™ä½äº‹åŠ¡å†²çªæ¦‚ç‡
        locked_indexes = set()
        for idx in self._component_cls.uniques_:
            for cmd, _, old_row, new_row in self._updates:
                if (cmd == 'update' and old_row[idx] != new_row[idx]) or cmd == 'insert':
                    if idx not in locked_indexes:
                        await pipe.watch(self._idx_prefix + idx)
                        locked_indexes.add(idx)
                    if len(await self._backend_query(idx, new_row[idx].item(), limit=1)) > 0:
                        raise RaceCondition()

        # æ‰§è¡Œäº‹åŠ¡
        pipe.multi()
        for cmd, row_id, old_row, new_row in self._updates:
            if cmd == 'delete':
                row_key = self._key_prefix + str(row_id)
                pipe.delete(row_key)
                for idx_name, str_type in self._component_cls.indexes_.items():
                    idx_key = self._idx_prefix + idx_name
                    if str_type:
                        pipe.zrem(idx_key, f'{old_row[idx_name]}:{row_id}')
                    else:
                        pipe.zrem(idx_key, row_id)
            else:
                # æ’å…¥/æ›´æ–°
                row_key = self._key_prefix + str(row_id)
                dict_row = dict(zip(new_row.dtype.names, new_row.tolist()))
                pipe.hset(row_key, mapping=dict_row)
                for idx_name, str_type in self._component_cls.indexes_.items():
                    idx_key = self._idx_prefix + idx_name
                    if str_type:
                        # å…ˆåˆ é™¤è€æ•°æ®
                        if cmd == 'update':
                            pipe.zrem(idx_key, f'{old_row[idx_name]}:{row_id}')
                        pipe.zadd(idx_key, {f'{dict_row[idx_name]}:{row_id}': 0})
                    else:
                        pipe.zadd(idx_key, {str(row_id): dict_row[idx_name]})

        try:
            await pipe.execute()
        except redis.WatchError:
            self._trans_pipe = None
            return False
        self._trans_pipe = None
        return True

    async def _backend_get(self, row_id: int):
        # è·å–è¡Œæ•°æ®çš„æ“ä½œ
        key = self._key_prefix + str(row_id)
        pipe = self._trans_pipe

        # åŒæ—¶è¦è®©ä¹è§‚é”é”å®šè¯¥è¡Œ
        await pipe.watch(key)
        # è¿”å›å€¼è¦é€šè¿‡dict_to_rowåŒ…è£¹ä¸‹
        row = await pipe.hgetall(key)
        if row:
            return self._component_cls.dict_to_row(row)
        else:
            return None

    async def _backend_get_max_id(self):
        if self._autoinc >= 0:
            return self._autoinc + sum([1 for cmd, _, _, _ in self._updates if cmd == 'insert'])

        # è·å–æœ€å¤§idçš„æ“ä½œ
        idx_key = self._idx_prefix + 'id'
        pipe = self._trans_pipe

        max_score = await pipe.zrange(idx_key, 0, 0, desc=True, withscores=True)
        max_score = max_score[0][1] if max_score else 0
        self._autoinc = max_score
        return max_score

    async def _backend_query(self, index_name: str, left, right=None, limit=10, desc=False):
        # èŒƒå›´æŸ¥è¯¢çš„æ“ä½œï¼Œè¿”å›List[int] of row_idã€‚å¦‚æœä½ çš„æ•°æ®åº“åŒæ—¶è¿”å›äº†æ•°æ®ï¼Œå¯ä»¥å­˜åˆ°_cacheä¸­
        idx_key = self._idx_prefix + index_name
        pipe = self._trans_pipe

        if right is None:
            right = left
        if desc:
            left, right = right, left

        # å¯¹äºstrç±»å‹æŸ¥è¯¢ï¼Œè¦ç”¨[å¼€å§‹
        str_type = self._component_cls.indexes_[index_name]
        by_lex = False
        if str_type:
            left = type(left) is np.str_ and left.item() or left
            right = type(right) is np.str_ and right.item() or right
            assert type(left) is str and type(right) is str, \
                f"å­—ç¬¦ä¸²ç±»å‹ç´¢å¼•`{index_name}`çš„æŸ¥è¯¢(left={left}, {type(left)})å˜é‡ç±»å‹å¿…é¡»æ˜¯str"
            if not left.startswith(('(', '[')):
                left = f'[{left}'
            if not right.startswith(('(', '[')):
                right = f'[{right}'

            if left == right:  # å¦‚æœæ˜¯ç²¾ç¡®æŸ¥è¯¢
                left = f'{left}:'  # name:id å½¢å¼ï¼Œæ‰€ä»¥:ä½œä¸ºç»“å°¾æ ‡è¯†ç¬¦
                right = f'{right};'  # ';' = 3B, ':' = 3A

            by_lex = True

        row_ids = await pipe.zrange(idx_key, left, right, desc=desc, offset=0, num=limit,
                                    byscore=not by_lex, bylex=by_lex)

        if str_type:
            row_ids = [vk.split(':')[-1] for vk in row_ids]

        # åŒæ—¶é”å®šæ‰€æœ‰è¯»å–çš„è¡Œ
        # row_keys = [self._key_prefix + row_id for row_id in row_ids]
        # await asyncio.gather(*[pipe.watch(row_key) for row_key in row_keys])
        # rtn = await asyncio.gather(*[pipe.hgetall(row_key) for row_key in row_keys])

        # æœªæŸ¥è¯¢åˆ°æ•°æ®æ—¶è¿”å›[]
        return list(map(int, row_ids))




