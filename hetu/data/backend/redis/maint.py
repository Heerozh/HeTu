"""
@author: Heerozh (Zhang Jianhao)
@copyright: Copyright 2024, Heerozh. All rights reserved.
@license: Apache2.0 å¯ç”¨ä½œå•†ä¸šé¡¹ç›®ï¼Œå†éšä¾¿æ‰¾ä¸ªè§’è½æåŠç”¨åˆ°äº†æ­¤é¡¹ç›® :D
@email: heeroz@gmail.com
"""

import hashlib
import logging
import warnings
from typing import TYPE_CHECKING, cast, final, override

import numpy as np
from redis.cluster import RedisCluster

from ....common.helper import batched
from ...component import BaseComponent
from ..base import TableMaintenance
from ..table import TableReference

if TYPE_CHECKING:
    import redis
    import redis.lock

    from .client import RedisBackendClient

logger = logging.getLogger("HeTu.root")


@final
class RedisTableMaintenance(TableMaintenance):
    """
    è¡¨ç»´æŠ¤ç±»ï¼ŒæœåŠ¡å™¨å¯åŠ¨æ—¶ä¼šè°ƒç”¨æ­¤ç±»æ£€æŸ¥ç»„ä»¶è¡¨çŠ¶æ€ï¼Œå¹¶åˆ›å»ºä¸å­˜åœ¨çš„è¡¨ã€‚
    å¦‚æœå‘ç°è¡¨çš„cluster_idæˆ–schemaä¸åŒ¹é…ï¼Œåˆ™æ˜¾ç¤ºè­¦å‘Šï¼Œè¦æ±‚ç®¡ç†å‘˜æ‰‹åŠ¨è¿è¡Œcliè¿ç§»å‘½ä»¤ã€‚

    ç»§æ‰¿æ­¤ç±»å®ç°å…·ä½“çš„ç»´æŠ¤é€»è¾‘ï¼Œæ­¤ç±»é™¤äº†check_table/create_tableï¼Œå…¶ä»–æ–¹æ³•ä»…åœ¨CLIç›¸å…³å‘½ä»¤æ—¶æ‰ä¼šå¯ç”¨ã€‚
    """

    _lock_key = "maintenance:lock"
    client: RedisBackendClient

    @staticmethod
    def meta_key(table_ref: TableReference) -> str:
        """è·å–redisè¡¨å…ƒæ•°æ®çš„keyå"""
        from .client import RedisBackendClient

        return f"{RedisBackendClient.table_prefix(table_ref)}:meta"

    @override
    def get_all_row_id(self, ref: TableReference) -> list[int]:
        # è·å–æ‰€æœ‰row id
        io = self.client.io
        keys = io.keys(
            self.client.cluster_prefix(ref) + ":id:*",
            target_nodes=RedisCluster.PRIMARIES,
        )
        # æ‰§è¡Œè¿ç§»è„šæœ¬å‡½æ•°
        keys = cast(list[bytes], keys)
        return [int(key.split(b":")[-1]) for key in keys]

    @override
    def delete_row(self, ref: TableReference, row_id: int):
        """åˆ é™¤æŒ‡å®šè¡¨çš„æŒ‡å®šè¡Œæ•°æ®"""
        key = self.client.row_key(ref, row_id)
        self.client.io.delete(key)

    @override
    def upsert_row(self, ref: TableReference, row_data: np.record):
        """æ›´æ–°æŒ‡å®šè¡¨çš„ä¸€è¡Œæ•°æ®ï¼Œå¦‚æœä¸å­˜åœ¨å°±æ’å…¥"""
        io = self.client.io
        key = self.client.row_key(ref, row_data.id)
        io.delete(key)
        mapping = ref.comp_cls.struct_to_dict(row_data)
        io.hset(key, mapping=mapping)

    @override
    def read_meta(
        self, instance_name: str, comp_cls: type[BaseComponent]
    ) -> TableMaintenance.TableMeta | None:
        """è¯»å–ç»„ä»¶è¡¨çš„metaä¿¡æ¯"""
        key = self.meta_key(
            TableReference(
                comp_cls=comp_cls,
                instance_name=instance_name,
                cluster_id=0,  # cluster_idä¸å½±å“metaè¯»å–
            )
        )

        io = self.client.io
        meta = cast(dict, io.hgetall(key))
        if not meta:
            return None
        return TableMaintenance.TableMeta(
            version=meta[b"version"].decode(),
            json=meta[b"json"].decode(),
            cluster_id=int(meta[b"cluster_id"]),
            extra={},
        )

    def __init__(self, master: RedisBackendClient):
        super().__init__(master)
        self.lock: redis.lock.Lock = self.client.io.lock(self._lock_key, timeout=60 * 5)

    @override
    def do_create_table_(self, table_ref: TableReference) -> TableMaintenance.TableMeta:
        """åˆ›å»ºç»„ä»¶è¡¨ã€‚å¦‚æœå·²å­˜åœ¨ï¼Œä¼šæŠ›å‡ºRaceConditionå¼‚å¸¸"""
        # åªéœ€è¦å†™å…¥metaï¼Œå…¶ä»–çš„_rebuild_indexä¼šåˆ›å»º
        meta = {
            "json": table_ref.comp_cls.json_,
            "version": hashlib.md5(
                table_ref.comp_cls.json_.encode("utf-8")
            ).hexdigest(),
            "cluster_id": table_ref.cluster_id,
        }
        assert not self.client.io.exists(self.meta_key(table_ref))
        self.client.io.hset(self.meta_key(table_ref), mapping=meta)
        meta_recon = self.read_meta(table_ref.instance_name, table_ref.comp_cls)
        assert meta_recon
        return meta_recon

    # æ— éœ€drop_table, æ­¤ç±»æ“ä½œé€‚åˆäººå·¥åˆ é™¤

    def do_rename_table_(self, from_: TableReference, to_: TableReference):
        """é‡å‘½åç»„ä»¶è¡¨"""
        # é‡å‘½åkey
        from_prefix = f"{self.client.cluster_prefix(from_)}:"
        from_prefix_len = len(from_prefix)
        to_prefix = f"{self.client.cluster_prefix(to_)}:"

        io = self.client.io
        from_keys = io.keys(
            from_prefix + ":*",
            target_nodes=RedisCluster.PRIMARIES,
        )
        from_keys = cast(list[bytes], from_keys)
        for from_key in from_keys:
            from_key = from_key.decode()
            to_key = to_prefix + from_key[from_prefix_len:]
            dump_data = cast(bytes, io.dump(from_key))
            ttl = cast(float, io.pttl(from_key))
            if ttl is None or ttl < 0:
                ttl = 0  # 0 ä»£è¡¨æ°¸ä¸è¿‡æœŸ
            io.restore(to_key, ttl, dump_data, replace=True)
            io.delete(from_key)  # cluster ä¸èƒ½è·¨èŠ‚ç‚¹renameï¼Œå¿…é¡»create+delete

        # æ›´æ–°metaï¼Œé‡å‘½åä¼šå¯¼è‡´json/versionå˜åŒ–çš„ï¼ˆé™¤éåªæ˜¯cluster idå˜æ›´ï¼‰ï¼‰ï¼Œæ‰€ä»¥éƒ½è¦å†™
        from_meta_key = self.meta_key(from_)
        to_meta_key = self.meta_key(to_)
        io.delete(from_meta_key)
        meta = {
            "json": to_.comp_cls.json_,
            "version": hashlib.md5(to_.comp_cls.json_.encode("utf-8")).hexdigest(),
            "cluster_id": to_.cluster_id,
        }
        self.client.io.hset(to_meta_key, mapping=meta)

        logger.warning(
            f"  âœ”ï¸ [ğŸ’¾Redis][{to_.comp_name}ç»„ä»¶] renameå®Œæˆï¼Œå…±æ”¹å{len(from_keys)}ä¸ªé”®å€¼ã€‚"
        )

    @override
    def do_drop_table_(self, table_ref: TableReference) -> int:
        """
        æ¸…ç©ºæ˜“å¤±æ€§ç»„ä»¶è¡¨æ•°æ®ï¼Œforceä¸ºTrueæ—¶å¼ºåˆ¶æ¸…ç©ºä»»æ„ç»„ä»¶è¡¨ã€‚
        æ³¨æ„ï¼šæ­¤æ“ä½œä¼šåˆ é™¤æ‰€æœ‰æ•°æ®ï¼
        """
        io = self.client.io
        # åˆ é™¤æ•°æ®
        del_keys = io.keys(
            self.client.table_prefix(table_ref) + ":*",
            target_nodes=RedisCluster.PRIMARIES,
        )
        del_keys = cast(list[bytes], del_keys)
        del_keys = [key.decode() for key in del_keys]
        for batch in batched(del_keys, 1000):
            with io.pipeline() as pipe:
                list(map(pipe.delete, batch))
                pipe.execute()
        # åˆ é™¤meta
        io.delete(self.meta_key(table_ref))
        return len(del_keys)

    @override
    def do_rebuild_index_(self, table_ref: TableReference) -> int:
        """é‡å»ºç»„ä»¶è¡¨çš„ç´¢å¼•æ•°æ®"""
        from .client import RedisBackendClient

        io = self.client.io
        keys = io.keys(
            self.client.cluster_prefix(table_ref) + ":id:*",
            target_nodes=RedisCluster.PRIMARIES,
        )
        keys = cast(list[bytes], keys)
        if len(keys) == 0:
            return 0

        for idx_name, _ in table_ref.comp_cls.indexes_.items():
            idx_key = self.client.index_key(table_ref, idx_name)
            # å…ˆåˆ é™¤æ‰€æœ‰_idx_keyå¼€å¤´çš„ç´¢å¼•
            io.delete(idx_key)
            # é‡å»ºæ‰€æœ‰ç´¢å¼•ï¼Œä¸ç®¡uniqueè¿˜æ˜¯indexéƒ½æ˜¯sset
            pipe = io.pipeline()
            b_row_ids: list[bytes] = []
            for key in keys:
                row_id = key.split(b":")[-1]
                b_row_ids.append(row_id)
                pipe.hget(key.decode(), idx_name)
            values: list[bytes] = pipe.execute()
            # æŠŠvaluesæŒ‰dtypeè½¬æ¢ä¸‹
            struct = table_ref.comp_cls.new_row()
            scalers: list[np.generic] = [np.str_()] * len(values)
            for i, v in enumerate(values):
                struct[idx_name] = v.decode()
                scalers[i] = struct[idx_name]

            # å»ºç«‹redisç´¢å¼•
            def get_member(_value: np.generic, _b_row_id) -> bytes:
                _sortable_value = RedisBackendClient.to_sortable_bytes(_value)
                return _sortable_value + b":" + _b_row_id

            io.zadd(
                idx_key,
                {
                    get_member(scaler, b_row_id): 0
                    for b_row_id, scaler in zip(b_row_ids, scalers)
                },
            )

            # æ£€æµ‹æ˜¯å¦æœ‰uniqueè¿å
            if idx_name in table_ref.comp_cls.uniques_:
                if len(values) != len(set(values)):
                    raise RuntimeError(
                        f"ç»„ä»¶{table_ref.comp_name}çš„uniqueç´¢å¼•`{idx_name}`åœ¨é‡å»ºæ—¶å‘ç°è¿åuniqueçº¦æŸï¼Œ"
                        f"å¯èƒ½æ˜¯è¿ç§»æ—¶ç¼©çŸ­äº†å€¼ç±»å‹ã€æˆ–æ–°å¢äº†Uniqueæ ‡è®°å¯¼è‡´ã€‚"
                    )
        return len(keys)
