"""
@author: Heerozh (Zhang Jianhao)
@copyright: Copyright 2024, Heerozh. All rights reserved.
@license: Apache2.0 å¯ç”¨ä½œå•†ä¸šé¡¹ç›®ï¼Œå†éšä¾¿æ‰¾ä¸ªè§’è½æåŠç”¨åˆ°äº†æ­¤é¡¹ç›® :D
@email: heeroz@gmail.com
"""

import hashlib
import logging
import warnings
from typing import TYPE_CHECKING

from ....common.helper import batched
from ...component import BaseComponent
from .. import (
    RaceCondition,
)
from ..base import CLITableMaintenance
from ..table import TableReference

if TYPE_CHECKING:
    from .client import RedisBackendClient

logger = logging.getLogger("HeTu.root")


class RedisCLITableMaintenance(CLITableMaintenance):
    _lock_key = "maintenance:lock"
    client: RedisBackendClient

    @staticmethod
    def meta_key(table_ref: TableReference) -> str:
        """è·å–redisè¡¨å…ƒæ•°æ®çš„keyå"""
        return f"{table_ref.instance_name}:{table_ref.comp_cls.component_name_}:meta"

    def __init__(self, client: RedisBackendClient):
        super().__init__(client)
        self.lock = self.client.io.lock(self._lock_key, timeout=60 * 5)

    async def lock(self):
        await self.lock.acquire()

    def check_table(self, table_ref: TableReference):
        """
        æ£€æŸ¥ç»„ä»¶è¡¨åœ¨æ•°æ®åº“ä¸­çš„çŠ¶æ€ã€‚
        æ­¤æ–¹æ³•æ£€æŸ¥å„ä¸ªç»„ä»¶è¡¨çš„metaé”®å€¼ã€‚

        Returns
        -------
        status: str
            "not_exists" - è¡¨ä¸å­˜åœ¨
            "ok" - è¡¨å­˜åœ¨ä¸”çŠ¶æ€æ­£å¸¸
            "cluster_mismatch" - è¡¨å­˜åœ¨ä½†cluster_idä¸åŒ¹é…
            "schema_mismatch" - è¡¨å­˜åœ¨ä½†schemaä¸åŒ¹é…
        """
        io = self.client.io

        # è·å–rediså·²å­˜çš„ç»„ä»¶ä¿¡æ¯
        key = self.meta_key(table_ref)
        meta = io.hgetall(key)
        if not meta:
            return "not_exists"
        else:
            version = hashlib.md5(table_ref.comp_cls.json_.encode("utf-8")).hexdigest()
            # å¦‚æœcluster_idæ”¹å˜ï¼Œåˆ™è¿ç§»æ”¹keyå
            if int(meta["cluster_id"]) != table_ref.cluster_id:
                return "cluster_mismatch"

            # å¦‚æœç‰ˆæœ¬ä¸ä¸€è‡´ï¼Œç»„ä»¶ç»“æ„å¯èƒ½æœ‰å˜åŒ–ï¼Œä¹Ÿå¯èƒ½åªæ˜¯æ”¹æƒé™ï¼Œæ€»ä¹‹è°ƒç”¨è¿ç§»ä»£ç 
            if meta["version"] != version:
                return "schema_mismatch"

        return "ok"

    def create_table(self, table_ref: TableReference) -> dict:
        """åˆ›å»ºç»„ä»¶è¡¨ã€‚å¦‚æœå·²å­˜åœ¨ï¼Œä¼šæŠ›å‡ºå¼‚å¸¸"""
        with self.lock:
            if self.check_table(table_ref) != "not_exists":
                raise RaceCondition(
                    f"[ğŸ’¾Redis][{table_ref.comp_name}ç»„ä»¶] ç»„ä»¶è¡¨å·²å­˜åœ¨ï¼Œæ— æ³•åˆ›å»ºã€‚"
                )
            logger.info(
                f"  â– [ğŸ’¾Redis][{table_ref.comp_name}ç»„ä»¶] ç»„ä»¶æ— metaä¿¡æ¯ï¼Œæ•°æ®ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»ºç©ºè¡¨..."
            )
            # åªéœ€è¦å†™å…¥metaï¼Œå…¶ä»–çš„_rebuild_indexä¼šåˆ›å»º
            meta = {
                "json": table_ref.comp_cls.json_,
                "version": hashlib.md5(
                    table_ref.comp_cls.json_.encode("utf-8")
                ).hexdigest(),
                "cluster_id": table_ref.cluster_id,
            }
            self._backend.io.hset(self.meta_key(table_ref), mapping=meta)
            logger.info(f"  âœ”ï¸ [ğŸ’¾Redis][{table_ref.comp_name}ç»„ä»¶] ç©ºè¡¨åˆ›å»ºå®Œæˆ")
            return meta

    # æ— éœ€drop_table, æ­¤ç±»æ“ä½œé€‚åˆäººå·¥åˆ é™¤

    def migration_cluster_id(
        self, table_ref: TableReference, old_cluster_id: int
    ) -> None:
        """è¿ç§»ç»„ä»¶è¡¨çš„cluster_id"""
        raise NotImplementedError

    def migration_schema(self, table_ref: TableReference, old_json: str) -> None:
        """è¿ç§»ç»„ä»¶è¡¨çš„schema"""
        raise NotImplementedError

    def flush(self, table_ref: TableReference, force=False) -> None:
        """
        æ¸…ç©ºæ˜“å¤±æ€§ç»„ä»¶è¡¨æ•°æ®ï¼Œforceä¸ºTrueæ—¶å¼ºåˆ¶æ¸…ç©ºä»»æ„ç»„ä»¶è¡¨ã€‚
        æ³¨æ„ï¼šæ­¤æ“ä½œä¼šåˆ é™¤æ‰€æœ‰æ•°æ®ï¼
        """
        raise NotImplementedError

    def rebuild_index(self, table_ref: TableReference) -> None:
        """é‡å»ºç»„ä»¶è¡¨çš„ç´¢å¼•æ•°æ®"""
        raise NotImplementedError

    def create_or_migrate(self, cluster_only=False):
        """
        æ£€æŸ¥è¡¨ç»“æ„æ˜¯å¦æ­£ç¡®ï¼Œä¸æ­£ç¡®åˆ™å°è¯•è¿›è¡Œè¿ç§»ã€‚æ­¤æ–¹æ³•åŒæ—¶ä¼šå¼ºåˆ¶é‡å»ºè¡¨çš„ç´¢å¼•ã€‚
        metaæ ¼å¼:
        json: ç»„ä»¶çš„ç»“æ„ä¿¡æ¯
        version: jsonçš„hash
        cluster_id: æ‰€å±ç°‡id

        Parameters
        ----------
        cluster_only : bool
            å¦‚æœä¸ºTrueï¼Œåˆ™åªå¤„ç†cluster_idçš„å˜æ›´ï¼Œå…¶ä»–ç»“æ„è¿ç§»å’Œé‡å»ºç´¢å¼•ç­‰ä¸å¤„ç†ã€‚
        """
        # todo è€ƒè™‘å–æ¶ˆhead_lockï¼Œé€šè¿‡è®°å½•ç‰ˆæœ¬å·æ¥å®ç°ï¼Œåªè¦ç‰ˆæœ¬å·ä¸ä¸€è‡´ï¼Œå°±åœæ­¢æœåŠ¡è¦æ±‚è¿ç§»
        #       åŒæ—¶æŠŠè¿ç§»ç§»åŠ¨åˆ°ä¸“é—¨çš„cliå‘½ä»¤ä¸­ï¼Œä¸æ˜¯è‡ªåŠ¨æ‰§è¡Œè€Œæ˜¯æ‰‹åŠ¨è®©ci/cdå‘å¸ƒæµç¨‹è°ƒç”¨
        if not self._backend.requires_head_lock():
            raise HeadLockFailed("redisä¸­head_locké”®")

        io = self._backend.io
        logger.info(f"âŒš [ğŸ’¾Redis][{self._name}ç»„ä»¶] å‡†å¤‡é”å®šæ£€æŸ¥metaä¿¡æ¯...")
        if cluster_only:
            logger.info(
                f"  â„¹ï¸ [ğŸ’¾Redis][{self._name}ç»„ä»¶] æ­¤è¡¨ä»…cluster idè¿ç§»æ¨¡å¼å¼€å¯ã€‚"
            )
        with io.lock(self._init_lock_key, timeout=60 * 5):
            # è·å–rediså·²å­˜çš„ç»„ä»¶ä¿¡æ¯
            meta = io.hgetall(self._meta_key)
            if not meta:
                self._create_emtpy()
            else:
                version = hashlib.md5(
                    self._component_cls.json_.encode("utf-8")
                ).hexdigest()
                # å¦‚æœcluster_idæ”¹å˜ï¼Œåˆ™è¿ç§»æ”¹keyå
                if int(meta["cluster_id"]) != self._cluster_id:
                    self._migration_cluster_id(old=int(meta["cluster_id"]))

                # å¦‚æœç‰ˆæœ¬ä¸ä¸€è‡´ï¼Œç»„ä»¶ç»“æ„å¯èƒ½æœ‰å˜åŒ–ï¼Œä¹Ÿå¯èƒ½åªæ˜¯æ”¹æƒé™ï¼Œæ€»ä¹‹è°ƒç”¨è¿ç§»ä»£ç 
                if meta["version"] != version and not cluster_only:
                    self._migration_schema(old=meta["json"])

            # é‡å»ºç´¢å¼•æ•°æ®
            if not cluster_only:
                self._rebuild_index()
            logger.info(f"âœ… [ğŸ’¾Redis][{self._name}ç»„ä»¶] æ£€æŸ¥å®Œæˆï¼Œè§£é”ç»„ä»¶")

    def flush(self, force=False):
        # todo è€ƒè™‘å–æ¶ˆhead_lockï¼Œå»ºè®®æ˜“å¤±æ•°æ®å…¨éƒ¨åŠ ä¸Šcomponentè¡Œçº§timeoutä¿¡æ¯ï¼Œè¿‡æœŸåç”±
        #       hetuè‡ªå·±å¯åŠ¨äº‹åŠ¡åˆ é™¤row,åŒ…æ‹¬Index.
        #       flushå‘½ä»¤åˆ™æ˜¯äº¤ç»™ci/cdæ¥æ‰§è¡Œï¼Œå› ä¸ºéœ€è¦é‡å¯æœåŠ¡å™¨å¿…ç„¶ç‰µæ¶‰åˆ°appç‰ˆæœ¬æå‡ï¼Œä¸ç„¶åœæœºå¹²å˜›ï¼Ÿ
        if not self._backend.requires_head_lock():
            raise HeadLockFailed("redisä¸­head_locké”®")

        if force:
            warnings.warn("flushæ­£åœ¨å¼ºåˆ¶åˆ é™¤æ‰€æœ‰æ•°æ®ï¼Œæ­¤æ–¹å¼åªå»ºè®®ç»´æŠ¤ä»£ç è°ƒç”¨ã€‚")

        # å¦‚æœéæŒä¹…åŒ–ç»„ä»¶ï¼Œåˆ™å…è®¸è°ƒç”¨flushä¸»åŠ¨æ¸…ç©ºæ•°æ®
        if not self._component_cls.persist_ or force:
            io = self._backend.io
            logger.info(
                f"âŒš [ğŸ’¾Redis][{self._name}ç»„ä»¶] å¯¹éæŒä¹…åŒ–ç»„ä»¶flushæ¸…ç©ºæ•°æ®ä¸­..."
            )

            with io.lock(self._init_lock_key, timeout=60 * 5):
                del_keys = io.keys(self._root_prefix + "*")
                del_keys.remove(self._init_lock_key)
                for batch in batched(del_keys, 1000):
                    with io.pipeline() as pipe:
                        list(map(pipe.delete, batch))
                        pipe.execute()
            logger.info(f"âœ… [ğŸ’¾Redis][{self._name}ç»„ä»¶] å·²åˆ é™¤{len(del_keys)}ä¸ªé”®å€¼")

            self.create_or_migrate()
        else:
            raise ValueError(f"{self._name}æ˜¯æŒä¹…åŒ–ç»„ä»¶ï¼Œä¸å…è®¸flushæ“ä½œ")

    def _create_emtpy(self):
        logger.info(
            f"  â– [ğŸ’¾Redis][{self._name}ç»„ä»¶] ç»„ä»¶æ— metaä¿¡æ¯ï¼Œæ•°æ®ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»ºç©ºè¡¨..."
        )

        # åªéœ€è¦å†™å…¥metaï¼Œå…¶ä»–çš„_rebuild_indexä¼šåˆ›å»º
        meta = {
            "json": self._component_cls.json_,
            "version": hashlib.md5(
                self._component_cls.json_.encode("utf-8")
            ).hexdigest(),
            "cluster_id": self._cluster_id,
        }
        self._backend.io.hset(self._meta_key, mapping=meta)
        logger.info(f"  âœ”ï¸ [ğŸ’¾Redis][{self._name}ç»„ä»¶] ç©ºè¡¨åˆ›å»ºå®Œæˆ")
        return meta

    def _rebuild_index(self):
        logger.info(f"  â– [ğŸ’¾Redis][{self._name}ç»„ä»¶] æ­£åœ¨é‡å»ºç´¢å¼•...")
        io = self._backend.io
        rows = io.keys(self._key_prefix + "*")
        if len(rows) == 0:
            logger.info(f"  âœ”ï¸ [ğŸ’¾Redis][{self._name}ç»„ä»¶] æ— æ•°æ®ï¼Œæ— éœ€é‡å»ºç´¢å¼•ã€‚")
            return

        for idx_name, str_type in self._component_cls.indexes_.items():
            idx_key = self._idx_prefix + idx_name
            # å…ˆåˆ é™¤æ‰€æœ‰_idx_keyå¼€å¤´çš„ç´¢å¼•
            io.delete(idx_key)
            # é‡å»ºæ‰€æœ‰ç´¢å¼•ï¼Œä¸ç®¡uniqueè¿˜æ˜¯indexéƒ½æ˜¯sset
            pipe = io.pipeline()
            row_ids = []
            for row in rows:
                row_id = row.split(":")[-1]
                row_ids.append(row_id)
                pipe.hget(row, idx_name)
            values = pipe.execute()
            # æŠŠvaluesæŒ‰dtypeè½¬æ¢ä¸‹
            struct = self._component_cls.new_row()
            for i, v in enumerate(values):
                struct[idx_name] = v
                values[i] = struct[idx_name].item()
            # å»ºç«‹redisç´¢å¼•
            if str_type:
                # å­—ç¬¦ä¸²ç±»å‹è¦ç‰¹æ®Šå¤„ç†ï¼Œscore=0, member='name:1'å½¢å¼
                io.zadd(
                    idx_key,
                    {f"{value}:{rid}": 0 for rid, value in zip(row_ids, values)},
                )
            else:
                # zadd ä¼šæ›¿æ¢æ‰memberç›¸åŒçš„å€¼ï¼Œç­‰äºæ˜¯set
                io.zadd(idx_key, dict(zip(row_ids, values)))
            # æ£€æµ‹æ˜¯å¦æœ‰uniqueè¿å
            if idx_name in self._component_cls.uniques_:
                if len(values) != len(set(values)):
                    raise RuntimeError(
                        f"ç»„ä»¶{self._name}çš„uniqueç´¢å¼•`{idx_name}`åœ¨é‡å»ºæ—¶å‘ç°è¿åuniqueçº¦æŸï¼Œ"
                        f"å¯èƒ½æ˜¯è¿ç§»æ—¶ç¼©çŸ­äº†å€¼ç±»å‹ã€æˆ–æ–°å¢äº†Uniqueæ ‡è®°å¯¼è‡´ã€‚"
                    )

        logger.info(
            f"  âœ”ï¸ [ğŸ’¾Redis][{self._name}ç»„ä»¶] ç´¢å¼•é‡å»ºå®Œæˆ, "
            f"{len(rows)}è¡Œ * {len(self._component_cls.indexes_)}ä¸ªç´¢å¼•ã€‚"
        )

    def _migration_cluster_id(self, old):
        logger.warning(
            f"  âš ï¸ [ğŸ’¾Redis][{self._name}ç»„ä»¶] "
            f"cluster_id ç”± {old} å˜æ›´ä¸º {self._cluster_id}ï¼Œ"
            f"å°†å°è¯•è¿ç§»clusteræ•°æ®..."
        )
        # é‡å‘½åkey
        old_hash_tag = f"{{CLU{old}}}:"
        new_hash_tag = f"{{CLU{self._cluster_id}}}:"
        old_prefix = f"{self._root_prefix}{old_hash_tag}"
        old_prefix_len = len(old_prefix)
        new_prefix = f"{self._root_prefix}{new_hash_tag}"

        io = self._backend.io
        old_keys = io.keys(old_prefix + "*")
        for old_key in old_keys:
            new_key = new_prefix + old_key[old_prefix_len:]
            io.rename(old_key, new_key)
        # æ›´æ–°meta
        io.hset(self._meta_key, "cluster_id", self._cluster_id)
        logger.warning(
            f"  âœ”ï¸ [ğŸ’¾Redis][{self._name}ç»„ä»¶] cluster è¿ç§»å®Œæˆï¼Œå…±è¿ç§»{len(old_keys)}ä¸ªé”®å€¼ã€‚"
        )

    def _migration_schema(self, old):
        """å¦‚æœæ•°æ®åº“ä¸­çš„å±æ€§å’Œå®šä¹‰ä¸ä¸€è‡´ï¼Œå°è¯•è¿›è¡Œç®€å•è¿ç§»ï¼Œå¯ä»¥å¤„ç†å±æ€§æ›´åä»¥å¤–çš„æƒ…å†µã€‚"""
        # åŠ è½½è€çš„ç»„ä»¶
        old_comp_cls = BaseComponent.load_json(old)

        # åªæœ‰propertiesåå­—å’Œç±»å‹å˜æ›´æ‰è¿ç§»
        dtypes_in_db = old_comp_cls.dtypes
        new_dtypes = self._component_cls.dtypes
        if dtypes_in_db == new_dtypes:
            return

        logger.warning(
            f"  âš ï¸ [ğŸ’¾Redis][{self._name}ç»„ä»¶] ä»£ç å®šä¹‰çš„Schemaä¸å·²å­˜çš„ä¸ä¸€è‡´ï¼Œ"
            f"æ•°æ®åº“ä¸­ï¼š\n"
            f"{dtypes_in_db}\n"
            f"ä»£ç å®šä¹‰çš„ï¼š\n"
            f"{new_dtypes}\n "
            f"å°†å°è¯•æ•°æ®è¿ç§»ï¼ˆåªå¤„ç†æ–°å±æ€§ï¼Œä¸å¤„ç†ç±»å‹å˜æ›´ï¼Œæ”¹åç­‰ç­‰æƒ…å†µï¼‰ï¼š"
        )

        # todo è°ƒç”¨è‡ªå®šä¹‰ç‰ˆæœ¬è¿ç§»ä»£ç ï¼ˆdefine_migrationï¼‰

        # æ£€æŸ¥æ˜¯å¦æœ‰å±æ€§è¢«åˆ é™¤
        for prop_name in dtypes_in_db.fields:
            if prop_name not in new_dtypes.fields:
                logger.warning(
                    f"  âš ï¸ [ğŸ’¾Redis][{self._name}ç»„ä»¶] "
                    f"æ•°æ®åº“ä¸­çš„å±æ€§ {prop_name} åœ¨æ–°çš„ç»„ä»¶å®šä¹‰ä¸­ä¸å­˜åœ¨ï¼Œå¦‚æœæ”¹åäº†éœ€è¦æ‰‹åŠ¨è¿ç§»ï¼Œ"
                    f"é»˜è®¤ä¸¢å¼ƒè¯¥å±æ€§æ•°æ®ã€‚"
                )

        # å¤šå‡ºæ¥çš„åˆ—å†æ¬¡æŠ¥è­¦å‘Šï¼Œç„¶åå¿½ç•¥
        io = self._backend.io
        rows = io.keys(self._key_prefix + "*")
        props = dict(self._component_cls.properties_)
        added = 0
        for prop_name in new_dtypes.fields:
            if prop_name not in dtypes_in_db.fields:
                logger.warning(
                    f"  âš ï¸ [ğŸ’¾Redis][{self._name}ç»„ä»¶] "
                    f"æ–°çš„ä»£ç å®šä¹‰ä¸­å¤šå‡ºå±æ€§ {prop_name}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼å¡«å……ã€‚"
                )
                default = props[prop_name].default
                if default is None:
                    logger.error(
                        f"  âš ï¸ [ğŸ’¾Redis][{self._name}ç»„ä»¶] "
                        f"è¿ç§»æ—¶å°è¯•æ–°å¢ {prop_name} å±æ€§å¤±è´¥ï¼Œè¯¥å±æ€§æ²¡æœ‰é»˜è®¤å€¼ï¼Œæ— æ³•æ–°å¢ã€‚"
                    )
                    raise ValueError("è¿ç§»å¤±è´¥")
                pipe = io.pipeline()
                for row in rows:
                    pipe.hset(row, prop_name, default)
                pipe.execute()
                added += 1

        # æ›´æ–°meta
        version = hashlib.md5(self._component_cls.json_.encode("utf-8")).hexdigest()
        io.hset(self._meta_key, "version", version)
        io.hset(self._meta_key, "json", self._component_cls.json_)

        logger.warning(
            f"  âœ”ï¸ [ğŸ’¾Redis][{self._name}ç»„ä»¶] æ–°å±æ€§å¢åŠ å®Œæˆï¼Œå…±å¤„ç†{len(rows)}è¡Œ * "
            f"{added}ä¸ªå±æ€§ã€‚"
        )
