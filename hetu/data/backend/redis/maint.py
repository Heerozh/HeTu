"""
@author: Heerozh (Zhang Jianhao)
@copyright: Copyright 2024, Heerozh. All rights reserved.
@license: Apache2.0 å¯ç”¨ä½œå•†ä¸šé¡¹ç›®ï¼Œå†éšä¾¿æ‰¾ä¸ªè§’è½æåŠç”¨åˆ°äº†æ­¤é¡¹ç›® :D
@email: heeroz@gmail.com
"""

import hashlib
import logging
import warnings
from typing import TYPE_CHECKING, cast, final, override, Any
import numpy as np

from ....common.helper import batched
from ...component import BaseComponent
from .. import RaceCondition
from ..base import TableMaintenance

from redis.cluster import RedisCluster

if TYPE_CHECKING:
    import redis
    import redis.lock
    from ..table import TableReference

    from .client import RedisBackendClient

logger = logging.getLogger("HeTu.root")


@final
class RedisTableMaintenance(TableMaintenance):
    """
    è¡¨ç»´æŠ¤ç±»ï¼ŒæœåŠ¡å™¨å¯åŠ¨æ—¶ä¼šè°ƒç”¨æ­¤ç±»æ£€æŸ¥ç»„ä»¶è¡¨çŠ¶æ€ï¼Œå¹¶åˆ›å»ºä¸å­˜åœ¨çš„è¡¨ã€‚
    å¦‚æœå‘ç°è¡¨çš„cluster_idæˆ–schemaä¸åŒ¹é…ï¼Œåˆ™æ˜¾ç¤ºè­¦å‘Šï¼Œè¦æ±‚ç®¡ç†å‘˜æ‰‹åŠ¨è¿è¡Œcliè¿ç§»å‘½ä»¤ã€‚

    ç»§æ‰¿æ­¤ç±»å®ç°å…·ä½“çš„ç»´æŠ¤é€»è¾‘ï¼Œæ­¤ç±»é™¤äº†check_table/create_tableï¼Œå…¶ä»–æ–¹æ³•ä»…åœ¨CLIç›¸å…³å‘½ä»¤æ—¶æ‰ä¼šå¯ç”¨ã€‚
    """

    _lock_key = "maintenance:lock"
    client: RedisBackendClient

    @staticmethod
    def meta_key(table_ref: TableReference) -> str:
        """è·å–redisè¡¨å…ƒæ•°æ®çš„keyå"""
        from .client import RedisBackendClient

        return f"{RedisBackendClient.table_prefix(table_ref)}:meta"

    def __init__(self, master: RedisBackendClient):
        super().__init__(master)
        self.lock: redis.lock.Lock = self.client.io.lock(self._lock_key, timeout=60 * 5)

    @override
    def check_table(self, table_ref: TableReference) -> tuple[str, Any]:
        """
        æ£€æŸ¥ç»„ä»¶è¡¨åœ¨æ•°æ®åº“ä¸­çš„çŠ¶æ€ã€‚
        æ­¤æ–¹æ³•æ£€æŸ¥å„ä¸ªç»„ä»¶è¡¨çš„metaé”®å€¼ã€‚

        Returns
        -------
        status: str
            "not_exists" - è¡¨ä¸å­˜åœ¨
            "ok" - è¡¨å­˜åœ¨ä¸”çŠ¶æ€æ­£å¸¸
            "cluster_mismatch" - è¡¨å­˜åœ¨ä½†cluster_idä¸åŒ¹é…
            "schema_mismatch" - è¡¨å­˜åœ¨ä½†schemaä¸åŒ¹é…
        meta: dict[bytes, Any]
            ç»„ä»¶è¡¨çš„metaä¿¡æ¯ï¼Œä¸€èˆ¬å«æœ‰ï¼š
                - b"version": ç»„ä»¶ç»“æ„çš„md5å€¼
                - b"json": ç»„ä»¶ç»“æ„çš„jsonå­—ç¬¦ä¸²
                - b"cluster_id": ç»„ä»¶æ‰€å±çš„cluster id
        """
        io = self.client.io

        # è·å–rediså·²å­˜çš„ç»„ä»¶ä¿¡æ¯
        key = self.meta_key(table_ref)
        meta = cast(dict, io.hgetall(key))
        if not meta:
            return "not_exists", None
        else:
            version = hashlib.md5(table_ref.comp_cls.json_.encode("utf-8")).hexdigest()
            # å¦‚æœcluster_idæ”¹å˜ï¼Œåˆ™è¿ç§»æ”¹keyåï¼Œå¿…é¡»å…ˆæ£€æŸ¥cluster_id
            if int(meta[b"cluster_id"]) != table_ref.cluster_id:
                return "cluster_mismatch", meta

            # å¦‚æœç‰ˆæœ¬ä¸ä¸€è‡´ï¼Œç»„ä»¶ç»“æ„å¯èƒ½æœ‰å˜åŒ–ï¼Œä¹Ÿå¯èƒ½åªæ˜¯æ”¹æƒé™ï¼Œæ€»ä¹‹è°ƒç”¨è¿ç§»ä»£ç 
            if meta[b"version"].decode() != version:
                return "schema_mismatch", meta

        return "ok", meta

    @override
    def create_table(self, table_ref: TableReference) -> Any:
        """åˆ›å»ºç»„ä»¶è¡¨ã€‚å¦‚æœå·²å­˜åœ¨ï¼Œä¼šæŠ›å‡ºå¼‚å¸¸"""
        with self.lock:
            if self.check_table(table_ref)[0] != "not_exists":
                raise RaceCondition(
                    f"[ğŸ’¾Redis][{table_ref.comp_name}ç»„ä»¶] ç»„ä»¶è¡¨å·²å­˜åœ¨ï¼Œæ— æ³•åˆ›å»ºã€‚"
                )
            logger.info(
                f"  â– [ğŸ’¾Redis][{table_ref.comp_name}ç»„ä»¶] ç»„ä»¶æ— metaä¿¡æ¯ï¼Œæ•°æ®ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»ºç©ºè¡¨..."
            )
            # åªéœ€è¦å†™å…¥metaï¼Œå…¶ä»–çš„_rebuild_indexä¼šåˆ›å»º
            meta = {
                "json": table_ref.comp_cls.json_,
                "version": hashlib.md5(
                    table_ref.comp_cls.json_.encode("utf-8")
                ).hexdigest(),
                "cluster_id": table_ref.cluster_id,
            }
            self.client.io.hset(self.meta_key(table_ref), mapping=meta)
            logger.info(f"  âœ”ï¸ [ğŸ’¾Redis][{table_ref.comp_name}ç»„ä»¶] ç©ºè¡¨åˆ›å»ºå®Œæˆ")
            return meta

    # æ— éœ€drop_table, æ­¤ç±»æ“ä½œé€‚åˆäººå·¥åˆ é™¤

    @override
    def migration_cluster_id(self, table_ref: TableReference, old_meta: Any) -> None:
        """è¿ç§»ç»„ä»¶è¡¨çš„cluster_id"""
        old_cluster_id = int(old_meta[b"cluster_id"])
        logger.warning(
            f"  âš ï¸ [ğŸ’¾Redis][{table_ref.comp_name}ç»„ä»¶] "
            f"cluster_id ç”± {old_cluster_id} å˜æ›´ä¸º {table_ref.cluster_id}ï¼Œ"
            f"å°†å°è¯•è¿ç§»clusteræ•°æ®..."
        )
        with self.lock:
            if self.check_table(table_ref)[0] != "cluster_mismatch":
                raise RaceCondition(
                    f"[ğŸ’¾Redis][{table_ref.comp_name}ç»„ä»¶] ç»„ä»¶è¡¨å·²è¿ç§»è¿‡ç°‡idã€‚"
                )
            # é‡å‘½åkey
            old_hash_tag = f"{{CLU{old_cluster_id}}}"
            new_hash_tag = f"{{CLU{table_ref.cluster_id}}}"
            old_prefix = f"{self.client.table_prefix(table_ref)}:{old_hash_tag}"
            old_prefix_len = len(old_prefix)
            new_prefix = f"{self.client.table_prefix(table_ref)}:{new_hash_tag}"

            io = self.client.io
            old_keys = io.keys(
                old_prefix + ":*",
                target_nodes=RedisCluster.PRIMARIES,
            )
            old_keys = cast(list[bytes], old_keys)
            for old_key in old_keys:
                old_key = old_key.decode()
                new_key = new_prefix + old_key[old_prefix_len:]
                dump_data = cast(bytes, io.dump(old_key))
                ttl = cast(float, io.pttl(old_key))
                if ttl is None or ttl < 0:
                    ttl = 0  # 0 ä»£è¡¨æ°¸ä¸è¿‡æœŸ
                io.restore(new_key, ttl, dump_data, replace=True)
                io.delete(old_key)  # cluster ä¸èƒ½è·¨èŠ‚ç‚¹renameï¼Œå¿…é¡»create+delete
            # æ›´æ–°meta
            io.hset(self.meta_key(table_ref), "cluster_id", str(table_ref.cluster_id))
            logger.warning(
                f"  âœ”ï¸ [ğŸ’¾Redis][{table_ref.comp_name}ç»„ä»¶] cluster è¿ç§»å®Œæˆï¼Œå…±è¿ç§»{len(old_keys)}ä¸ªé”®å€¼ã€‚"
            )

    @override
    def migration_schema(
        self, table_ref: TableReference, old_meta: Any, force=False
    ) -> bool:
        """
        è¿ç§»ç»„ä»¶è¡¨çš„schemaï¼Œæœ¬æ–¹æ³•å¿…é¡»åœ¨migration_cluster_idä¹‹åæ‰§è¡Œã€‚
        æ­¤æ–¹æ³•è°ƒç”¨åéœ€è¦rebuild_index

        æœ¬æ–¹æ³•å°†å…ˆå¯»æ‰¾æ˜¯å¦æœ‰è¿ç§»è„šæœ¬ï¼Œå¦‚æœæœ‰åˆ™è°ƒç”¨è„šæœ¬è¿›è¡Œè¿ç§»ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤è¿ç§»é€»è¾‘ã€‚

        é»˜è®¤è¿ç§»é€»è¾‘æ— æ³•å¤„ç†æ•°æ®è¢«åˆ é™¤çš„æƒ…å†µï¼Œä»¥åŠç±»å‹è½¬æ¢å¤±è´¥çš„æƒ…å†µï¼Œ
        forceå‚æ•°æŒ‡å®šæ˜¯å¦å¼ºåˆ¶è¿ç§»ï¼Œä¹Ÿå°±æ˜¯é‡åˆ°ä¸Šè¿°æƒ…å†µç›´æ¥ä¸¢å¼ƒæ•°æ®ã€‚
        """
        old_json = old_meta[b"json"].decode()
        old_version = old_meta[b"version"].decode()

        # todo é¦–å…ˆè°ƒç”¨æ‰‹åŠ¨è¿ç§»ï¼Œå®Œæˆåå†è°ƒç”¨è‡ªåŠ¨è¿ç§»
        # migration_script = self._load_migration_schema_script(table_ref, old_version)

        # åŠ è½½è€çš„ç»„ä»¶
        old_comp_cls = BaseComponent.load_json(old_json)

        # åªæœ‰propertiesåå­—å’Œç±»å‹å˜æ›´æ‰è¿ç§»
        dtypes_in_db = old_comp_cls.dtypes
        new_dtypes = table_ref.comp_cls.dtypes
        if dtypes_in_db == new_dtypes:
            return True

        logger.warning(
            f"  âš ï¸ [ğŸ’¾Redis][{table_ref.comp_name}ç»„ä»¶] ä»£ç å®šä¹‰çš„Schemaä¸å·²å­˜çš„ä¸ä¸€è‡´ï¼Œ"
            f"æ•°æ®åº“ä¸­ï¼š\n"
            f"{dtypes_in_db}\n"
            f"ä»£ç å®šä¹‰çš„ï¼š\n"
            f"{new_dtypes}\n "
            f"å°†å°è¯•æ•°æ®è¿ç§»ï¼ˆåªå¤„ç†æ–°å±æ€§ï¼Œä¸å¤„ç†ç±»å‹å˜æ›´ï¼Œæ”¹åç­‰ç­‰æƒ…å†µï¼‰ï¼š"
        )

        # æ£€æŸ¥æ˜¯å¦æœ‰å±æ€§è¢«åˆ é™¤
        assert dtypes_in_db.fields and new_dtypes.fields  # for type checker
        for prop_name in dtypes_in_db.fields:
            if prop_name not in new_dtypes.fields:
                msg = (
                    f"  âš ï¸ [ğŸ’¾Redis][{table_ref.comp_name}ç»„ä»¶] "
                    f"æ•°æ®åº“ä¸­çš„å±æ€§ {prop_name} åœ¨æ–°çš„ç»„ä»¶å®šä¹‰ä¸­ä¸å­˜åœ¨ï¼Œå¦‚æœæ”¹åäº†éœ€è¦æ‰‹åŠ¨è¿ç§»ï¼Œ"
                    f"å¼ºåˆ¶æ‰§è¡Œå°†ä¸¢å¼ƒè¯¥å±æ€§æ•°æ®ã€‚"
                )
                logger.warning(msg)
                if not force:
                    return False

        # æ£€æŸ¥æ˜¯å¦æœ‰å±æ€§ç±»å‹å˜æ›´ä¸”æ— æ³•è‡ªåŠ¨è½¬æ¢
        for prop_name in new_dtypes.fields:
            if prop_name in dtypes_in_db.fields:
                old_type = dtypes_in_db.fields[prop_name]
                new_type = new_dtypes.fields[prop_name]
                if not np.can_cast(old_type[0], new_type[0]):
                    msg = (
                        f"  âš ï¸ [ğŸ’¾Redis][{table_ref.comp_name}ç»„ä»¶] "
                        f"å±æ€§ {prop_name} çš„ç±»å‹ç”± {old_type} å˜æ›´ä¸º {new_type}ï¼Œ"
                        f"æ— æ³•è‡ªåŠ¨è½¬æ¢ç±»å‹ï¼Œéœ€è¦æ‰‹åŠ¨è¿ç§»ï¼Œå¼ºåˆ¶æ‰§è¡Œå°†æˆªæ–­/ä¸¢å¼ƒè¯¥å±æ€§æ•°æ®ã€‚"
                    )
                    logger.warning(msg)
                    if not force:
                        return False

        with self.lock:
            if self.check_table(table_ref)[0] != "schema_mismatch":
                raise RaceCondition(
                    f"[ğŸ’¾Redis][{table_ref.comp_name}ç»„ä»¶] ç»„ä»¶è¡¨å·²è¿ç§»è¿‡schemaã€‚"
                )

            # å¤šå‡ºæ¥çš„åˆ—å†æ¬¡æŠ¥è­¦å‘Šï¼Œç„¶åå¿½ç•¥
            io = self.client.io
            keys = io.keys(
                self.client.cluster_prefix(table_ref) + ":id:*",
                target_nodes=RedisCluster.PRIMARIES,
            )
            keys = cast(list[bytes], keys)
            props = dict(table_ref.comp_cls.properties_)
            added = 0
            converted = 0
            convert_failed = 0
            for prop_name in new_dtypes.fields:
                if prop_name not in dtypes_in_db.fields:
                    logger.warning(
                        f"  âš ï¸ [ğŸ’¾Redis][{table_ref.comp_name}ç»„ä»¶] "
                        f"æ–°çš„ä»£ç å®šä¹‰ä¸­å¤šå‡ºå±æ€§ {prop_name}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼å¡«å……ã€‚"
                    )
                    default = props[prop_name].default
                    if default is None:
                        logger.error(
                            f"  âš ï¸ [ğŸ’¾Redis][{table_ref.comp_name}ç»„ä»¶] "
                            f"è¿ç§»æ—¶å°è¯•æ–°å¢ {prop_name} å±æ€§å¤±è´¥ï¼Œè¯¥å±æ€§æ²¡æœ‰é»˜è®¤å€¼ï¼Œæ— æ³•æ–°å¢ã€‚"
                        )
                        raise ValueError("è¿ç§»å¤±è´¥")
                    pipe = io.pipeline()
                    for key in keys:
                        pipe.hset(key.decode(), prop_name, default)
                    pipe.execute()
                    added += 1
                elif force:  # ç±»å‹è½¬æ¢
                    old_type = dtypes_in_db.fields[prop_name][0]
                    new_type = new_dtypes.fields[prop_name][0]
                    if old_type == new_type:
                        continue
                    default = props[prop_name].default
                    pipe = io.pipeline()
                    for key in keys:
                        val = io.hget(key.decode(), prop_name)
                        if val is None:
                            continue
                        try:
                            val = cast(bytes, cast(object, val))
                            casted_val = new_type.type(old_type.type(val.decode()))

                            if np.issubdtype(new_type, np.character):
                                # å­—ç¬¦ä¸²ç±»å‹éœ€è¦ç‰¹æ®Šæˆªæ–­å¤„ç†ï¼Œä¸ç„¶npä¼šè‡ªåŠ¨å»¶é•¿
                                def fixed_str_len(dt: np.dtype) -> int:
                                    dt = np.dtype(dt)
                                    if dt.kind == "U":
                                        return dt.itemsize // 4
                                    if dt.kind == "S":
                                        return dt.itemsize
                                    raise TypeError(
                                        f"not a fixed-length string dtype: {dt!r}"
                                    )

                                casted_val = casted_val[: fixed_str_len(new_type)]

                            pipe.hset(key.decode(), prop_name, str(casted_val))
                            converted += 1
                        except ValueError as _:
                            # å¼ºåˆ¶æ¨¡å¼ä¸‹ä¸¢å¼ƒè¯¥å±æ€§
                            pipe.hset(key.decode(), prop_name, default)
                            convert_failed += 1
                    pipe.execute()

            # æ›´æ–°meta
            version = hashlib.md5(table_ref.comp_cls.json_.encode("utf-8")).hexdigest()
            io.hset(self.meta_key(table_ref), "version", version)
            io.hset(self.meta_key(table_ref), "json", table_ref.comp_cls.json_)

            logger.warning(
                f"  âœ”ï¸ [ğŸ’¾Redis][{table_ref.comp_name}ç»„ä»¶] æ–°å±æ€§å¢åŠ å®Œæˆï¼Œå…±å¤„ç†{len(keys)}è¡Œ * "
                f"{added}ä¸ªå±æ€§ã€‚ è½¬æ¢ç±»å‹æˆåŠŸ{converted}æ¬¡ï¼Œå¤±è´¥{convert_failed}æ¬¡ã€‚"
            )
            return True

    @override
    def flush(self, table_ref: TableReference, force=False) -> None:
        """
        æ¸…ç©ºæ˜“å¤±æ€§ç»„ä»¶è¡¨æ•°æ®ï¼Œforceä¸ºTrueæ—¶å¼ºåˆ¶æ¸…ç©ºä»»æ„ç»„ä»¶è¡¨ã€‚
        æ³¨æ„ï¼šæ­¤æ“ä½œä¼šåˆ é™¤æ‰€æœ‰æ•°æ®ï¼
        """
        if force:
            warnings.warn("flushæ­£åœ¨å¼ºåˆ¶åˆ é™¤æ‰€æœ‰æ•°æ®ï¼Œæ­¤æ–¹å¼åªå»ºè®®ç»´æŠ¤ä»£ç è°ƒç”¨ã€‚")

        # å¦‚æœéæŒä¹…åŒ–ç»„ä»¶ï¼Œåˆ™å…è®¸è°ƒç”¨flushä¸»åŠ¨æ¸…ç©ºæ•°æ®
        if table_ref.comp_cls.volatile_ or force:
            io = self.client.io
            logger.info(
                f"âŒš [ğŸ’¾Redis][{table_ref.comp_name}ç»„ä»¶] å¯¹éæŒä¹…åŒ–ç»„ä»¶flushæ¸…ç©ºæ•°æ®ä¸­..."
            )

            with self.lock:
                del_keys = io.keys(
                    self.client.table_prefix(table_ref) + ":*",
                    target_nodes=RedisCluster.PRIMARIES,
                )
                del_keys = cast(list[bytes], del_keys)
                del_keys = [key.decode() for key in del_keys]
                for batch in batched(del_keys, 1000):
                    with io.pipeline() as pipe:
                        list(map(pipe.delete, batch))
                        pipe.execute()

            logger.info(
                f"âœ… [ğŸ’¾Redis][{table_ref.comp_name}ç»„ä»¶] å·²åˆ é™¤{len(del_keys)}ä¸ªé”®å€¼"
            )
            self.create_table(table_ref)
        else:
            raise ValueError(f"{table_ref.comp_name}æ˜¯æŒä¹…åŒ–ç»„ä»¶ï¼Œä¸å…è®¸flushæ“ä½œ")

    @override
    def rebuild_index(self, table_ref: TableReference) -> None:
        """é‡å»ºç»„ä»¶è¡¨çš„ç´¢å¼•æ•°æ®"""
        from .client import RedisBackendClient

        logger.info(f"  â– [ğŸ’¾Redis][{table_ref.comp_name}ç»„ä»¶] æ­£åœ¨é‡å»ºç´¢å¼•...")
        with self.lock:
            io = self.client.io
            keys = io.keys(
                self.client.cluster_prefix(table_ref) + ":id:*",
                target_nodes=RedisCluster.PRIMARIES,
            )
            keys = cast(list[bytes], keys)
            if len(keys) == 0:
                logger.info(
                    f"  âœ”ï¸ [ğŸ’¾Redis][{table_ref.comp_name}ç»„ä»¶] æ— æ•°æ®ï¼Œæ— éœ€é‡å»ºç´¢å¼•ã€‚"
                )
                return

            for idx_name, _ in table_ref.comp_cls.indexes_.items():
                idx_key = self.client.index_key(table_ref, idx_name)
                # å…ˆåˆ é™¤æ‰€æœ‰_idx_keyå¼€å¤´çš„ç´¢å¼•
                io.delete(idx_key)
                # é‡å»ºæ‰€æœ‰ç´¢å¼•ï¼Œä¸ç®¡uniqueè¿˜æ˜¯indexéƒ½æ˜¯sset
                pipe = io.pipeline()
                b_row_ids: list[bytes] = []
                for key in keys:
                    row_id = key.split(b":")[-1]
                    b_row_ids.append(row_id)
                    pipe.hget(key.decode(), idx_name)
                values: list[bytes] = pipe.execute()
                # æŠŠvaluesæŒ‰dtypeè½¬æ¢ä¸‹
                struct = table_ref.comp_cls.new_row()
                scalers: list[np.generic] = [np.str_()] * len(values)
                for i, v in enumerate(values):
                    struct[idx_name] = v.decode()
                    scalers[i] = struct[idx_name]

                # å»ºç«‹redisç´¢å¼•
                def get_member(_value: np.generic, _b_row_id) -> bytes:
                    _sortable_value = RedisBackendClient.to_sortable_bytes(_value)
                    return _sortable_value + b":" + _b_row_id

                io.zadd(
                    idx_key,
                    {
                        get_member(scaler, b_row_id): 0
                        for b_row_id, scaler in zip(b_row_ids, scalers)
                    },
                )

                # æ£€æµ‹æ˜¯å¦æœ‰uniqueè¿å
                if idx_name in table_ref.comp_cls.uniques_:
                    if len(values) != len(set(values)):
                        raise RuntimeError(
                            f"ç»„ä»¶{table_ref.comp_name}çš„uniqueç´¢å¼•`{idx_name}`åœ¨é‡å»ºæ—¶å‘ç°è¿åuniqueçº¦æŸï¼Œ"
                            f"å¯èƒ½æ˜¯è¿ç§»æ—¶ç¼©çŸ­äº†å€¼ç±»å‹ã€æˆ–æ–°å¢äº†Uniqueæ ‡è®°å¯¼è‡´ã€‚"
                        )

            logger.info(
                f"  âœ”ï¸ [ğŸ’¾Redis][{table_ref.comp_name}ç»„ä»¶] ç´¢å¼•é‡å»ºå®Œæˆ, "
                f"{len(keys)}è¡Œ * {len(table_ref.comp_cls.indexes_)}ä¸ªç´¢å¼•ã€‚"
            )
