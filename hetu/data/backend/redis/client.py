"""
@author: Heerozh (Zhang Jianhao)
@copyright: Copyright 2024, Heerozh. All rights reserved.
@license: Apache2.0 å¯ç”¨ä½œå•†ä¸šé¡¹ç›®ï¼Œå†éšä¾¿æ‰¾ä¸ªè§’è½æåŠç”¨åˆ°äº†æ­¤é¡¹ç›® :D
@email: heeroz@gmail.com
"""

import asyncio
import itertools
import logging
import random
import struct
from pathlib import Path
from typing import TYPE_CHECKING, Any, Literal, cast, final, overload, override

# from msgspec import msgpack  # ä¸æ”¯æŒå…³é—­bin typeï¼Œlua çš„msgpackåº“7å¹´æ²¡æ›´æ–°äº†
import msgpack
import numpy as np
import redis

from ..base import BackendClient, RaceCondition, RowFormat

if TYPE_CHECKING:
    import redis.asyncio
    import redis.asyncio.cluster
    import redis.cluster
    import redis.exceptions

    from ...component import BaseComponent
    from ..idmap import IdentityMap
    from ..table import TableReference
    from .maint import RedisTableMaintenance
    from .mq import RedisMQClient
    from .worker_keeper import RedisWorkerKeeper

logger = logging.getLogger("HeTu.root")


@final
class RedisBackendClient(BackendClient, alias="redis"):
    """å’ŒRedisåç«¯çš„æ“ä½œçš„ç±»ï¼ŒæœåŠ¡å™¨å¯åŠ¨æ—¶ç”±server.pyæ ¹æ®Configåˆå§‹åŒ–"""

    @staticmethod
    def _get_referred_components() -> list[type[BaseComponent]]:
        """è·å–å½“å‰appç”¨åˆ°çš„Componentåˆ—è¡¨"""
        from ....system.definer import SystemClusters

        return [comp_cls for comp_cls in SystemClusters().get_components().keys()]

    def _schema_checking_for_redis(self):
        """æ£€æŸ¥Componentçš„schemaå®šä¹‰ï¼Œç¡®ä¿ç¬¦åˆRedisçš„è¦æ±‚"""
        for comp_cls in self._get_referred_components():
            for field, _ in comp_cls.indexes_.items():
                dtype = comp_cls.dtype_map_[field]
                # ç´¢å¼•ä¸æ”¯æŒå¤æ•°
                if np.issubdtype(dtype, np.complexfloating):
                    raise ValueError(
                        f"Component `{comp_cls.component_name_}` çš„ç´¢å¼•å­—æ®µ`{field}`"
                        "ä½¿ç”¨äº†å¤æ•°ï¼ŒRedisåç«¯ä¸æ”¯æŒæ­¤ç±»å‹ä½œä¸ºç´¢å¼•å­—æ®µ"
                    )
                # å…¶ä»–ç±»å‹ä¸æ”¯æŒç´¢å¼•
                elif np.issubdtype(dtype, np.object_):
                    raise ValueError(
                        f"Component `{comp_cls.component_name_}` çš„ç´¢å¼•å­—æ®µ`{field}`"
                        f"ä½¿ç”¨äº†ä¸å¯ç”¨çš„ç±»å‹ `{dtype}`ï¼Œæ­¤ç±»å‹ä¸æ”¯æŒç´¢å¼•"
                    )

    def load_commit_scripts(self, file: str | Path):
        assert self._async_ios, "è¿æ¥å·²å…³é—­ï¼Œå·²è°ƒç”¨è¿‡close"
        assert self.is_servant is False, (
            "Servantä¸å…è®¸åŠ è½½Luaäº‹åŠ¡è„šæœ¬ï¼ŒLuaäº‹åŠ¡è„šæœ¬åªèƒ½åœ¨Masterä¸ŠåŠ è½½"
        )
        assert len(self._async_ios) == 1, (
            "Luaäº‹åŠ¡è„šæœ¬åªèƒ½åœ¨Masterä¸ŠåŠ è½½ï¼Œä½†å½“å‰è¿æ¥æ± ä¸­æœ‰å¤šä¸ªæœåŠ¡å™¨"
        )
        # read file to text
        with open(file, "r", encoding="utf-8") as f:
            script_text = f.read()

        # ä¸Šä¼ è„šæœ¬åˆ°æœåŠ¡å™¨ä½¿ç”¨åŒæ­¥io
        self._ios[0].script_load(script_text)
        # æ³¨å†Œè„šæœ¬åˆ°å¼‚æ­¥ioï¼Œå› ä¸ºmasteråªèƒ½æœ‰ä¸€ä¸ªè¿æ¥ï¼Œç›´æ¥[0]å°±è¡Œäº†
        return self._async_ios[0].register_script(script_text)  # type: ignore

    @property
    def io(self) -> redis.Redis | redis.cluster.RedisCluster:
        """éšæœºè¿”å›ä¸€ä¸ªåŒæ­¥è¿æ¥"""
        return random.choice(self._ios)

    @property
    def aio(self):
        """éšæœºè¿”å›ä¸€ä¸ªå¼‚æ­¥è¿æ¥"""
        if self.loop_id == 0:
            self.loop_id = hash(asyncio.get_running_loop())
        # redis-pyçš„async connectionç”¨çš„pythonçš„steam.connectï¼Œç»‘å®šåˆ°å½“å‰åç¨‹
        # è€Œaioæ˜¯ä¸€ä¸ªconnection poolï¼Œæ–­å¼€çš„è¿æ¥ä¼šæ”¾å›poolä¸­ï¼Œæ‰€ä»¥aioä¸èƒ½è·¨åç¨‹ä¼ é€’
        assert hash(asyncio.get_running_loop()) == self.loop_id, (
            "Backendåªèƒ½åœ¨åŒä¸€ä¸ªcoroutineä¸­ä½¿ç”¨ã€‚æ£€æµ‹åˆ°è°ƒç”¨æ­¤å‡½æ•°çš„åç¨‹å‘ç”Ÿäº†å˜åŒ–"
        )

        return random.choice(self._async_ios)

    @staticmethod
    def table_prefix(table_ref: TableReference) -> str:
        """è·å–redisè¡¨åå‰ç¼€"""
        return f"{table_ref.instance_name}:{table_ref.comp_cls.component_name_}"

    @staticmethod
    def cluster_prefix(table_ref: TableReference) -> str:
        """è·å–redisè¡¨åå‰ç¼€"""
        return (
            f"{table_ref.instance_name}:{table_ref.comp_cls.component_name_}:"
            f"{{CLU{table_ref.cluster_id}}}"
        )

    @classmethod
    def row_key(cls, table_ref: TableReference, row_id: str | int) -> str:
        """è·å–redisè¡¨è¡Œçš„keyå"""
        return f"{cls.cluster_prefix(table_ref)}:id:{str(row_id)}"

    @classmethod
    def index_key(cls, table_ref: TableReference, index_name: str) -> str:
        """è·å–redisè¡¨ç´¢å¼•çš„keyå"""
        return f"{cls.cluster_prefix(table_ref)}:index:{index_name}"

    @override
    def index_channel(self, table_ref: TableReference, index_name: str):
        """è¿”å›ç´¢å¼•çš„é¢‘é“åã€‚å¦‚æœç´¢å¼•æœ‰æ•°æ®å˜åŠ¨ï¼Œä¼šé€šçŸ¥åˆ°è¯¥é¢‘é“"""
        return f"__keyspace@{self.dbi}__:{self.index_key(table_ref, index_name)}"

    @override
    def row_channel(self, table_ref: TableReference, row_id: int):
        """è¿”å›è¡Œæ•°æ®çš„é¢‘é“åã€‚å¦‚æœè¡Œæœ‰å˜åŠ¨ï¼Œä¼šé€šçŸ¥åˆ°è¯¥é¢‘é“"""
        return f"__keyspace@{self.dbi}__:{self.row_key(table_ref, row_id)}"

    async def reset_async_connection_pool(self):
        """é‡ç½®å¼‚æ­¥è¿æ¥æ± ï¼Œç”¨äºåç¨‹åˆ‡æ¢åï¼Œè§£å†³aioä¸èƒ½è·¨åç¨‹ä¼ é€’çš„é—®é¢˜"""
        self.loop_id = 0
        for aio in self._async_ios:
            if isinstance(aio, redis.asyncio.cluster.RedisCluster):
                await aio.aclose()  # æœªæµ‹è¯•
            else:
                aio.connection_pool.reset()

    @staticmethod
    def to_sortable_bytes(value: np.generic) -> bytes:
        """å°†npç±»å‹çš„å€¼è½¬æ¢ä¸ºå¯æ’åºçš„bytesï¼Œç”¨äºç´¢å¼•"""
        dtype = value.dtype
        if np.issubdtype(dtype, np.signedinteger):
            data = value.item() + (1 << 63)
            return struct.pack(">Q", data)
        elif np.issubdtype(dtype, np.unsignedinteger):
            return struct.pack(">Q", value)
        elif np.issubdtype(dtype, np.floating):
            double = value.item()
            packed = struct.pack(">d", value)
            [u64] = struct.unpack(">Q", packed)
            # IEEE 754 æµ®ç‚¹æ•°æ’åºè°ƒæ•´
            if double >= 0:
                # æ­£æ•°è®©ç¬¦å·ä½å˜1
                u64 = u64 | (1 << 63)
            else:
                # è´Ÿæ•°è¦å…¨éƒ¨å–åï¼Œå› ä¸ºæµ®ç‚¹è´Ÿæ•°æ˜¯ç»å¯¹å€¼ï¼Œå˜æˆinté‚£ç§ä»0xFFé€’å‡
                u64 = ~u64 & 0xFFFFFFFFFFFFFFFF
            return struct.pack(">Q", u64)
        elif np.issubdtype(dtype, np.str_):
            encoded = value.item().encode("utf-8")
            return encoded
        elif np.issubdtype(dtype, np.bytes_):
            return value.item()
        elif np.issubdtype(dtype, np.bool_):
            return b"\x01" if value else b"\x00"
        assert False, f"ä¸å¯æ’åºçš„ç´¢å¼•ç±»å‹: {dtype}"

    # ============ ä¸»è¦æ–¹æ³• ============

    def __init__(self, endpoint: str | list[str], clustering: bool, is_servant=False):
        super().__init__(endpoint, clustering, is_servant)
        # redisçš„endpointé…ç½®ä¸ºurl, æˆ–list of url
        self.urls = [endpoint] if type(endpoint) is str else endpoint
        assert len(self.urls) > 0, "å¿…é¡»è‡³å°‘æŒ‡å®šä¸€ä¸ªæ•°æ®åº“è¿æ¥URL"

        # åˆ›å»ºè¿æ¥
        self._ios: list[redis.Redis | redis.cluster.RedisCluster] = []
        self._async_ios: list[
            redis.asyncio.Redis | redis.asyncio.cluster.RedisCluster
        ] = []
        for url in self.urls:
            if self.clustering:
                self._ios.append(redis.cluster.RedisCluster.from_url(url))
                self._async_ios.append(redis.asyncio.cluster.RedisCluster.from_url(url))
            else:
                self._ios.append(redis.Redis.from_url(url))
                self._async_ios.append(redis.asyncio.Redis.from_url(url))

        # æµ‹è¯•è¿æ¥æ˜¯å¦æ­£å¸¸
        for i, io in enumerate(self._ios):
            try:
                io.ping()
            except redis.exceptions.ConnectionError as e:
                raise ConnectionError(f"æ— æ³•è¿æ¥åˆ°Redisæ•°æ®åº“ï¼š{self.urls[i]}") from e

        # è·å¾—db index
        if self.clustering:
            self.dbi = 0  # é›†ç¾¤æ¨¡å¼æ²¡æœ‰dbçš„æ¦‚å¿µï¼Œé»˜è®¤0
        else:
            io = self._ios[0]
            assert isinstance(io, redis.Redis)  # for type checking
            self.dbi = io.connection_pool.connection_kwargs["db"]

        self.lua_commit = None

        # é™åˆ¶aioè¿è¡Œçš„coroutine
        try:
            self.loop_id = hash(asyncio.get_running_loop())
        except RuntimeError:
            self.loop_id = 0

    @override
    def post_configure(self) -> None:
        """
        å¯¹æ•°æ®åº“åšçš„é…ç½®å·¥ä½œæ”¾åœ¨è¿™ï¼Œå¯ä»¥åšäº›å‡å°‘è¿ç»´å‹åŠ›çš„å·¥ä½œï¼Œæˆ–æ˜¯éœ€è¦é¡¹ç›®åŠ è½½å®Œæˆåæ‰èƒ½åšçš„åˆå§‹åŒ–å·¥ä½œã€‚
        æ­¤é¡¹åœ¨æœåŠ¡å™¨å®Œå…¨åŠ è½½å®Œæ¯•åæ‰ä¼šæ‰§è¡Œï¼Œåœ¨æµ‹è¯•ç¯å¢ƒä¸­ï¼Œä¹Ÿæ˜¯æœ€åè°ƒç”¨ã€‚
        """
        if self.is_servant:
            self.configure_servant()
        else:
            self.configure_master()

    def configure_master(self) -> None:
        if not self._ios:
            raise ConnectionError("è¿æ¥å·²å…³é—­ï¼Œå·²è°ƒç”¨è¿‡close")

        # æ£€æµ‹redisç‰ˆæœ¬
        def parse_version(x):
            return tuple(map(int, x.split(".")))

        for i, io in enumerate(self._ios):
            info: dict = cast(dict, io.info("server"))  # é˜²æ­¢Awaitableç±»å‹æ£€æŸ¥æŠ¥é”™
            redis_ver = parse_version(info["redis_version"])
            assert redis_ver >= (7, 0), "Redis/Valkey ç‰ˆæœ¬è¿‡ä½ï¼Œè‡³å°‘éœ€è¦7.0ç‰ˆæœ¬"

        # åŠ è½½luaè„šæœ¬ï¼Œæ³¨æ„redis-pyçš„pipelineé‡Œä¸èƒ½ç”¨luaï¼Œä¼šåå¤æ£€æµ‹script existsæ€§èƒ½æä½
        self.lua_commit = self.load_commit_scripts(
            Path(__file__).parent.resolve() / "commit_v2.lua"
        )
        # æç¤ºç”¨æˆ·schemaå®šä¹‰æ˜¯å¦ç¬¦åˆredisè¦æ±‚ï¼Œæ¯”å¦‚ç´¢å¼•ç±»å‹ä¸èƒ½æœ‰å¤æ•°ç­‰
        self._schema_checking_for_redis()

    def configure_servant(self) -> None:
        if not self._ios:
            raise ConnectionError("è¿æ¥å·²å…³é—­ï¼Œå·²è°ƒç”¨è¿‡close")
            # æ£€æŸ¥servantsè®¾ç½®

        target_keyspace = "Kghz"
        for i, io in enumerate(self._ios):
            try:
                # è®¾ç½®keyspaceé€šçŸ¥ï¼Œå…ˆcasté˜²æ­¢Awaitableç±»å‹æ£€æŸ¥æŠ¥é”™
                notify_config = cast(dict, io.config_get("notify-keyspace-events"))
                db_keyspace = notify_config["notify-keyspace-events"]
                db_keyspace = db_keyspace.replace("A", "g$lshztxed")
                db_keyspace_new = db_keyspace
                for flag in list(target_keyspace):
                    if flag not in db_keyspace:
                        db_keyspace_new += flag
                if db_keyspace_new != db_keyspace:
                    io.config_set("notify-keyspace-events", db_keyspace_new)
            except (
                redis.exceptions.NoPermissionError,
                redis.exceptions.ResponseError,
            ):
                msg = (
                    f"âš ï¸ [ğŸ’¾Redis] æ— æƒé™è°ƒç”¨æ•°æ®åº“{self.urls[i]}çš„config_setå‘½ä»¤ï¼Œæ•°æ®è®¢é˜…å°†"
                    f"ä¸èµ·æ•ˆã€‚å¯æ‰‹åŠ¨è®¾ç½®é…ç½®æ–‡ä»¶ï¼šnotify-keyspace-events={target_keyspace}"
                )
                logger.warning(msg)
            # æ£€æŸ¥æ˜¯å¦æ˜¯replicaæ¨¡å¼
            db_replica = cast(dict, io.config_get("replica-read-only"))
            if db_replica.get("replica-read-only") != "yes":
                msg = (
                    "âš ï¸ [ğŸ’¾Redis] servantå¿…é¡»æ˜¯Read Only Replicaæ¨¡å¼ã€‚"
                    f"{self.urls[i]} æœªè®¾ç½®replica-read-only=yes"
                )
                logger.warning(msg)
                # ä¸æ£€æŸ¥replicaof masteråœ°å€ï¼Œå› ä¸ºreplicaofçš„å¯èƒ½æ˜¯å…¶ä»–replicaåœ°å€
            # è€ƒè™‘å¯ä»¥æ£€æŸ¥pubsub client buffè®¾ç½®ï¼Œçœ‹çœ‹èƒ½å¦rediså´©äº†æé†’ä¸‹
            # pubsubå€¼å»ºè®®ä¸º$å‰©ä½™å†…å­˜/é¢„ä¼°åœ¨çº¿æ•°$

    @override
    async def is_synced(self) -> bool:
        if not self._ios:
            raise ConnectionError("è¿æ¥å·²å…³é—­ï¼Œå·²è°ƒç”¨è¿‡close")

        assert not self.is_servant, "is_syncedåªèƒ½åœ¨masterä¸Šè°ƒç”¨"

        info = await self.aio.info("replication")
        master_offset = int(info.get("master_repl_offset", 0))
        for key, value in info.items():
            # å…¼å®¹ Redis æ–°æ—§ç‰ˆæœ¬ï¼ˆslave/replica å­—æ®µï¼‰
            if key.startswith("slave") or key.startswith("replica"):
                if type(value) is not dict:  # å¯èƒ½æ˜¯ replicas_waiting_psync:0
                    continue
                lag_of_offset = master_offset - int(value.get("offset", 0))
                if lag_of_offset > 0:
                    return False
        return True

    @override
    def get_worker_keeper(self, sequence_id: int) -> RedisWorkerKeeper:
        """
        è·å–RedisWorkerKeeperå®ä¾‹ï¼Œç”¨äºé›ªèŠ±IDçš„worker idç®¡ç†ã€‚

        Parameters
        ----------
        sequence_id: int
            å¯åŠ¨è¿›ç¨‹çš„é¡ºåºIDï¼Œä»0å¼€å§‹ã€‚
        """
        if not self._ios:
            raise ConnectionError("è¿æ¥å·²å…³é—­ï¼Œå·²è°ƒç”¨è¿‡close")

        assert not self.is_servant, "get_worker_keeper"
        from .worker_keeper import RedisWorkerKeeper

        return RedisWorkerKeeper(sequence_id, self.io, self.aio)

    @override
    async def close(self):
        if not self._ios:
            return

        for io in self._ios:
            io.close()
        self._ios = []

        for aio in self._async_ios:
            await aio.aclose()
        self._async_ios = []

    @staticmethod
    def _row_decode(
        comp_cls: type[BaseComponent], row: dict[bytes, bytes], fmt: RowFormat
    ) -> np.record | dict[str, Any]:
        """å°†redisè·å–çš„è¡Œbyteæ•°æ®è§£ç ä¸ºæŒ‡å®šæ ¼å¼"""
        row_decoded = {
            k.decode("utf-8", "ignore"): v.decode("utf-8", "ignore")
            for k, v in row.items()
        }
        match fmt:
            case RowFormat.RAW:
                return row_decoded
            case RowFormat.STRUCT:
                return comp_cls.dict_to_row(row_decoded)
            case RowFormat.TYPED_DICT:
                struct_row = comp_cls.dict_to_row(row_decoded)
                return comp_cls.row_to_dict(struct_row)
            case _:
                raise ValueError(f"ä¸å¯ç”¨çš„è¡Œæ ¼å¼: {fmt}")

    @overload
    async def get(
        self,
        table_ref: TableReference,
        row_id: int,
        row_format: Literal[RowFormat.STRUCT] = RowFormat.STRUCT,
    ) -> np.record | None: ...
    @overload
    async def get(
        self,
        table_ref: TableReference,
        row_id: int,
        row_format: Literal[RowFormat.RAW] = ...,
    ) -> dict[str, str] | None: ...
    @overload
    async def get(
        self,
        table_ref: TableReference,
        row_id: int,
        row_format: Literal[RowFormat.TYPED_DICT] = ...,
    ) -> dict[str, Any] | None: ...
    @overload
    async def get(
        self,
        table_ref: TableReference,
        row_id: int,
        row_format: RowFormat = ...,
    ) -> np.record | dict[str, str] | dict[str, Any] | None: ...
    @override
    async def get(
        self, table_ref: TableReference, row_id: int, row_format=RowFormat.STRUCT
    ) -> np.record | dict[str, Any] | None:
        """
        ä»æ•°æ®åº“ç›´æ¥è·å–å•è¡Œæ•°æ®ã€‚

        Parameters
        ----------
        table_ref: TableReference
            è¡¨ä¿¡æ¯ï¼ŒæŒ‡å®šComponentã€å®ä¾‹åã€åˆ†ç‰‡ç°‡idã€‚
        row_id: int
            row idä¸»é”®
        row_format
            è¿”å›æ•°æ®è§£ç æ ¼å¼ï¼Œè§ "Returns"

        Returns
        -------
        row: np.record or dict[str, any] or None
            å¦‚æœæœªæŸ¥è¯¢åˆ°åŒ¹é…æ•°æ®ï¼Œåˆ™è¿”å› Noneã€‚
            å¦åˆ™æ ¹æ® `row_format` å‚æ•°è¿”å›ä»¥ä¸‹æ ¼å¼ä¹‹ä¸€ï¼š

            - RowFormat.STRUCT - **é»˜è®¤å€¼**
                è¿”å› np.record (c-struct) çš„å•è¡Œæ•°æ®
            - RowFormat.RAW
                è¿”å›æ— ç±»å‹çš„åŸå§‹æ•°æ® (dict[str, str])
            - RowFormat.TYPED_DICT
                è¿”å›ç¬¦åˆComponentå®šä¹‰çš„ï¼Œæœ‰æ ¼å¼çš„dictç±»å‹ã€‚
                æ­¤æ–¹æ³•æ€§èƒ½ä½äº `RowFormat.STRUCT` ï¼Œä¸»è¦ç”¨äºjsonåä¼ é€’ç»™å®¢æˆ·ç«¯ã€‚
        """
        # todo æ‰€æœ‰get queryè¦åˆæ‰¹
        key = self.row_key(table_ref, row_id)
        if row := await self.aio.hgetall(key):  # type: ignore
            return self._row_decode(table_ref.comp_cls, row, row_format)
        else:
            return None

    @classmethod
    def _range_normalize(
        cls,
        dtype: np.dtype,
        left: int | float | str | bytes | bool,
        right: int | float | str | bytes | bool | None,
        desc: bool,
    ) -> tuple[bytes, bytes]:
        """è§„èŒƒåŒ–èŒƒå›´æŸ¥è¯¢çš„å·¦è¾¹ç•Œå’Œå³è¾¹ç•Œ"""
        # å¤„ç†right none, é¡ºåºé—®é¢˜
        if right is None:
            right = left
        if desc:
            left, right = right, left

        if issubclass(dtype.type, np.character):
            # componentå­—æ®µå¦‚æœæ˜¯str/bytesç±»å‹çš„ç´¢å¼•ï¼Œä¸èƒ½æŸ¥è¯¢æ•°å­—
            assert type(left) in (str, bytes) and type(right) in (str, bytes), (
                f"å­—ç¬¦ä¸²ç±»å‹çš„æŸ¥è¯¢å˜é‡ç±»å‹å¿…é¡»æ˜¯str/bytesï¼Œä½ çš„ï¼šleft={type(left)}({left}), "
                f"right={type(right)}({right})"
            )
        else:
            # componentå­—æ®µå¦‚æœæ˜¯intæ•°å­—ï¼Œåˆ™å¤„ç†inf
            # æµ®ç‚¹ä¸ç”¨å¤„ç†ï¼Œå› ä¸ºæµ®ç‚¹çš„infæ˜¯é«˜ä½çš„Exponentå…¨FFï¼Œå¤§äº2**1023æœ€å¤§å€¼ï¼Œè‡ªç„¶æ°¸è¿œæœ€å¤§
            if issubclass(dtype.type, np.integer):
                type_info = np.iinfo(dtype)

                def clamp_inf(x):
                    if type(x) is float and np.isinf(x):
                        return type_info.max if x > 0 else type_info.min
                    return x

                left = clamp_inf(left)
                right = clamp_inf(right)

        # å¤„ç†èŒƒå›´åŒºé—´
        def peel(x, _inclusive):
            if type(x) in (str, bytes) and len(x) >= 1:
                ch = x[0:1]  # byteså¿…é¡»ç”¨èŒƒå›´åˆ‡ç‰‡
                if ch in ("(", "[") or ch in (b"(", b"["):
                    _inclusive = ch == "[" or ch == b"["
                    x = x[1:]

            return x, _inclusive

        left, li = peel(left, True)
        right, ri = peel(right, True)
        # å› ä¸ºmemberæ˜¯value:idï¼Œæ‰€ä»¥left="value;"ä¸ºæ’é™¤ï¼Œright="value:"ä¸ºæ’é™¤
        ls = b":" if li else b";"
        rs = b";" if ri else b":"
        if desc:
            ls, rs = rs, ls

        # äºŒè¿›åˆ¶åŒ–ã€‚
        b_left = b"[" + cls.to_sortable_bytes(dtype.type(left)) + ls
        b_right = b"[" + cls.to_sortable_bytes(dtype.type(right)) + rs
        return b_left, b_right

    @overload
    async def range(
        self,
        table_ref: TableReference,
        index_name: str,
        left: int | float | str | bytes | bool,
        right: int | float | str | bytes | bool | None = None,
        limit: int = 100,
        desc: bool = False,
        row_format: Literal[RowFormat.STRUCT] = RowFormat.STRUCT,
    ) -> np.recarray: ...
    @overload
    async def range(
        self,
        table_ref: TableReference,
        index_name: str,
        left: int | float | str | bytes | bool,
        right: int | float | str | bytes | bool | None = None,
        limit: int = 100,
        desc: bool = False,
        row_format: Literal[RowFormat.RAW] = ...,
    ) -> list[dict[str, str]]: ...
    @overload
    async def range(
        self,
        table_ref: TableReference,
        index_name: str,
        left: int | float | str | bytes | bool,
        right: int | float | str | bytes | bool | None = None,
        limit: int = 100,
        desc: bool = False,
        row_format: Literal[RowFormat.TYPED_DICT] = ...,
    ) -> list[dict[str, Any]]: ...
    @overload
    async def range(
        self,
        table_ref: TableReference,
        index_name: str,
        left: int | float | str | bytes | bool,
        right: int | float | str | bytes | bool | None = None,
        limit: int = 100,
        desc: bool = False,
        row_format: Literal[RowFormat.ID_LIST] = ...,
    ) -> list[int]: ...
    @overload
    async def range(
        self,
        table_ref: TableReference,
        index_name: str,
        left: int | float | str | bytes | bool,
        right: int | float | str | bytes | bool | None = None,
        limit: int = 100,
        desc: bool = False,
        row_format: RowFormat = ...,
    ) -> np.recarray | list[dict[str, str]] | list[dict[str, Any]] | list[int]: ...
    @override
    async def range(
        self,
        table_ref: TableReference,
        index_name: str,
        left: int | float | str | bytes | bool,
        right: int | float | str | bytes | bool | None = None,
        limit: int = 100,
        desc: bool = False,
        row_format=RowFormat.STRUCT,
    ) -> list[int] | list[dict[str, Any]] | np.recarray:
        """
        ä»æ•°æ®åº“ç›´æ¥æŸ¥è¯¢ç´¢å¼• `index_name`ï¼Œè¿”å›åœ¨ [`left`, `right`] é—­åŒºé—´å†…æ•°æ®ã€‚
        å¦‚æœ `right` ä¸º `None`ï¼Œåˆ™æŸ¥è¯¢ç­‰äº `left` çš„æ•°æ®ï¼Œé™åˆ¶ `limit` æ¡ã€‚

        Parameters
        ----------
        table_ref: TableReference
            è¡¨ä¿¡æ¯ï¼ŒæŒ‡å®šComponentã€å®ä¾‹åã€åˆ†ç‰‡ç°‡idã€‚
        index_name: str
            æŸ¥è¯¢Componentä¸­çš„å“ªæ¡ç´¢å¼•
        left, right: str or number
            æŸ¥è¯¢èŒƒå›´ï¼Œé—­åŒºé—´ã€‚å¯ä»¥åœ¨å¼€å¤´åŠ ä¸Š"["æŒ‡å®šé—­åŒºé—´ï¼Œè¿˜æ˜¯"("å¼€åŒºé—´ã€‚
            å¦‚æœrightä¸å¡«å†™ï¼Œåˆ™ç²¾ç¡®æŸ¥è¯¢ç­‰äºleftçš„æ•°æ®ã€‚
        limit: int
            é™åˆ¶è¿”å›çš„è¡Œæ•°ï¼Œæ–¹æ³•å¾€æ•°æ®åº“æŸ¥è¯¢çš„æ¬¡æ•°ä¸º `1 + limit` æ¬¡ã€‚
        desc: bool
            æ˜¯å¦é™åºæ’åˆ—
        row_format
            è¿”å›æ•°æ®è§£ç æ ¼å¼ï¼Œè§ "Returns"

        Returns
        -------
        row: np.recarray or list[int] or list[dict]
            æ ¹æ® `row_format` å‚æ•°è¿”å›ä»¥ä¸‹æ ¼å¼ä¹‹ä¸€ï¼š

            - RowFormat.STRUCT - **é»˜è®¤å€¼**
                è¿”å› `numpy.recarray`ï¼Œå¦‚æœæ²¡æœ‰æŸ¥è¯¢åˆ°æ•°æ®ï¼Œè¿”å›ç©º `numpy.recarray`ã€‚
                `numpy.recarray` æ˜¯ä¸€ç§ c-struct arrayã€‚
            - RowFormat.RAW
                è¿”å›æ— ç±»å‹çš„åŸå§‹æ•°æ® (dict[str, str]) çš„åˆ—è¡¨ï¼Œå¦‚æœæ²¡æœ‰æŸ¥è¯¢åˆ°æ•°æ®ï¼Œè¿”å›ç©ºlist
            - RowFormat.TYPED_DICT
                è¿”å›ç¬¦åˆComponentå®šä¹‰çš„ï¼Œæœ‰æ ¼å¼çš„dictç±»å‹åˆ—è¡¨ï¼Œå¦‚æœæ²¡æœ‰æŸ¥è¯¢åˆ°æ•°æ®ï¼Œè¿”å›ç©ºlist
                æ­¤æ–¹æ³•æ€§èƒ½ä½äº `RowFormat.STRUCT` ï¼Œä¸»è¦ç”¨äºjsonåä¼ é€’ç»™å®¢æˆ·ç«¯ã€‚
            - RowFormat.ID_LIST
                è¿”å›æŸ¥è¯¢åˆ°çš„ row id åˆ—è¡¨ï¼Œå¦‚æœæ²¡æœ‰æŸ¥è¯¢åˆ°æ•°æ®ï¼Œè¿”å›ç©ºlist

        Notes
        -----
        å¦‚ä½•å¤åˆæ¡ä»¶æŸ¥è¯¢ï¼Ÿ
        è¯·åˆ©ç”¨pythonçš„ç‰¹æ€§ï¼Œå…ˆåœ¨æ•°æ®åº“ä¸Šç­›é€‰å‡ºæœ€å°‘é‡çš„æ•°æ®ï¼Œç„¶åæœ¬åœ°äºŒæ¬¡ç­›é€‰::

            items = client.range(ref, "owner", player_id, limit=100)
            few_items = items[items.amount < 10]

        ç”±äºpython numpyæ”¯æŒSIMDï¼Œæ¯”ç›´æ¥åœ¨æ•°æ®åº“å¤åˆæŸ¥è¯¢å¿«ã€‚
        """
        # todo æ‰€æœ‰get queryè¦åˆæ‰¹
        idx_key = self.index_key(table_ref, index_name)
        aio = self.aio  # ä¿å­˜éšæœºé€‰ä¸­çš„aioè¿æ¥

        # ç”Ÿæˆzrangeå‘½ä»¤
        comp_cls = table_ref.comp_cls
        assert index_name in comp_cls.indexes_
        b_left, b_right = self._range_normalize(
            comp_cls.dtype_map_[index_name],
            left,
            right,
            desc,
        )
        assert (b_left >= b_right) if desc else (b_right >= b_left), (
            f"leftå¿…é¡»å¤§äºç­‰äºrightï¼Œä½ çš„:right={right}, left={left}"
        )

        # å¯¹äºstrç±»å‹æŸ¥è¯¢ï¼Œè¦ç”¨bylex
        cmds = {
            "start": b_left,
            "end": b_right,
            "desc": desc,
            "offset": 0,
            "num": limit,
            "bylex": True,
            "byscore": False,
        }

        row_ids = await aio.zrange(name=idx_key, **cmds)
        row_ids = [int(vk.rsplit(b":", 1)[-1]) for vk in row_ids]

        if row_format == RowFormat.ID_LIST:
            return row_ids

        key_prefix = self.cluster_prefix(table_ref) + ":id:"  # å­˜ä¸‹å‰ç¼€ç»„åˆkeyå¿«1å€
        rows = []
        for _id in row_ids:
            # todo è¦ä¹ˆç”¨åˆæ‰¹çš„è¯·æ±‚æ–¹æ³•ï¼Œè¦ä¹ˆç”¨pipeline
            if row := await aio.hgetall(key_prefix + str(_id)):  # type: ignore
                rows.append(self._row_decode(comp_cls, row, row_format))

        if row_format == RowFormat.RAW or row_format == RowFormat.TYPED_DICT:
            return cast(list[dict[str, Any]], rows)
        else:
            if len(rows) == 0:
                return np.rec.array(np.empty(0, dtype=comp_cls.dtypes))
            else:
                record_list = cast(list[np.record], rows)
                return np.rec.array(np.stack(record_list, dtype=comp_cls.dtypes))

    @override
    async def commit(self, idmap: IdentityMap) -> None:
        """
        ä½¿ç”¨äº‹åŠ¡ï¼Œå‘æ•°æ®åº“æäº¤IdentityMapä¸­çš„æ‰€æœ‰æ•°æ®ä¿®æ”¹

        Exceptions
        --------
        RaceCondition
            å½“æäº¤æ•°æ®æ—¶ï¼Œå‘ç°æ•°æ®å·²è¢«å…¶ä»–äº‹åŠ¡ä¿®æ”¹ï¼ŒæŠ›å‡ºæ­¤å¼‚å¸¸

        """

        def _key_must_not_exist(_key: str):
            """æ·»åŠ key must not existçš„æ£€æŸ¥"""
            checks.append(["NX", _key])

        def _version_must_match(_key: str, _old_version):
            """æ·»åŠ version matchçš„æ£€æŸ¥"""
            checks.append(["VER", _key, _old_version])

        def _unique_meet(_unique_fields, _dtype_map, _idx_prefix, _row: dict[str, str]):
            """æ·»åŠ uniqueç´¢å¼•æ£€æŸ¥"""
            for _field, _value in _row.items():
                if _field in _unique_fields:
                    _idx_key = _idx_prefix + _field
                    _sortable_value = self.to_sortable_bytes(
                        _dtype_map[_field].type(_value)
                    )
                    _start_val = b"[" + _sortable_value + b":"
                    _end_val = b"[" + _sortable_value + b";"
                    checks.append(["UNIQ", _idx_key, _start_val, _end_val])

        def _hset_key(_key, _old_version, _update: dict[str, str]):
            """æ·»åŠ hsetçš„pushå‘½ä»¤"""
            # ç‰ˆæœ¬+1
            _ver = int(_old_version) + 1
            _update.pop("_version", None)  # æ— è§†ç”¨æˆ·ä¼ å…¥çš„_versionå­—æ®µ
            # ç»„åˆhset, åˆ«å¿˜è®°å†™_version
            _kvs = itertools.chain.from_iterable(_update.items())
            pushes.append(["HSET", _key, "_version", str(_ver), *_kvs])

        def _exc_index(_indexes, _dtype_map, _idx_prefix, _old, _new, _add):
            """exchange index(zadd/zrem)çš„pushå‘½ä»¤"""
            _b_row_id = _old["id"].encode("ascii")
            _values = _new if _add else _old
            for _field in _new.keys():
                if _field in _indexes:
                    _idx_key = _idx_prefix + _field
                    # ç´¢å¼•å…¨éƒ¨è½¬æ¢ä¸ºbytesç´¢å¼•ï¼Œæµ‹è¯•ä¸‹æ¥lexå’Œscoreæ’åºæ€§èƒ½æ˜¯ä¸€æ ·çš„
                    _sortable_value = self.to_sortable_bytes(
                        _dtype_map[_field].type(_values[_field])
                    )
                    _member = _sortable_value + b":" + _b_row_id
                    if _add:
                        # scoreç»Ÿä¸€ç”¨0ï¼Œå› ä¸ºæˆ‘ä»¬ä¸éœ€è¦scoreæ’åºåŠŸèƒ½
                        pushes.append(["ZADD", _idx_key, "0", _member])
                    else:
                        pushes.append(["ZREM", _idx_key, _member])

        def _del_key(_key):
            """æ·»åŠ delçš„pushå‘½ä»¤"""
            pushes.append(["DEL", _key])

        assert not self.is_servant, "ä»èŠ‚ç‚¹ä¸å…è®¸æäº¤äº‹åŠ¡"

        dirties = idmap.get_dirty_rows()
        if not dirties:
            raise ValueError("æ²¡æœ‰è„æ•°æ®éœ€è¦æäº¤")

        first_ref = idmap.first_reference()
        assert first_ref is not None, "typingæ£€æŸ¥"

        # ç»„åˆæˆchecks/pusheså‘½ä»¤è¡¨ï¼Œå‡å°‘luaè„šæœ¬çš„å¤æ‚åº¦
        # checksæœ‰exists/unique/version
        # pushesæœ‰hset/zadd/zrem/del
        checks: list[list[str | bytes]] = []
        pushes: list[list[str | bytes]] = []

        for ref, (inserts, (old_rows, new_rows), deletes) in dirties.items():
            id_prefix = self.cluster_prefix(ref) + ":id:"
            idx_prefix = self.cluster_prefix(ref) + ":index:"
            comp_cls = ref.comp_cls
            unique_fields = comp_cls.uniques_
            indexes = comp_cls.indexes_
            dtype_map = comp_cls.dtype_map_
            # insert
            for insert in inserts:
                row_id = insert["id"]
                key = id_prefix + row_id
                _key_must_not_exist(key)
                _unique_meet(unique_fields, dtype_map, idx_prefix, insert)
                _hset_key(key, 0, insert)
                _exc_index(indexes, dtype_map, idx_prefix, insert, insert, _add=True)
            # update
            for old_row, new_row in zip(old_rows, new_rows):
                row_id = old_row["id"]
                key = id_prefix + row_id
                old_version = old_row["_version"]
                _version_must_match(key, old_version)
                _unique_meet(unique_fields, dtype_map, idx_prefix, new_row)
                _hset_key(key, old_version, new_row)
                _exc_index(indexes, dtype_map, idx_prefix, old_row, new_row, _add=False)
                _exc_index(indexes, dtype_map, idx_prefix, old_row, new_row, _add=True)
            # delete
            for delete in deletes:
                key = id_prefix + str(delete["id"])
                old_version = delete["_version"]
                _version_must_match(key, old_version)
                _exc_index(indexes, dtype_map, idx_prefix, delete, delete, _add=False)
                _del_key(key)

        payload_json: bytes = msgpack.packb([checks, pushes], use_bin_type=False)  # type: ignore
        # æ·»åŠ ä¸€ä¸ªå¸¦cluster idçš„keyï¼ŒæŒ‡æ˜luaè„šæœ¬æ‰§è¡Œçš„é›†ç¾¤
        keys = [self.row_key(first_ref, 1)]

        # è¿™é‡Œä¸éœ€è¦åˆ¤æ–­redis.exceptions.NoScriptErrorï¼Œå› ä¸ºé‡Œé¢ä¼šå¤„ç†
        assert self.lua_commit is not None, "typingæ£€æŸ¥"
        resp = await self.lua_commit(keys, [payload_json])
        resp = resp.decode("utf-8")  # type: ignore

        if resp != "committed":
            if resp.startswith("RACE"):
                raise RaceCondition(resp)
            elif resp.startswith("UNIQUE"):
                # uniqueè¿åå°±æ˜¯indexçš„ç«æ€åŸå› 
                raise RaceCondition(resp)
            else:
                raise RuntimeError(f"æœªçŸ¥çš„æäº¤é”™è¯¯ï¼š{resp}")

    def get_table_maintenance(self) -> RedisTableMaintenance:
        """
        è·å–è¡¨ç»´æŠ¤å¯¹è±¡ã€‚
        """
        if not self._ios:
            raise ConnectionError("è¿æ¥å·²å…³é—­ï¼Œå·²è°ƒç”¨è¿‡close")

        from .maint import RedisTableMaintenance

        return RedisTableMaintenance(self)

    def get_mq_client(self) -> RedisMQClient:
        """è·å–æ¶ˆæ¯é˜Ÿåˆ—è¿æ¥"""
        if not self._ios:
            raise ConnectionError("è¿æ¥å·²å…³é—­ï¼Œå·²è°ƒç”¨è¿‡close")
        from .mq import RedisMQClient

        return RedisMQClient(self)
