"""
@author: Heerozh (Zhang Jianhao)
@copyright: Copyright 2024, Heerozh. All rights reserved.
@license: Apache2.0 å¯ç”¨ä½œå•†ä¸šé¡¹ç›®ï¼Œå†éšä¾¿æ‰¾ä¸ªè§’è½æåŠç”¨åˆ°äº†æ­¤é¡¹ç›® :D
@email: heeroz@gmail.com
"""

import asyncio
import logging
import random
from pathlib import Path
from typing import Any, Callable

import msgspec
import numpy as np
import redis
import redis.asyncio
import redis.asyncio.cluster
import redis.cluster
import redis.exceptions

from ...component import BaseComponent
from ...idmap import IdentityMap
from ..base import BackendClient, RowFormat

logger = logging.getLogger("HeTu.root")


class RedisBackendClient(BackendClient, alias="redis"):
    """å’ŒRedisåç«¯çš„æ“ä½œçš„ç±»ï¼ŒæœåŠ¡å™¨å¯åŠ¨æ—¶ç”±server.pyæ ¹æ®Configåˆå§‹åŒ–"""

    def load_lua_scripts(self, file: str | Path) -> Callable:
        assert self._async_ios
        # read file to text
        with open(file, "r", encoding="utf-8") as f:
            script_text = f.read()
        # ä¸Šä¼ è„šæœ¬åˆ°æœåŠ¡å™¨ä½¿ç”¨åŒæ­¥io
        self.io.script_load(script_text)
        # æ³¨å†Œè„šæœ¬åˆ°å¼‚æ­¥io
        return self.aio.register_script(script_text)

    @property
    def io(self):
        """éšæœºè¿”å›ä¸€ä¸ªåŒæ­¥è¿æ¥"""
        return random.choice(self._ios)

    @property
    def aio(self):
        """éšæœºè¿”å›ä¸€ä¸ªå¼‚æ­¥è¿æ¥"""
        if self.loop_id == 0:
            self.loop_id = hash(asyncio.get_running_loop())
        # redis-pyçš„async connectionç”¨çš„pythonçš„steam.connectï¼Œç»‘å®šåˆ°å½“å‰åç¨‹
        # è€Œaioæ˜¯ä¸€ä¸ªconnection poolï¼Œæ–­å¼€çš„è¿æ¥ä¼šæ”¾å›poolä¸­ï¼Œæ‰€ä»¥aioä¸èƒ½è·¨åç¨‹ä¼ é€’
        assert hash(asyncio.get_running_loop()) == self.loop_id, (
            "Backendåªèƒ½åœ¨åŒä¸€ä¸ªcoroutineä¸­ä½¿ç”¨ã€‚æ£€æµ‹åˆ°è°ƒç”¨æ­¤å‡½æ•°çš„åç¨‹å‘ç”Ÿäº†å˜åŒ–"
        )

        return random.choice(self._async_ios)

    def __init__(self, endpoint: str | list[str], clustering: bool, is_servant=False):
        super().__init__(endpoint, clustering, is_servant)
        # redisçš„endpointé…ç½®ä¸ºurl, æˆ–list of url
        self.urls = [endpoint] if type(endpoint) is str else endpoint
        assert len(self.urls) > 0, "å¿…é¡»è‡³å°‘æŒ‡å®šä¸€ä¸ªæ•°æ®åº“è¿æ¥URL"

        # åˆ›å»ºè¿æ¥
        self._ios = []
        self._async_ios = []
        for url in self.urls:
            if self.clustering:
                # todo: æµ‹è¯•byteæ•°æ®æ˜¯å¦èƒ½æ­£ç¡®çš„å‚¨å­˜å’Œè¯»å–
                self._ios.append(redis.cluster.RedisCluster.from_url(url))
                self._async_ios.append(redis.asyncio.cluster.RedisCluster.from_url(url))
            else:
                self._ios.append(redis.Redis.from_url(url))
                self._async_ios.append(redis.asyncio.Redis.from_url(url))

        # æµ‹è¯•è¿æ¥æ˜¯å¦æ­£å¸¸
        for i, io in enumerate(self._ios):
            try:
                io.ping()
            except redis.exceptions.ConnectionError as e:
                raise ConnectionError(f"æ— æ³•è¿æ¥åˆ°Redisæ•°æ®åº“ï¼š{self.urls[i]}") from e

        # è·å¾—db index
        self.dbi = self._ios[0].connection_pool.connection_kwargs["db"]

        # åŠ è½½luaè„šæœ¬ï¼Œæ³¨æ„pipelineé‡Œä¸èƒ½ç”¨luaï¼Œä¼šåå¤æ£€æµ‹script existsæ€§èƒ½æä½
        self.lua_commit = self.load_lua_scripts(
            Path(__file__).parent.resolve() / "commit.lua"
        )

        # é™åˆ¶aioè¿è¡Œçš„coroutine
        try:
            self.loop_id = hash(asyncio.get_running_loop())
        except RuntimeError:
            self.loop_id = 0

    def configure(self) -> None:
        if self.is_servant:
            self.configure_servant()
        else:
            self.configure_master()

    def configure_master(self) -> None:
        if not self._ios:
            raise ConnectionError("è¿æ¥å·²å…³é—­ï¼Œå·²è°ƒç”¨è¿‡close")

        # æ£€æµ‹redisç‰ˆæœ¬
        def parse_version(x):
            return tuple(map(int, x.split(".")))

        for i, io in enumerate(self._ios):
            version = io.info("server")["redis_version"]
            assert parse_version(version) >= (7, 0), "Redisç‰ˆæœ¬è¿‡ä½ï¼Œè‡³å°‘éœ€è¦7.0ç‰ˆæœ¬"

    def configure_servant(self) -> None:
        if not self._ios:
            raise ConnectionError("è¿æ¥å·²å…³é—­ï¼Œå·²è°ƒç”¨è¿‡close")
            # æ£€æŸ¥servantsè®¾ç½®

        target_keyspace = "Kghz"
        for i, io in enumerate(self._ios):
            try:
                # è®¾ç½®keyspaceé€šçŸ¥
                db_keyspace = io.config_get("notify-keyspace-events")[
                    "notify-keyspace-events"
                ]
                db_keyspace = db_keyspace.replace("A", "g$lshztxed")
                db_keyspace_new = db_keyspace
                for flag in list(target_keyspace):
                    if flag not in db_keyspace:
                        db_keyspace_new += flag
                if db_keyspace_new != db_keyspace:
                    io.config_set("notify-keyspace-events", db_keyspace_new)
            except (
                redis.exceptions.NoPermissionError,
                redis.exceptions.ResponseError,
            ):
                logger.warning(
                    f"âš ï¸ [ğŸ’¾Redis] æ— æƒé™è°ƒç”¨æ•°æ®åº“{self.urls[i]}çš„config_setå‘½ä»¤ï¼Œæ•°æ®è®¢é˜…å°†"
                    f"ä¸èµ·æ•ˆã€‚å¯æ‰‹åŠ¨è®¾ç½®é…ç½®æ–‡ä»¶ï¼šnotify-keyspace-events={target_keyspace}"
                )
            # æ£€æŸ¥æ˜¯å¦æ˜¯replicaæ¨¡å¼
            db_replica = io.config_get("replica-read-only")
            if db_replica.get("replica-read-only") != "yes":
                logger.warning(
                    "âš ï¸ [ğŸ’¾Redis] servantå¿…é¡»æ˜¯Read Only Replicaæ¨¡å¼ã€‚"
                    f"{self.urls[i]} æœªè®¾ç½®replica-read-only=yes"
                )
                # ä¸æ£€æŸ¥replicaof masteråœ°å€ï¼Œå› ä¸ºreplicaofçš„å¯èƒ½æ˜¯å…¶ä»–replicaåœ°å€
            # è€ƒè™‘å¯ä»¥æ£€æŸ¥pubsub client buffè®¾ç½®ï¼Œçœ‹çœ‹èƒ½å¦rediså´©äº†æé†’ä¸‹
            # pubsubå€¼å»ºè®®ä¸º$å‰©ä½™å†…å­˜/é¢„ä¼°åœ¨çº¿æ•°$

    async def is_synced(self) -> bool:
        if not self._ios:
            raise ConnectionError("è¿æ¥å·²å…³é—­ï¼Œå·²è°ƒç”¨è¿‡close")

        assert not self.is_servant, "is_syncedåªèƒ½åœ¨masterä¸Šè°ƒç”¨"

        info = await self.aio.info("replication")
        master_offset = info.get("master_repl_offset", 0)
        for key, value in info.items():
            # å…¼å®¹ Redis æ–°æ—§ç‰ˆæœ¬ï¼ˆslave/replica å­—æ®µï¼‰
            if key.startswith("slave") or key.startswith("replica"):
                lag_of_offset = master_offset - int(value.get("offset", 0))
                if lag_of_offset > 0:
                    return False
        return True

    def reset_async_connection_pool(self):
        """é‡ç½®å¼‚æ­¥è¿æ¥æ± ï¼Œç”¨äºåç¨‹åˆ‡æ¢åï¼Œè§£å†³aioä¸èƒ½è·¨åç¨‹ä¼ é€’çš„é—®é¢˜"""
        self.loop_id = 0
        for aio in self._async_ios:
            aio.connection_pool.reset()

    async def close(self):
        if not self._ios:
            return

        for io in self._ios:
            io.close()
        self._ios = []

        for aio in self._async_ios:
            await aio.aclose()
        self._async_ios = []

    # def get_mq_client(self) -> RedisMQClient:
    #     """æ¯ä¸ªwebsocketè¿æ¥è·å¾—ä¸€ä¸ªéšæœºçš„replicaè¿æ¥ï¼Œç”¨äºè¯»å–è®¢é˜…"""
    #     if not self.io:
    #         raise ConnectionError("è¿æ¥å·²å…³é—­ï¼Œå·²è°ƒç”¨è¿‡close")
    #     return RedisMQClient(self.random_replica())

    async def get(
        self,
        comp_cls: type[BaseComponent],
        row_id: int,
        row_format=RowFormat.STRUCT,
    ) -> np.record | dict[str, Any] | None:
        """è·å–è¡Œæ•°æ®"""
        # todo æ‰€æœ‰get queryè¦åˆæ‰¹
        key = self._key_prefix + str(row_id)
        row = await self.aio.hgetall(key)
        if row:
            # todo æ­¤æ—¶çš„rowæ•°æ®éƒ½æ˜¯byte

            match row_format:
                case RowFormat.RAW:
                    return row
                case RowFormat.STRUCT:
                    return comp_cls.dict_to_row(row)
                case RowFormat.TYPED_DICT:
                    struct_row = comp_cls.dict_to_row(row)
                    return comp_cls.row_to_dict(struct_row)
                case _:
                    raise ValueError(f"ä¸å¯ç”¨çš„è¡Œæ ¼å¼: {row_format}")
        else:
            return None

    def _range_normalize(
        self,
        is_str_index: bool,
        left: int | float | str,
        right: int | float | str | None,
        desc: bool,
    ) -> tuple[int | float | str, int | float | str]:
        """è§„èŒƒåŒ–èŒƒå›´æŸ¥è¯¢çš„å·¦è¾¹ç•Œå’Œå³è¾¹ç•Œ"""
        if right is None:
            right = left
        if desc:
            left, right = right, left

        # å¯¹äºstrç±»å‹æŸ¥è¯¢ï¼Œè¦ç”¨[å¼€å§‹
        if is_str_index:
            left = str(left)
            right = str(right)
            # åˆ¤æ–­typeæ•ˆç‡å¤ªä½äº†ï¼Œç‰¹åˆ«æ˜¯isinstanceï¼Œå–æ¶ˆæ‰
            # assert (
            #     isinstance(left, (str, np.str_)) and isinstance(right, (str, np.str_))
            # ), f"å­—ç¬¦ä¸²ç±»å‹ç´¢å¼•`{index_name}`çš„æŸ¥è¯¢(left={left}, {type(left)})å˜é‡ç±»å‹å¿…é¡»æ˜¯str"
            if not left.startswith(("(", "[")):
                left = f"[{left}"
            if not right.startswith(("(", "[")):
                right = f"[{right}"

            if not left.endswith((":", ";")):
                left = f"{left}:"  # name:id å½¢å¼ï¼Œæ‰€ä»¥:ä½œä¸ºç»“å°¾æ ‡è¯†ç¬¦
            if not right.endswith((":", ";")):
                right = f"{right};"  # ';' = 3B, ':' = 3A

        return left, right

    async def range(
        self,
        comp_cls: type[BaseComponent],
        index_name: str,
        left: int | float | str,
        right: int | float | str | None,
        limit: int = 100,
        desc: bool = False,
        row_format=RowFormat.STRUCT,
    ) -> list[int] | np.recarray:
        """æŸ¥è¯¢indexæ•°æ®"""
        # todo æ‰€æœ‰get queryè¦åˆæ‰¹
        # todo æƒ³ä¸€ä¸‹è¿™å‡ ä¸ªkeyprefixå¦‚ä½•å¤„ç†ï¼Œå¯ä»¥å…ˆæŠŠsessionåšå®Œäº†å†è€ƒè™‘ï¼Ÿ
        idx_key = self._idx_prefix + index_name
        aio = self.aio  # ä¿å­˜éšæœºé€‰ä¸­çš„aioè¿æ¥

        # ç”Ÿæˆzrangeå‘½ä»¤
        is_str_index = comp_cls.indexes_[index_name]
        left, right = self._range_normalize(
            is_str_index,
            left,
            right,
            desc,
        )

        # å¯¹äºstrç±»å‹æŸ¥è¯¢ï¼Œè¦ç”¨bylex
        by_lex = True if is_str_index else False
        cmds = {
            "start": left,
            "end": right,
            "desc": desc,
            "offset": 0,
            "num": limit,
            "bylex": by_lex,
            "byscore": not by_lex,
        }

        row_ids = await aio.zrange(name=idx_key, **cmds)
        if is_str_index:
            row_ids = [vk.split(":")[-1] for vk in row_ids]

        if row_format == RowFormat.ID_LIST:
            return list(map(int, row_ids))

        typed = row_format == RowFormat.TYPED_DICT or row_format == RowFormat.STRUCT
        dict_fmt = row_format == RowFormat.RAW or row_format == RowFormat.TYPED_DICT

        key_prefix = self._key_prefix
        rows = []
        for _id in row_ids:
            if row := await aio.hgetall(key_prefix + str(_id)):
                if typed:
                    row = comp_cls.dict_to_row(row)
                if dict_fmt:
                    row = comp_cls.row_to_dict(row)
                rows.append(row)

        if dict_fmt:
            return rows
        else:
            if len(rows) == 0:
                return np.rec.array(np.empty(0, dtype=comp_cls.dtypes))
            else:
                return np.rec.array(np.stack(rows, dtype=comp_cls.dtypes))

    async def commit(self, idmap: IdentityMap) -> None:
        """æäº¤ä¿®æ”¹äº‹åŠ¡ï¼Œä½¿ç”¨ä»IdentityMapä¸­è·å–çš„è„æ•°æ®"""
        assert not self.is_servant, "ä»èŠ‚ç‚¹ä¸å…è®¸æäº¤äº‹åŠ¡"

        dirty_rows = idmap.get_dirty_rows()
        assert len(dirty_rows) > 0, "æ²¡æœ‰è„æ•°æ®éœ€è¦æäº¤"

        payload = msgspec.msgpack.encode(dirty_rows)
        keys = []  # todo éœ€è¦æ·»åŠ ä¸€ä¸ªè¡¨ç¤ºclusterçš„key
        return await self.lua_commit(keys, payload)

    # è¿˜éœ€è¦
    # create table
    # migration table schema
    # migration cluster id
    # flush table
    # rebuild table index
    # å¯ä»¥è€ƒè™‘ä¸€ä¸ªtable_maintenanceç±»ä¸“é—¨åšè¿™ä¸ª

    # def flush(self, comp_cls: type[BaseComponent], force=False):
    #     if force:
    #         warnings.warn("flushæ­£åœ¨å¼ºåˆ¶åˆ é™¤æ‰€æœ‰æ•°æ®ï¼Œæ­¤æ–¹å¼åªå»ºè®®ç»´æŠ¤ä»£ç è°ƒç”¨ã€‚")

    #     # å¦‚æœéæŒä¹…åŒ–ç»„ä»¶ï¼Œåˆ™å…è®¸è°ƒç”¨flushä¸»åŠ¨æ¸…ç©ºæ•°æ®
    #     if not comp_cls.persist_ or force:
    #         io = self.io
    #         logger.info(
    #             f"âŒš [ğŸ’¾Redis][{self._name}ç»„ä»¶] å¯¹éæŒä¹…åŒ–ç»„ä»¶flushæ¸…ç©ºæ•°æ®ä¸­..."
    #         )

    #         # è¿™éƒ¨åˆ†è¦æƒ³åŠæ³•
    #         with io.lock(self._init_lock_key, timeout=60 * 5):
    #             del_keys = io.keys(self._root_prefix + "*")
    #             del_keys.remove(self._init_lock_key)
    #             for batch in batched(del_keys, 1000):
    #                 with io.pipeline() as pipe:
    #                     list(map(pipe.delete, batch))
    #                     pipe.execute()
    #         logger.info(f"âœ… [ğŸ’¾Redis][{self._name}ç»„ä»¶] å·²åˆ é™¤{len(del_keys)}ä¸ªé”®å€¼")

    #         self.create_or_migrate()
    #     else:
    #         raise ValueError(f"{self._name}æ˜¯æŒä¹…åŒ–ç»„ä»¶ï¼Œä¸å…è®¸flushæ“ä½œ")
